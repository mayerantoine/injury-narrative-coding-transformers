{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a SageMaker Pipeline to train and register the injury narrative BERT classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install tensorflow\n",
    "!pip install transformers\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import activations, optimizers, losses\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import joblib \n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cdc-cdh-sagemaker-s3fs-dev'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket = 'cdc-cdh-sagemaker-s3fs-dev'\n",
    "sagemaker_session = sagemaker.Session(default_bucket=bucket)\n",
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "default_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 08/18/2021 05:35:25 PM Start.....\n",
      "INFO: 08/18/2021 05:35:25 PM Parsing arguments\n",
      "INFO: 08/18/2021 05:35:25 PM Getting and splitting data\n",
      "INFO: 08/18/2021 05:35:25 PM nb classes in final data:28\n",
      "INFO: 08/18/2021 05:35:25 PM  X (7668,) , y : (7668,)\n",
      "INFO: 08/18/2021 05:35:25 PM X_train shape (6901,) y_train shape : (6901,)\n",
      "INFO: 08/18/2021 05:35:25 PM X_valid shape (767,) y_valid shape : (767,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 08/18/2021 05:35:25 PM Preprocessing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62, 71, 63, 11, 43, 55, 42, 52, 60, 73, 13, 66, 12, 53, 64, 27, 24, 99, 26, 72, 70, 51, 44, 41, 31, 78, 32, 23]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 08/18/2021 05:35:27 PM Tokenization and encoding...\n",
      "INFO: 08/18/2021 05:35:28 PM Encoding Labels .....\n",
      "INFO: 08/18/2021 05:35:28 PM Create TF Dataset....\n",
      "INFO: 08/18/2021 05:35:28 PM Saving train and valid TF Records...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/injury-narrative-coding-transformers/src/pre-processing.py:231: TFRecordWriter.__init__ (from tensorflow.python.data.experimental.ops.writers) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To write TFRecords to disk, use `tf.io.TFRecordWriter`. To save and load the contents of a dataset, use `tf.data.experimental.save` and `tf.data.experimental.load`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: 08/18/2021 05:35:28 PM From /root/injury-narrative-coding-transformers/src/pre-processing.py:231: TFRecordWriter.__init__ (from tensorflow.python.data.experimental.ops.writers) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To write TFRecords to disk, use `tf.io.TFRecordWriter`. To save and load the contents of a dataset, use `tf.data.experimental.save` and `tf.data.experimental.load`\n",
      "INFO: 08/18/2021 05:36:44 PM Saving test dataset...\n",
      "INFO: 08/18/2021 05:36:56 PM Complete\n"
     ]
    }
   ],
   "source": [
    "%run ./src/pre-processing.py --data_path ./data/raw  --train_percentage 0.05 --is_sample_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 08/17/2021 09:25:54 PM Start.....\n",
      "INFO: 08/17/2021 09:25:54 PM Parsing arguments\n",
      "INFO: 08/17/2021 09:25:54 PM Create sagemaker session\n",
      "INFO: 08/17/2021 09:25:54 PM input_data: ./data/test\n",
      "INFO: 08/17/2021 09:25:54 PM input_model: ./output/model/training-BaseBERT-08-02-58-54-2021-08-02-19-58-55-017\n",
      "INFO: 08/17/2021 09:25:54 PM Listing contents of input model dir: ./output/model/training-BaseBERT-08-02-58-54-2021-08-02-19-58-55-017\n",
      "INFO: 08/17/2021 09:25:54 PM Loading model..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow: 2.6.0 \n",
      "\n",
      "Hugging Face transfomers: 4.9.2 \n",
      "\n",
      "NLTK: 3.4.5 \n",
      "\n",
      "BaseBERT\n",
      ".ipynb_checkpoints\n",
      "model.tar.gz\n",
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: 08/17/2021 09:26:19 PM SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
      "INFO: 08/17/2021 09:26:37 PM Listing contents of input data dir: ./data/test\n",
      "INFO: 08/17/2021 09:26:37 PM Loading encoder..\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/logging/__init__.py\", line 1025, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/opt/conda/lib/python3.7/logging/__init__.py\", line 869, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/opt/conda/lib/python3.7/logging/__init__.py\", line 608, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/opt/conda/lib/python3.7/logging/__init__.py\", line 369, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/opt/conda/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/opt/conda/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 583, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.7/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/ioloop.py\", line 688, in <lambda>\n",
      "    lambda f: self._run_callback(functools.partial(callback, future))\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/ioloop.py\", line 741, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/gen.py\", line 814, in inner\n",
      "    self.ctx_run(self.run)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/gen.py\", line 775, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
      "    yield gen.maybe_future(dispatch(*args))\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n",
      "    yield gen.maybe_future(handler(stream, idents, msg))\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 541, in execute_request\n",
      "    user_expressions, allow_stdin,\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/tornado/gen.py\", line 234, in wrapper\n",
      "    yielded = ctx_run(next, result)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 300, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2858, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2886, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3063, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3254, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-7-f2a7a757c313>\", line 1, in <module>\n",
      "    get_ipython().run_line_magic('run', \"./src/evaluate_model_metrics.py --input_data './data/test'              --input_model './output/model/training-BaseBERT-08-02-58-54-2021-08-02-19-58-55-017'              --max_len 45              --output_data './output/model'\")\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2317, in run_line_magic\n",
      "    result = fn(*args, **kwargs)\n",
      "  File \"</opt/conda/lib/python3.7/site-packages/decorator.py:decorator-gen-59>\", line 2, in run\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/magic.py\", line 187, in <lambda>\n",
      "    call = lambda f, *a, **k: f(*a, **k)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/magics/execution.py\", line 827, in run\n",
      "    run()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/magics/execution.py\", line 813, in run\n",
      "    exit_ignore=exit_ignore)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2722, in safe_execfile\n",
      "    self.compile if shell_futures else None)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/utils/py3compat.py\", line 168, in execfile\n",
      "    exec(compiler(f.read(), fname, 'exec'), glob, loc)\n",
      "  File \"/root/injury-narrative-coding-transformers/src/evaluate_model_metrics.py\", line 201, in <module>\n",
      "    main()\n",
      "  File \"/root/injury-narrative-coding-transformers/src/evaluate_model_metrics.py\", line 178, in main\n",
      "    logging.info(\"Encoder class:\",encode.classes_.tolist())\n",
      "Message: 'Encoder class:'\n",
      "Arguments: ([11, 12, 13, 20, 21, 22, 23, 24, 25, 26, 27, 31, 32, 40, 41, 42, 43, 44, 45, 49, 50, 51, 52, 53, 54, 55, 56, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 78, 79, 99],)\n",
      "INFO: 08/17/2021 09:26:37 PM  Load and preprocess Test data\n",
      "INFO: 08/17/2021 09:26:37 PM Batch predict test results\n",
      "/opt/conda/lib/python3.7/site-packages/keras/engine/functional.py:585: UserWarning: Input dict contained keys ['token_type_ids'] which did not match any model input. They will be ignored by the model.\n",
      "  [n for n in tensors.keys() if n not in ref_input_names])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['f puncture wound fiinger attaching cap insulin syring used home care patient'], 'predict_proba': 0.9990896, 'predicted_class': 55}\n",
      "75687\n",
      "                                                    text  true_event  \\\n",
      "0      f puncture wound fiinger attaching cap insulin...          55   \n",
      "1      contusion lt lower leg p mvc hit car guiding c...          24   \n",
      "2      pt works quarry attempting dislodge large rock...          71   \n",
      "3      walking work twisted lt ankle later right knee...          73   \n",
      "4      c low back pain lifting box work today dx left...          71   \n",
      "...                                                  ...         ...   \n",
      "75682  coaching football collided player pain rt leg ...          12   \n",
      "75683  male using wire brush work piece got eye dx fo...          66   \n",
      "75684            lifting work back px dx thoracic strain          71   \n",
      "75685  f pt work poked lt thumb wth needle drawing bl...          55   \n",
      "75686        picking car hurt back work dx low back pain          71   \n",
      "\n",
      "       preds_encode  preds_event  \n",
      "0                25           55  \n",
      "1                 9           26  \n",
      "2                37           71  \n",
      "3                39           73  \n",
      "4                37           71  \n",
      "...             ...          ...  \n",
      "75682             1           12  \n",
      "75683            29           62  \n",
      "75684            37           71  \n",
      "75685            25           55  \n",
      "75686            37           71  \n",
      "\n",
      "[75687 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "%run ./src/evaluate_model_metrics.py --input_data './data/test' \\\n",
    "            --input_model './output/model/training-BaseBERT-08-02-58-54-2021-08-02-19-58-55-017' \\\n",
    "            --max_len 45 \\\n",
    "            --output_data './output/model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records = 6901\n",
    "num_valid_records = 767\n",
    "max_len = 45\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "valid_batch_size = 16\n",
    "steps_per_epoch = num_records // batch_size\n",
    "validation_steps = num_valid_records // valid_batch_size\n",
    "learning_rate = 5e-5\n",
    "optimizer = 'adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6901\n",
      "431\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "print(num_records)\n",
    "print(steps_per_epoch)\n",
    "print(validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input train:  ./data/train\n",
      "input valid:  ./data/valid\n",
      "train_dir :  ./data/train\n",
      "train_file :  ./data/train/train.tfrecord\n",
      "valid_dir: ./data/valid\n",
      "valid_file: ./data/valid/valid.tfrecord\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 45)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 45)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (TFBertMainLayer)          TFBaseModelOutputWit 109482240   input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 43, 512)      1180160     bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 512)          0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 256)          131328      global_max_pooling1d_8[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_332 (Dropout)           (None, 256)          0           dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 28)           7196        dropout_332[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 110,800,924\n",
      "Trainable params: 110,800,924\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "validation steps: 47\n",
      "Training....\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification_7/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification_7/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: 08/18/2021 07:33:51 PM Gradients do not exist for variables ['tf_bert_for_sequence_classification_7/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification_7/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification_7/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification_7/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: 08/18/2021 07:33:56 PM Gradients do not exist for variables ['tf_bert_for_sequence_classification_7/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification_7/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "767/767 [==============================] - 1586s 2s/step - loss: 1.1223 - acc: 0.6743 - val_loss: 1.0073 - val_acc: 0.7314\n",
      "saving model...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'odel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/injury-narrative-coding-transformers/src/train.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/injury-narrative-coding-transformers/src/train.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;31m#model.save(os.path.join(args.sm_model_dir,f\"{args.model_name}.h5\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;31m#model.save(os.path.join(args.sm_model_dir,f\"{args.model_name}\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m     \u001b[0model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"./output/model/{args.model_name}/{args.model_name}_test.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'odel' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 50min 38s, sys: 7min 39s, total: 2h 58min 17s\n",
      "Wall time: 26min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%run ./src/train.py --train ./data/train --validation ./data/valid --epochs 1 --num_records 6901 --steps_per_epoch 767 --validation_steps 47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input train:  ./data/train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 45)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 45)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (TFBertMainLayer)          TFBaseModelOutputWit 109482240   input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 43, 512)      1180160     bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 512)          0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 256)          131328      global_max_pooling1d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_215 (Dropout)           (None, 256)          0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 28)           7196        dropout_215[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 110,800,924\n",
      "Trainable params: 110,800,924\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "validation steps: 47\n",
      "Training....\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification_4/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification_4/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: 08/18/2021 06:20:12 PM Gradients do not exist for variables ['tf_bert_for_sequence_classification_4/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification_4/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification_4/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification_4/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: 08/18/2021 06:20:17 PM Gradients do not exist for variables ['tf_bert_for_sequence_classification_4/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification_4/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "767/767 [==============================] - 1589s 2s/step - loss: 1.1447 - acc: 0.6668 - val_loss: 0.9734 - val_acc: 0.7154\n",
      "saving model...\n",
      "CPU times: user 2h 50min 48s, sys: 7min 39s, total: 2h 58min 28s\n",
      "Wall time: 26min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%run ./src/train_old.py --train ./data/train  --epochs 1 --num_records 6901 --steps_per_epoch 767 --validation_steps 47"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipeline name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "timestamp = int(time.time())\n",
    "\n",
    "pipeline_name = 'BERT-pipeline-{}'.format(timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Dataset and preprocessing step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://cdc-cdh-sagemaker-s3fs-dev/projects/project006/injury-data/raw'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = 'projects/project006/injury-data/raw'\n",
    "input_data_train = sagemaker_session.upload_data(path = './data/raw',\n",
    "                                                      bucket = bucket,\n",
    "                                                      key_prefix = prefix)\n",
    "input_data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure pre-processing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 params\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\",\n",
    "    default_value=\"ml.c5.2xlarge\"\n",
    ")\n",
    "\n",
    "processing_instance_count = ParameterInteger(\n",
    "    name=\"ProcessingInstanceCount\",\n",
    "    default_value=1\n",
    ")\n",
    "\n",
    "train_percentage = ParameterFloat(\n",
    "    name=\"TrainPercentage\",\n",
    "    default_value=0.05,\n",
    ")\n",
    "\n",
    "is_sample_dataset = ParameterString(\n",
    "    name=\"SampleDataset\",\n",
    "    default_value=\"True\",\n",
    ")\n",
    "input_data = ParameterString(\n",
    "    name=\"InputData\",\n",
    "    default_value=input_data_train,\n",
    ")\n",
    "\n",
    "transformer_model = ParameterString(\n",
    "    name=\"TransformerModel\",\n",
    "    default_value='bert-base-uncased',\n",
    ")\n",
    "max_seq_length = ParameterInteger(\n",
    "    name=\"MaxSeqLength\",\n",
    "    default_value=45,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = sagemaker_session.boto_region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "processor = SKLearnProcessor(\n",
    "    framework_version='0.23-1',\n",
    "    role=role,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    env={'AWS_DEFAULT_REGION': sagemaker_session.boto_region_name},                             \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProcessingStep(name='Pre-Processing', step_type=<StepTypeEnum.PROCESSING: 'Processing'>, depends_on=None)\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "\n",
    "processing_inputs=[\n",
    "    ProcessingInput(\n",
    "        input_name='raw-input-data',\n",
    "        source=input_data,\n",
    "        destination='/opt/ml/processing/input/data/',\n",
    "        s3_data_distribution_type='ShardedByS3Key'\n",
    "    )\n",
    "]\n",
    "\n",
    "processing_outputs=[\n",
    "    ProcessingOutput(output_name='processed-train',\n",
    "                     source='/opt/ml/processing/output/processed/train',\n",
    "                     s3_upload_mode='EndOfJob'),\n",
    "    ProcessingOutput(output_name='processed-validation',\n",
    "                     source='/opt/ml/processing/output/processed/validation',\n",
    "                     s3_upload_mode='EndOfJob'),\n",
    "    ProcessingOutput(output_name='processed-test',\n",
    "                     source='/opt/ml/processing/output/processed/test',\n",
    "                     s3_upload_mode='EndOfJob'),\n",
    "]        \n",
    "\n",
    "processing_step = ProcessingStep(\n",
    "    name='Pre-Processing', \n",
    "    code='./src/pre-processing.py',\n",
    "    processor=processor,\n",
    "    inputs=processing_inputs,\n",
    "    outputs=processing_outputs,\n",
    "    job_arguments=['--train_percentage', str(train_percentage.default_value),                   \n",
    "                   '--is_sample_dataset', str(is_sample_dataset.default_value),\n",
    "                   '--max_len',str(max_seq_length.default_value),\n",
    "                   '--transformer_model',str(transformer_model.default_value)\n",
    "                  ]\n",
    ")        \n",
    "\n",
    "print(processing_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"AppSpecification\": \"<sagemaker.workflow.properties.Properties object at 0x7fd3e2c456d0>\",\n",
      "    \"AutoMLJobArn\": \"<sagemaker.workflow.properties.Properties object at 0x7fd3e2c45e50>\",\n",
      "    \"CreationTime\": \"<sagemaker.workflow.properties.Properties object at 0x7fd3e2c45dd0>\",\n",
      "    \"Environment\": \"<sagemaker.workflow.properties.Properties object at 0x7fd3e2c45850>\",\n",
      "    \"ExitMessage\": \"<sagemaker.workflow.properties.Properties object at 0x7fd3e2c45c90>\",\n",
      "    \"ExperimentConfig\": \"<sagemaker.workflow.properties.Properties object at 0x7fd3e2c45a90>\",\n",
      "    \"FailureReason\": \"<sagemaker.workflow.properties.Properties object at 0x7fd3e2c45cd0>\",\n",
      "    \"LastModifiedTime\": \"<sagemaker.workflow.properties.Properties object at 0x7fd3e2c45d90>\",\n",
      "    \"MonitoringScheduleArn\": \"<sagemaker.workflow.properties.Properties object at 0x7fd3e2c45e10>\",\n",
      "    \"NetworkConfig\": \"<sagemaker.workflow.properties.Properties object at 0x7fd3e2c45950>\",\n",
      "    \"ProcessingEndTime\": \"<sagemaker.workflow.properties.Properties object at 0x7fd3e2c45d10>\",\n",
      "    \"ProcessingInputs\": \"<sagemaker.workflow.properties.PropertiesList object at 0x7fd3e2c454d0>\",\n",
      "    \"ProcessingJobArn\": \"<sagemaker.workflow.properties.Properties object at 0x7fd3e2c45b50>\",\n",
      "    \"ProcessingJobName\": \"<sagemaker.workflow.properties.Properties object at 0x7fd3e2c45550>\",\n",
      "    \"ProcessingJobStatus\": \"<sagemaker.workflow.properties.Properties object at 0x7fd3e2c45c50>\",\n",
      "    \"ProcessingOutputConfig\": \"<sagemaker.workflow.properties.Properties object at 0x7fd3e2c45510>\",\n",
      "    \"ProcessingResources\": \"<sagemaker.workflow.properties.Properties object at 0x7fd3e2c45610>\",\n",
      "    \"ProcessingStartTime\": \"<sagemaker.workflow.properties.Properties object at 0x7fd3e2c45d50>\",\n",
      "    \"RoleArn\": \"<sagemaker.workflow.properties.Properties object at 0x7fd3e2c45990>\",\n",
      "    \"StoppingCondition\": \"<sagemaker.workflow.properties.Properties object at 0x7fd3e2c45650>\",\n",
      "    \"TrainingJobArn\": \"<sagemaker.workflow.properties.Properties object at 0x7fd3e2c45e90>\",\n",
      "    \"_path\": \"Steps.Pre-Processing\",\n",
      "    \"_shape_names\": [\n",
      "        \"DescribeProcessingJobResponse\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# print out the list of the processing job properties\n",
    "print(json.dumps(\n",
    "    processing_step.properties.__dict__,\n",
    "    indent=4, sort_keys=True, default=str\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"AppManaged\": \"<sagemaker.workflow.properties.Properties object at 0x7fd3e9e5b250>\",\n",
      "    \"FeatureStoreOutput\": \"<sagemaker.workflow.properties.Properties object at 0x7fd3e9e5b050>\",\n",
      "    \"OutputName\": \"<sagemaker.workflow.properties.Properties object at 0x7fd3e9e5b0d0>\",\n",
      "    \"S3Output\": \"<sagemaker.workflow.properties.Properties object at 0x7fd3e9e5b190>\",\n",
      "    \"_path\": \"Steps.Pre-Processing.ProcessingOutputConfig.Outputs['processed-train']\",\n",
      "    \"_shape_names\": [\n",
      "        \"ProcessingOutput\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(\n",
    "    processing_step.properties.ProcessingOutputConfig.Outputs['processed-train'].__dict__, \n",
    "    indent=4, sort_keys=True, default=str\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"__str__\": \"S3Uri\",\n",
      "    \"_path\": \"Steps.Pre-Processing.ProcessingOutputConfig.Outputs['processed-train'].S3Output.S3Uri\",\n",
      "    \"_shape_names\": [\n",
      "        \"S3Uri\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(\n",
    "    processing_step.properties.ProcessingOutputConfig.Outputs['processed-train'].S3Output.S3Uri.__dict__,\n",
    "    indent=4, sort_keys=True, default=str\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12 params\n",
    "\n",
    "epochs = ParameterInteger(\n",
    "    name=\"Epochs\",\n",
    "    default_value=3\n",
    ")\n",
    "\n",
    "num_records = ParameterInteger(\n",
    "    name=\"NumRecords\"\n",
    ")\n",
    "   \n",
    "\n",
    "learning_rate = ParameterFloat(\n",
    "    name=\"LearningRate\",\n",
    "    default_value=5e-5\n",
    ") \n",
    "    \n",
    "train_batch_size = ParameterInteger(\n",
    "    name=\"TrainBatchSize\",\n",
    "    default_value=16\n",
    ")\n",
    "\n",
    "train_steps_per_epoch = ParameterInteger(\n",
    "    name=\"TrainStepsPerEpoch\",\n",
    "    default_value=500\n",
    ")\n",
    "\n",
    "validation_batch_size = ParameterInteger(\n",
    "    name=\"ValidationBatchSize\",\n",
    "    default_value=16\n",
    ")\n",
    "\n",
    "validation_steps_per_epoch = ParameterInteger(\n",
    "    name=\"ValidationStepsPerEpoch\",\n",
    "    default_value=500\n",
    ")\n",
    "\n",
    "\n",
    "train_instance_count = ParameterInteger(\n",
    "    name=\"TrainInstanceCount\",\n",
    "    default_value=1\n",
    ")\n",
    "\n",
    "train_instance_type = ParameterString(\n",
    "    name=\"TrainInstanceType\",\n",
    "    default_value=\"ml.p3.2xlarge\"\n",
    ")\n",
    "\n",
    "\n",
    "max_seq_length = ParameterInteger(\n",
    "    name=\"MaxSeqLength\",\n",
    "    default_value=45,\n",
    ")\n",
    "\n",
    "optimizer = ParameterString(\n",
    "    name=\"optimizer\",\n",
    "    default_value='Adam'\n",
    ")\n",
    "\n",
    "input_mode = ParameterString(\n",
    "    name=\"InputMode\",\n",
    "    default_value=\"File\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters={\n",
    "    'max_seq_length': max_seq_length,\n",
    "    'epochs': epochs,\n",
    "    'num_records':num_records,\n",
    "    'learning_rate': learning_rate,\n",
    "    'batch_size': train_batch_size,\n",
    "    'steps_per_epoch': train_steps_per_epoch,\n",
    "    'validation_batch_size': validation_batch_size,\n",
    "    'validation_steps': validation_steps_per_epoch,\n",
    "    'optimizer':optimizer\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = [{'Name':'train:loss','Regex':'loss: ([0-9\\\\.]+)'},\n",
    "                                    {'Name':'train:accuracy','Regex':'acc: ([0-9\\\\.]+)'},\n",
    "                                    {'Name':'validation:loss','Regex':'val_loss: ([0-9\\\\.]+)'},\n",
    "                                    {'Name':'validation:accuracy','Regex':'val_acc: ([0-9\\\\.]+)'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "estimator = HuggingFace(\n",
    "        entry_point=\"train.py\",\n",
    "        source_dir = \"./src/\",\n",
    "        role=role,\n",
    "        instance_count=train_instance_count,\n",
    "        volume_size = 50,\n",
    "        max_run = 18000,\n",
    "        instance_type=train_instance_type,\n",
    "        transformers_version = \"4.4\",\n",
    "        tensorflow_version  = \"2.4\",\n",
    "        py_version=\"py37\",\n",
    "        hyperparameters = hyperparameters,\n",
    "        metric_definitions = metric_definitions,\n",
    "        enable_sagemaker_metrics = True\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import CacheConfig\n",
    "\n",
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"PT1H\") # PT1H represents `one hour`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingStep(name='Train', step_type=<StepTypeEnum.TRAINING: 'Training'>, depends_on=None)\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "training_step = TrainingStep(\n",
    "    name='Train',\n",
    "    estimator=estimator,\n",
    "    inputs={\n",
    "        'train': TrainingInput(\n",
    "            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                'processed-train'\n",
    "            ].S3Output.S3Uri\n",
    "        ),\n",
    "        'validation': TrainingInput(\n",
    "            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                'processed-validation'\n",
    "            ].S3Output.S3Uri\n",
    "        )\n",
    "    },\n",
    "    cache_config=cache_config\n",
    ")\n",
    "\n",
    "print(training_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "evaluation_processor = SKLearnProcessor(\n",
    "    framework_version='0.23-1',\n",
    "    role=role,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    env={'AWS_DEFAULT_REGION': region},\n",
    "    max_runtime_in_seconds=7200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name='EvaluationReport',\n",
    "    output_name='metrics',\n",
    "    path='evaluation.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "evaluation_step = ProcessingStep(\n",
    "    name='EvaluateModel',\n",
    "    processor=evaluation_processor,\n",
    "    code='src/evaluate_model_metrics.py',\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination='/opt/ml/processing/input/model'\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=processing_step.properties.ProcessingOutputConfig.Outputs['processed-test'].S3Output.S3Uri,\n",
    "            destination='/opt/ml/processing/input/data'\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name='metrics', \n",
    "                         s3_upload_mode='EndOfJob',\n",
    "                         source='/opt/ml/processing/output/metrics/'),\n",
    "    ],\n",
    "    job_arguments=[\n",
    "        '--max_len', str(max_seq_length.default_value),\n",
    "    ],\n",
    "    property_files=[evaluation_report],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register model step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 parameters\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\",\n",
    "    default_value=\"PendingManualApproval\"\n",
    ")\n",
    "\n",
    "deploy_instance_type = ParameterString(\n",
    "    name=\"DeployInstanceType\",\n",
    "    default_value=\"ml.m5.large\"\n",
    ")\n",
    "\n",
    "deploy_instance_count = ParameterInteger(\n",
    "    name=\"DeployInstanceCount\",\n",
    "    default_value=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_group_name = f\"BERT-Reviews-{timestamp}\"\n",
    "\n",
    "print(model_package_group_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics \n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=\"{}/evaluation.json\".format(\n",
    "            evaluation_step.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "        ),\n",
    "        content_type=\"application/json\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define deployment image for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"tensorflow\",\n",
    "    region=region,\n",
    "    version=\"2.4.0\",\n",
    "    py_version=\"py37\",\n",
    "    instance_type=deploy_instance_type,\n",
    "    image_scope=\"inference\"\n",
    ")\n",
    "print(inference_image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "\n",
    "register_step = RegisterModel(\n",
    "    name=\"RegisterModel\",\n",
    "    estimator=estimator,\n",
    "    image_uri=inference_image_uri, # Replace None \n",
    "    model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"application/jsonlines\"],\n",
    "    response_types=[\"application/jsonlines\"],\n",
    "    inference_instances=[deploy_instance_type],\n",
    "    transform_instances=[deploy_instance_type], # batch transform is not used in this lab\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    "    model_metrics=model_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "\n",
    "model_name = 'bert-model-{}'.format(timestamp)\n",
    "\n",
    "model = Model(\n",
    "    name=model_name,\n",
    "    image_uri=inference_image_uri, # Replace None\n",
    "    model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import CreateModelInput\n",
    "\n",
    "create_inputs = CreateModelInput(\n",
    "    instance_type=deploy_instance_type, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "\n",
    "create_step = CreateModelStep(\n",
    "    name=\"CreateModel\",\n",
    "    model=model, # Replace None\n",
    "    inputs=create_inputs, # Replace None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Check accuracy condition step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_accuracy_value = ParameterFloat(\n",
    "    name=\"MinAccuracyValue\",\n",
    "    default_value=0.85 # random choice from three classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import (\n",
    "    ConditionStep,\n",
    "    JsonGet,\n",
    ")\n",
    "\n",
    "minimum_accuracy_condition = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step=evaluation_step,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"metrics.accuracy.value\",\n",
    "    ),\n",
    "    right=min_accuracy_value # minimum accuracy threshold\n",
    ")\n",
    "\n",
    "minimum_accuracy_condition_step = ConditionStep(\n",
    "    name=\"AccuracyCondition\",\n",
    "    conditions=[minimum_accuracy_condition],\n",
    "    if_steps=[register_step, create_step], # successfully exceeded or equaled the minimum accuracy, continue with model registration\n",
    "    else_steps=[], # did not exceed the minimum accuracy, the model will not be registered\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Create pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        input_data,\n",
    "        processing_instance_count,\n",
    "        processing_instance_type,\n",
    "        max_seq_length,\n",
    "        is_sample_dataset,\n",
    "        transformer_model,\n",
    "        train_percentage,\n",
    "        \n",
    "        epochs,\n",
    "        num_records,\n",
    "        learning_rate,\n",
    "        optimizer,\n",
    "        train_batch_size,\n",
    "        train_steps_per_epoch,\n",
    "        validation_batch_size,\n",
    "        validation_steps_per_epoch,\n",
    "        input_mode,\n",
    "        train_instance_count,\n",
    "        train_instance_type,   \n",
    "        \n",
    "        min_accuracy_value,\n",
    "        \n",
    "        model_approval_status,\n",
    "        deploy_instance_type,\n",
    "        deploy_instance_count\n",
    "    ],\n",
    "    steps=[processing_step, training_step, evaluation_step, minimum_accuracy_condition_step],\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "\n",
    "pprint(definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pipeline.create(role_arn=role)\n",
    "\n",
    "pipeline_arn = response[\"PipelineArn\"]\n",
    "print(pipeline_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start(\n",
    "    parameters=dict(\n",
    "        InputData=raw_input_data_s3_uri,\n",
    "        ProcessingInstanceCount=1,\n",
    "        ProcessingInstanceType='ml.c5.2xlarge',\n",
    "        MaxSeqLength=45,\n",
    "        SampleDataset='True',\n",
    "        TransformerModel = 'bert-based-uncased',\n",
    "        TrainPercentage=0.9,\n",
    "        Epochs=3,\n",
    "        num_records = 138549,\n",
    "        LearningRate=5e-5,\n",
    "        optimizer = 'Adam'\n",
    "        TrainBatchSize=16,,\n",
    "        TrainStepsPerEpoch=50,\n",
    "        ValidationBatchSize=16,\n",
    "        ValidationStepsPerEpoch=64,\n",
    "        InputMode= 'File',\n",
    "        TrainInstanceCount=1,\n",
    "        TrainInstanceType='ml.p3.2xlarge',\n",
    "        MinAccuracyValue=0.75,\n",
    "        ModelApprovalStatus='PendingManualApproval', \n",
    "        DeployInstanceType='ml.m5.large',\n",
    "        DeployInstanceCount=1 \n",
    "    )\n",
    ")\n",
    "\n",
    "print(execution.arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
