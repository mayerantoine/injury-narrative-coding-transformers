{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a SageMaker Pipeline to train and register the injury narrative BERT classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install tensorflow\n",
    "!pip install transformers\n",
    "!pip install nltk\n",
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import activations, optimizers, losses\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import joblib \n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-us-east-1-979294212144'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "bucket = default_bucket\n",
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::979294212144:role/service-role/AmazonSageMaker-ExecutionRole-20210423T122185'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 08/25/2021 08:42:58 PM Start.....\n",
      "INFO: 08/25/2021 08:42:58 PM Parsing arguments\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "INFO: 08/25/2021 08:42:59 PM Getting and splitting data\n",
      "INFO: 08/25/2021 08:42:59 PM nb classes in final data:28\n",
      "INFO: 08/25/2021 08:42:59 PM  X (7668,) , y : (7668,)\n",
      "INFO: 08/25/2021 08:42:59 PM X_train shape (6901,) y_train shape : (6901,)\n",
      "INFO: 08/25/2021 08:42:59 PM X_valid shape (767,) y_valid shape : (767,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 08/25/2021 08:42:59 PM Preprocessing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb classes in final data: 28\n",
      "test_data_small.shape (3768, 2)\n",
      "[62, 71, 63, 11, 43, 55, 42, 52, 60, 73, 13, 66, 12, 53, 64, 27, 24, 99, 26, 72, 70, 51, 44, 41, 31, 78, 32, 23]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 08/25/2021 08:43:01 PM Tokenization and encoding...\n",
      "INFO: 08/25/2021 08:43:02 PM Encoding Labels .....\n",
      "INFO: 08/25/2021 08:43:02 PM Create TF Dataset....\n",
      "INFO: 08/25/2021 08:43:02 PM Saving train and valid TF Records...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/injury-narrative-coding-transformers/src/pre-processing.py:243: TFRecordWriter.__init__ (from tensorflow.python.data.experimental.ops.writers) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To write TFRecords to disk, use `tf.io.TFRecordWriter`. To save and load the contents of a dataset, use `tf.data.experimental.save` and `tf.data.experimental.load`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: 08/25/2021 08:43:02 PM From /root/injury-narrative-coding-transformers/src/pre-processing.py:243: TFRecordWriter.__init__ (from tensorflow.python.data.experimental.ops.writers) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To write TFRecords to disk, use `tf.io.TFRecordWriter`. To save and load the contents of a dataset, use `tf.data.experimental.save` and `tf.data.experimental.load`\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "/opt/ml/processing/output/processed/validation/valid.tfrecord; No such file or directory [Op:DatasetToTFRecord]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/injury-narrative-coding-transformers/src/pre-processing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/injury-narrative-coding-transformers/src/pre-processing.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;31m#validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m     \u001b[0m_save_feature_as_tfrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_tfdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'/opt/ml/processing/output/processed/validation/valid.tfrecord'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m     \u001b[0;31m#_save_feature_as_tfrecord(valid_tfdataset,'./data/valid/valid.tfrecord')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/injury-narrative-coding-transformers/src/pre-processing.py\u001b[0m in \u001b[0;36m_save_feature_as_tfrecord\u001b[0;34m(tfdataset, file_path)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFRecordWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized_tfdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/data/experimental/ops/writers.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_debug_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     return gen_experimental_dataset_ops.dataset_to_tf_record(\n\u001b[0;32m--> 126\u001b[0;31m         dataset._variant_tensor, self._filename, self._compression_type)\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/gen_experimental_dataset_ops.py\u001b[0m in \u001b[0;36mdataset_to_tf_record\u001b[0;34m(input_dataset, filename, compression_type, name)\u001b[0m\n\u001b[1;32m   1300\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1303\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6939\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6940\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6941\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6942\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: /opt/ml/processing/output/processed/validation/valid.tfrecord; No such file or directory [Op:DatasetToTFRecord]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 24s, sys: 597 ms, total: 1min 25s\n",
      "Wall time: 1min 41s\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "#%run ./src/pre-processing.py --data_path ./data/raw  --train_percentage 0.05 --is_sample_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run ./src/evaluate_model_metrics.py --input_data './data/test' \\\n",
    "#            --input_model './output/model/training-BaseBERT-08-02-58-54-2021-08-02-19-58-55-017' \\\n",
    "#            --max_len 45 \\\n",
    "#            --output_data './output/model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records = 6901\n",
    "num_valid_records = 767\n",
    "max_len = 45\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "valid_batch_size = 16\n",
    "steps_per_epoch = num_records // batch_size\n",
    "validation_steps = num_valid_records // valid_batch_size\n",
    "learning_rate = 5e-5\n",
    "optimizer = 'adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6901\n",
      "431\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "print(num_records)\n",
    "print(steps_per_epoch)\n",
    "print(validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input train:  ./data/train\n",
      "input valid:  ./data/valid\n",
      "loading data...\n",
      "train_dir :  ./data/train\n",
      "train_file :  ./data/train/train.tfrecord\n",
      "valid_dir: ./data/valid\n",
      "valid_file: ./data/valid/valid.tfrecord\n",
      "loading encoder...\n",
      "Building model...\n",
      "[2021-08-20 13:34:04.000 tensorflow-2-3-gpu--ml-g4dn-xlarge-c85184389676cdfa7bdf06745c9b:69 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2021-08-20 13:34:04.025 tensorflow-2-3-gpu--ml-g4dn-xlarge-c85184389676cdfa7bdf06745c9b:69 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 45)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 45)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (TFBertMainLayer)          TFBaseModelOutputWit 109482240   input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 43, 512)      1180160     bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 512)          0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          131328      global_max_pooling1d[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 43)           11051       dropout_38[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 110,804,779\n",
      "Trainable params: 110,804,779\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "validation steps: 962\n",
      "Training....\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: 08/20/2021 01:34:14 PM Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: 08/20/2021 01:34:14 PM Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: 08/20/2021 01:34:19 PM Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: 08/20/2021 01:34:19 PM Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8659/8659 [==============================] - 2472s 285ms/step - loss: 0.6467 - acc: 0.8075 - val_loss: 0.5283 - val_acc: 0.8363\n",
      "Epoch 2/5\n",
      "8659/8659 [==============================] - 2476s 286ms/step - loss: 0.2849 - acc: 0.9075 - val_loss: 0.5380 - val_acc: 0.8467\n",
      "Epoch 3/5\n",
      "8659/8659 [==============================] - 2480s 286ms/step - loss: 0.1613 - acc: 0.9480 - val_loss: 0.5681 - val_acc: 0.8442\n",
      "Epoch 4/5\n",
      "8659/8659 [==============================] - 2481s 287ms/step - loss: 0.1400 - acc: 0.9551 - val_loss: 0.5681 - val_acc: 0.8442\n",
      "Epoch 5/5\n",
      "8659/8659 [==============================] - 2479s 286ms/step - loss: 0.1380 - acc: 0.9554 - val_loss: 0.5681 - val_acc: 0.8442\n",
      "saving model...\n",
      "CPU times: user 2h 6min 26s, sys: 55min 53s, total: 3h 2min 20s\n",
      "Wall time: 3h 28min 20s\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "#%run ./src/train.py --train ./data/train --validation ./data/valid --epochs 5 --num_records 138549 --steps_per_epoch 8659 --validation_steps 962"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipeline name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "timestamp = int(time.time())\n",
    "\n",
    "pipeline_name = 'BaseBERT-Injury-Coding-pipeline-{}'.format(timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Dataset and preprocessing step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-979294212144/injury-data/raw'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = 'injury-data/raw'\n",
    "input_data_train = sagemaker_session.upload_data(path = './data/raw',\n",
    "                                                      bucket = bucket,\n",
    "                                                      key_prefix = prefix)\n",
    "input_data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure pre-processing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 params\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\",\n",
    "    default_value=\"ml.c5.2xlarge\"\n",
    ")\n",
    "\n",
    "processing_instance_count = ParameterInteger(\n",
    "    name=\"ProcessingInstanceCount\",\n",
    "    default_value=1\n",
    ")\n",
    "\n",
    "train_percentage = ParameterFloat(\n",
    "    name=\"TrainPercentage\",\n",
    "    default_value=0.05\n",
    ")\n",
    "\n",
    "is_sample_dataset = ParameterString(\n",
    "    name=\"SampleDataset\",\n",
    "    default_value=\"True\"\n",
    ")\n",
    "input_data = ParameterString(\n",
    "    name=\"InputData\",\n",
    "    default_value=input_data_train\n",
    ")\n",
    "\n",
    "transformer_model = ParameterString(\n",
    "    name=\"TransformerModel\",\n",
    "    default_value='bert-base-uncased'\n",
    ")\n",
    "max_seq_length = ParameterInteger(\n",
    "    name=\"MaxSeqLength\",\n",
    "    default_value=45\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = sagemaker_session.boto_region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:2.4.1-cpu-py37\n"
     ]
    }
   ],
   "source": [
    "processing_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"tensorflow\",\n",
    "    region=region,\n",
    "    version=\"2.4.1\",\n",
    "    py_version=\"py37\",\n",
    "    instance_type=processing_instance_type,\n",
    "    image_scope=\"training\"\n",
    ")\n",
    "print(processing_image_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 08/25/2021 08:56:43 PM Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO: 08/25/2021 08:56:43 PM Defaulting to only available Python version: py3\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor,ScriptProcessor\n",
    "\n",
    "processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    role=role,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    env={'AWS_DEFAULT_REGION': sagemaker_session.boto_region_name},     \n",
    "     max_runtime_in_seconds=18000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProcessingStep(name='Pre-Processing', step_type=<StepTypeEnum.PROCESSING: 'Processing'>, depends_on=None)\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "\n",
    "processing_inputs=[\n",
    "    ProcessingInput(\n",
    "        input_name='raw-input-data',\n",
    "        source=input_data,\n",
    "        destination='/opt/ml/processing/input/data/',\n",
    "        s3_data_distribution_type='ShardedByS3Key'\n",
    "    )\n",
    "]\n",
    "\n",
    "processing_outputs=[\n",
    "    ProcessingOutput(output_name='processed-train',\n",
    "                     source='/opt/ml/processing/output/processed/train',\n",
    "                     s3_upload_mode='EndOfJob'),\n",
    "    ProcessingOutput(output_name='processed-validation',\n",
    "                     source='/opt/ml/processing/output/processed/validation',\n",
    "                     s3_upload_mode='EndOfJob'),\n",
    "    ProcessingOutput(output_name='processed-test',\n",
    "                     source='/opt/ml/processing/output/processed/test',\n",
    "                     s3_upload_mode='EndOfJob'),\n",
    "]        \n",
    "\n",
    "processing_step = ProcessingStep(\n",
    "    name='Pre-Processing', \n",
    "    code='./src/pre-processing.py',\n",
    "    processor=processor,\n",
    "    inputs=processing_inputs,\n",
    "    outputs=processing_outputs,\n",
    "    job_arguments=['--train_percentage', str(train_percentage.default_value),                   \n",
    "                   '--max_len',str(max_seq_length.default_value),\n",
    "                   '--transformer_model',str(transformer_model.default_value),\n",
    "                   '--is_sample_dataset'\n",
    "                  ]\n",
    ")        \n",
    "\n",
    "print(processing_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"AppSpecification\": \"<sagemaker.workflow.properties.Properties object at 0x7f5efe106690>\",\n",
      "    \"AutoMLJobArn\": \"<sagemaker.workflow.properties.Properties object at 0x7f5efcb5c690>\",\n",
      "    \"CreationTime\": \"<sagemaker.workflow.properties.Properties object at 0x7f5efcb5c710>\",\n",
      "    \"Environment\": \"<sagemaker.workflow.properties.Properties object at 0x7f5efe106390>\",\n",
      "    \"ExitMessage\": \"<sagemaker.workflow.properties.Properties object at 0x7f5efcb5cdd0>\",\n",
      "    \"ExperimentConfig\": \"<sagemaker.workflow.properties.Properties object at 0x7f5efe106dd0>\",\n",
      "    \"FailureReason\": \"<sagemaker.workflow.properties.Properties object at 0x7f5efcb5c9d0>\",\n",
      "    \"LastModifiedTime\": \"<sagemaker.workflow.properties.Properties object at 0x7f5efcb5c950>\",\n",
      "    \"MonitoringScheduleArn\": \"<sagemaker.workflow.properties.Properties object at 0x7f5efcb5c290>\",\n",
      "    \"NetworkConfig\": \"<sagemaker.workflow.properties.Properties object at 0x7f5efe106190>\",\n",
      "    \"ProcessingEndTime\": \"<sagemaker.workflow.properties.Properties object at 0x7f5efcb5c450>\",\n",
      "    \"ProcessingInputs\": \"<sagemaker.workflow.properties.PropertiesList object at 0x7f5efe1068d0>\",\n",
      "    \"ProcessingJobArn\": \"<sagemaker.workflow.properties.Properties object at 0x7f5efe6da0d0>\",\n",
      "    \"ProcessingJobName\": \"<sagemaker.workflow.properties.Properties object at 0x7f5efe106850>\",\n",
      "    \"ProcessingJobStatus\": \"<sagemaker.workflow.properties.Properties object at 0x7f5efcb5c890>\",\n",
      "    \"ProcessingOutputConfig\": \"<sagemaker.workflow.properties.Properties object at 0x7f5efe106890>\",\n",
      "    \"ProcessingResources\": \"<sagemaker.workflow.properties.Properties object at 0x7f5efe106790>\",\n",
      "    \"ProcessingStartTime\": \"<sagemaker.workflow.properties.Properties object at 0x7f5efcb5c190>\",\n",
      "    \"RoleArn\": \"<sagemaker.workflow.properties.Properties object at 0x7f5efe106110>\",\n",
      "    \"StoppingCondition\": \"<sagemaker.workflow.properties.Properties object at 0x7f5efe106750>\",\n",
      "    \"TrainingJobArn\": \"<sagemaker.workflow.properties.Properties object at 0x7f5efcb5ced0>\",\n",
      "    \"_path\": \"Steps.Pre-Processing\",\n",
      "    \"_shape_names\": [\n",
      "        \"DescribeProcessingJobResponse\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# print out the list of the processing job properties\n",
    "print(json.dumps(\n",
    "    processing_step.properties.__dict__,\n",
    "    indent=4, sort_keys=True, default=str\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"AppManaged\": \"<sagemaker.workflow.properties.Properties object at 0x7f5efe1062d0>\",\n",
      "    \"FeatureStoreOutput\": \"<sagemaker.workflow.properties.Properties object at 0x7f5efe1063d0>\",\n",
      "    \"OutputName\": \"<sagemaker.workflow.properties.Properties object at 0x7f5efe106c90>\",\n",
      "    \"S3Output\": \"<sagemaker.workflow.properties.Properties object at 0x7f5efe106450>\",\n",
      "    \"_path\": \"Steps.Pre-Processing.ProcessingOutputConfig.Outputs['processed-train']\",\n",
      "    \"_shape_names\": [\n",
      "        \"ProcessingOutput\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(\n",
    "    processing_step.properties.ProcessingOutputConfig.Outputs['processed-train'].__dict__, \n",
    "    indent=4, sort_keys=True, default=str\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"__str__\": \"S3Uri\",\n",
      "    \"_path\": \"Steps.Pre-Processing.ProcessingOutputConfig.Outputs['processed-train'].S3Output.S3Uri\",\n",
      "    \"_shape_names\": [\n",
      "        \"S3Uri\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(\n",
    "    processing_step.properties.ProcessingOutputConfig.Outputs['processed-train'].S3Output.S3Uri.__dict__,\n",
    "    indent=4, sort_keys=True, default=str\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12 params\n",
    "\n",
    "epochs = ParameterInteger(\n",
    "    name=\"Epochs\",\n",
    "    default_value=3\n",
    ")\n",
    "\n",
    "num_records = ParameterInteger(\n",
    "    name=\"NumRecords\",\n",
    "    default_value = 6901\n",
    ")\n",
    "   \n",
    "\n",
    "learning_rate = ParameterFloat(\n",
    "    name=\"LearningRate\",\n",
    "    default_value=5e-5\n",
    ") \n",
    "    \n",
    "train_batch_size = ParameterInteger(\n",
    "    name=\"TrainBatchSize\",\n",
    "    default_value=16\n",
    ")\n",
    "\n",
    "train_steps_per_epoch = ParameterInteger(\n",
    "    name=\"TrainStepsPerEpoch\",\n",
    "    default_value=431\n",
    ")\n",
    "\n",
    "validation_batch_size = ParameterInteger(\n",
    "    name=\"ValidationBatchSize\",\n",
    "    default_value=16\n",
    ")\n",
    "\n",
    "validation_steps_per_epoch = ParameterInteger(\n",
    "    name=\"ValidationStepsPerEpoch\",\n",
    "    default_value=47\n",
    ")\n",
    "\n",
    "\n",
    "train_instance_count = ParameterInteger(\n",
    "    name=\"TrainInstanceCount\",\n",
    "    default_value=1\n",
    ")\n",
    "\n",
    "train_instance_type = ParameterString(\n",
    "    name=\"TrainInstanceType\",\n",
    "    default_value=\"ml.p3.2xlarge\"\n",
    ")\n",
    "\n",
    "\n",
    "max_seq_length = ParameterInteger(\n",
    "    name=\"MaxSeqLength\",\n",
    "    default_value=45\n",
    ")\n",
    "\n",
    "optimizer = ParameterString(\n",
    "    name=\"optimizer\",\n",
    "    default_value='Adam'\n",
    ")\n",
    "\n",
    "input_mode = ParameterString(\n",
    "    name=\"InputMode\",\n",
    "    default_value=\"File\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch_size.default_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters={\n",
    "    'max_seq_length': max_seq_length.default_value,\n",
    "    'epochs': epochs.default_value,\n",
    "    'num_records': num_records.default_value,\n",
    "    'learning_rate': learning_rate.default_value,\n",
    "    'batch_size': train_batch_size.default_value,\n",
    "    'steps_per_epoch': train_steps_per_epoch.default_value,\n",
    "    'validation_batch_size': validation_batch_size.default_value,\n",
    "    'validation_steps': validation_steps_per_epoch.default_value,\n",
    "    'optimizer': optimizer.default_value\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = [{'Name':'train:loss','Regex':'loss: ([0-9\\\\.]+)'},\n",
    "                                    {'Name':'train:accuracy','Regex':'acc: ([0-9\\\\.]+)'},\n",
    "                                    {'Name':'validation:loss','Regex':'val_loss: ([0-9\\\\.]+)'},\n",
    "                                    {'Name':'validation:accuracy','Regex':'val_acc: ([0-9\\\\.]+)'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "estimator = HuggingFace(\n",
    "        entry_point=\"train.py\",\n",
    "        source_dir = \"./src/\",\n",
    "        role=role,\n",
    "        instance_count=train_instance_count.default_value,\n",
    "        volume_size = 50,\n",
    "        max_run = 18000,\n",
    "        instance_type=train_instance_type.default_value,\n",
    "        transformers_version = \"4.4\",\n",
    "        tensorflow_version  = \"2.4\",\n",
    "        py_version=\"py37\",\n",
    "        input_mode = input_mode.default_value,\n",
    "        hyperparameters = hyperparameters,\n",
    "        metric_definitions = metric_definitions,\n",
    "        enable_sagemaker_metrics = True\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import CacheConfig\n",
    "\n",
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"PT1H\") # PT1H represents `one hour`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingStep(name='Train', step_type=<StepTypeEnum.TRAINING: 'Training'>, depends_on=None)\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "training_step = TrainingStep(\n",
    "    name='Train',\n",
    "    estimator=estimator,\n",
    "    inputs={\n",
    "        'train': TrainingInput(\n",
    "            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                'processed-train'\n",
    "            ].S3Output.S3Uri\n",
    "        ),\n",
    "        'validation': TrainingInput(\n",
    "            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                'processed-validation'\n",
    "            ].S3Output.S3Uri\n",
    "        )\n",
    "    },\n",
    "    cache_config=cache_config\n",
    ")\n",
    "\n",
    "print(training_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 08/25/2021 08:56:48 PM Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO: 08/25/2021 08:56:48 PM Defaulting to only available Python version: py3\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor,ScriptProcessor\n",
    "\n",
    "evaluation_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    role=role,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    env={'AWS_DEFAULT_REGION': region},\n",
    "    max_runtime_in_seconds=7200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name='EvaluationReport',\n",
    "    output_name='metrics',\n",
    "    path='evaluation.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "evaluation_step = ProcessingStep(\n",
    "    name='EvaluateModel',\n",
    "    processor=evaluation_processor,\n",
    "    code='./src/evaluate_model_metrics.py',\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination='/opt/ml/processing/input/model'\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=processing_step.properties.ProcessingOutputConfig.Outputs['processed-test'].S3Output.S3Uri,\n",
    "            destination='/opt/ml/processing/input/data'\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name='metrics', \n",
    "                         s3_upload_mode='EndOfJob',\n",
    "                         source='/opt/ml/processing/output/metrics/'),\n",
    "    ],\n",
    "    job_arguments=[\n",
    "        '--max_len', str(max_seq_length.default_value)\n",
    "    ],\n",
    "    property_files=[evaluation_report],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register model step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 parameters\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\",\n",
    "    default_value=\"PendingManualApproval\"\n",
    ")\n",
    "\n",
    "deploy_instance_type = ParameterString(\n",
    "    name=\"DeployInstanceType\",\n",
    "    default_value=\"ml.m5.large\"\n",
    ")\n",
    "\n",
    "deploy_instance_count = ParameterInteger(\n",
    "    name=\"DeployInstanceCount\",\n",
    "    default_value=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseBERT-Injury-Coding-1629925000\n"
     ]
    }
   ],
   "source": [
    "model_package_group_name = f\"BaseBERT-Injury-Coding-{timestamp}\"\n",
    "\n",
    "print(model_package_group_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sagemaker.model_metrics.ModelMetrics object at 0x7f5efd2ac110>\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics \n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=\"{}/evaluation.json\".format(\n",
    "            evaluation_step.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "        ),\n",
    "        content_type=\"application/json\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define deployment image for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 08/25/2021 08:56:51 PM Ignoring unnecessary Python version: py37.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.4.1-cpu\n"
     ]
    }
   ],
   "source": [
    "inference_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"tensorflow\",\n",
    "    region=region,\n",
    "    version=\"2.4.1\",\n",
    "    py_version=\"py37\",\n",
    "    instance_type=deploy_instance_type,\n",
    "    image_scope=\"inference\"\n",
    ")\n",
    "print(inference_image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "\n",
    "register_step = RegisterModel(\n",
    "    name=\"RegisterModel\",\n",
    "    estimator=estimator,\n",
    "    image_uri=inference_image_uri, # Replace None \n",
    "    model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"application/jsonlines\"],\n",
    "    response_types=[\"application/jsonlines\"],\n",
    "    inference_instances=[deploy_instance_type],\n",
    "    transform_instances=[deploy_instance_type], # batch transform is not used in this lab\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    "    model_metrics=model_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "\n",
    "model_name = 'bert-model-{}'.format(timestamp)\n",
    "\n",
    "model = Model(\n",
    "    name=model_name,\n",
    "    image_uri=inference_image_uri, # Replace None\n",
    "    model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import CreateModelInput\n",
    "\n",
    "create_inputs = CreateModelInput(\n",
    "    instance_type=deploy_instance_type, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "\n",
    "create_step = CreateModelStep(\n",
    "    name=\"CreateModel\",\n",
    "    model=model, # Replace None\n",
    "    inputs=create_inputs, # Replace None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Check accuracy condition step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_accuracy_value = ParameterFloat(\n",
    "    name=\"MinAccuracyValue\",\n",
    "    default_value=0.75 # random choice from three classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: 08/25/2021 08:56:55 PM The class JsonGet has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import (\n",
    "    ConditionStep,\n",
    "    JsonGet,\n",
    ")\n",
    "\n",
    "minimum_accuracy_condition = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step=evaluation_step,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"metrics.accuracy.value\",\n",
    "    ),\n",
    "    right=min_accuracy_value # minimum accuracy threshold\n",
    ")\n",
    "\n",
    "minimum_accuracy_condition_step = ConditionStep(\n",
    "    name=\"AccuracyCondition\",\n",
    "    conditions=[minimum_accuracy_condition],\n",
    "    if_steps=[register_step, create_step], # successfully exceeded or equaled the minimum accuracy, continue with model registration\n",
    "    else_steps=[], # did not exceed the minimum accuracy, the model will not be registered\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Create pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[ \n",
    "        input_data,        \n",
    "        processing_instance_count, \n",
    "        processing_instance_type, \n",
    "        max_seq_length, \n",
    "        is_sample_dataset, \n",
    "        transformer_model, \n",
    "        train_percentage,   \n",
    "        epochs, \n",
    "        num_records, \n",
    "        learning_rate, \n",
    "        optimizer, \n",
    "        train_batch_size, \n",
    "        train_steps_per_epoch,  \n",
    "        validation_batch_size, \n",
    "        validation_steps_per_epoch, \n",
    "        input_mode, \n",
    "        train_instance_count, \n",
    "        train_instance_type,   \n",
    "        min_accuracy_value, \n",
    "        model_approval_status, \n",
    "        deploy_instance_type, \n",
    "        deploy_instance_count \n",
    "    ],\n",
    "    steps=[processing_step, training_step, evaluation_step, minimum_accuracy_condition_step],\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 08/25/2021 08:56:57 PM Defaulting to the only supported framework/algorithm version: latest.\n",
      "INFO: 08/25/2021 08:56:57 PM Ignoring unnecessary instance type: None.\n",
      "WARNING: 08/25/2021 08:56:57 PM No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Metadata': {},\n",
      " 'Parameters': [{'DefaultValue': 's3://sagemaker-us-east-1-979294212144/injury-data/raw',\n",
      "                 'Name': 'InputData',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 1,\n",
      "                 'Name': 'ProcessingInstanceCount',\n",
      "                 'Type': 'Integer'},\n",
      "                {'DefaultValue': 'ml.c5.2xlarge',\n",
      "                 'Name': 'ProcessingInstanceType',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 45, 'Name': 'MaxSeqLength', 'Type': 'Integer'},\n",
      "                {'DefaultValue': 'True',\n",
      "                 'Name': 'SampleDataset',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 'bert-base-uncased',\n",
      "                 'Name': 'TransformerModel',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 0.05,\n",
      "                 'Name': 'TrainPercentage',\n",
      "                 'Type': 'Float'},\n",
      "                {'DefaultValue': 3, 'Name': 'Epochs', 'Type': 'Integer'},\n",
      "                {'DefaultValue': 6901, 'Name': 'NumRecords', 'Type': 'Integer'},\n",
      "                {'DefaultValue': 5e-05,\n",
      "                 'Name': 'LearningRate',\n",
      "                 'Type': 'Float'},\n",
      "                {'DefaultValue': 'Adam', 'Name': 'optimizer', 'Type': 'String'},\n",
      "                {'DefaultValue': 16,\n",
      "                 'Name': 'TrainBatchSize',\n",
      "                 'Type': 'Integer'},\n",
      "                {'DefaultValue': 431,\n",
      "                 'Name': 'TrainStepsPerEpoch',\n",
      "                 'Type': 'Integer'},\n",
      "                {'DefaultValue': 16,\n",
      "                 'Name': 'ValidationBatchSize',\n",
      "                 'Type': 'Integer'},\n",
      "                {'DefaultValue': 47,\n",
      "                 'Name': 'ValidationStepsPerEpoch',\n",
      "                 'Type': 'Integer'},\n",
      "                {'DefaultValue': 'File', 'Name': 'InputMode', 'Type': 'String'},\n",
      "                {'DefaultValue': 1,\n",
      "                 'Name': 'TrainInstanceCount',\n",
      "                 'Type': 'Integer'},\n",
      "                {'DefaultValue': 'ml.p3.2xlarge',\n",
      "                 'Name': 'TrainInstanceType',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 0.75,\n",
      "                 'Name': 'MinAccuracyValue',\n",
      "                 'Type': 'Float'},\n",
      "                {'DefaultValue': 'PendingManualApproval',\n",
      "                 'Name': 'ModelApprovalStatus',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 'ml.m5.large',\n",
      "                 'Name': 'DeployInstanceType',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 1,\n",
      "                 'Name': 'DeployInstanceCount',\n",
      "                 'Type': 'Integer'}],\n",
      " 'PipelineExperimentConfig': {'ExperimentName': {'Get': 'Execution.PipelineName'},\n",
      "                              'TrialName': {'Get': 'Execution.PipelineExecutionId'}},\n",
      " 'Steps': [{'Arguments': {'AppSpecification': {'ContainerArguments': ['--train_percentage',\n",
      "                                                                      '0.05',\n",
      "                                                                      '--max_len',\n",
      "                                                                      '45',\n",
      "                                                                      '--transformer_model',\n",
      "                                                                      'bert-base-uncased',\n",
      "                                                                      '--is_sample_dataset'],\n",
      "                                               'ContainerEntrypoint': ['python3',\n",
      "                                                                       '/opt/ml/processing/input/code/pre-processing.py'],\n",
      "                                               'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3'},\n",
      "                          'Environment': {'AWS_DEFAULT_REGION': 'us-east-1'},\n",
      "                          'ProcessingInputs': [{'AppManaged': False,\n",
      "                                                'InputName': 'raw-input-data',\n",
      "                                                'S3Input': {'LocalPath': '/opt/ml/processing/input/data/',\n",
      "                                                            'S3CompressionType': 'None',\n",
      "                                                            'S3DataDistributionType': 'ShardedByS3Key',\n",
      "                                                            'S3DataType': 'S3Prefix',\n",
      "                                                            'S3InputMode': 'File',\n",
      "                                                            'S3Uri': {'Get': 'Parameters.InputData'}}},\n",
      "                                               {'AppManaged': False,\n",
      "                                                'InputName': 'code',\n",
      "                                                'S3Input': {'LocalPath': '/opt/ml/processing/input/code',\n",
      "                                                            'S3CompressionType': 'None',\n",
      "                                                            'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                            'S3DataType': 'S3Prefix',\n",
      "                                                            'S3InputMode': 'File',\n",
      "                                                            'S3Uri': 's3://sagemaker-us-east-1-979294212144/sagemaker-scikit-learn-2021-08-25-20-56-56-768/input/code/pre-processing.py'}}],\n",
      "                          'ProcessingOutputConfig': {'Outputs': [{'AppManaged': False,\n",
      "                                                                  'OutputName': 'processed-train',\n",
      "                                                                  'S3Output': {'LocalPath': '/opt/ml/processing/output/processed/train',\n",
      "                                                                               'S3UploadMode': 'EndOfJob',\n",
      "                                                                               'S3Uri': 's3://sagemaker-us-east-1-979294212144/sagemaker-scikit-learn-2021-08-25-20-56-56-768/output/processed-train'}},\n",
      "                                                                 {'AppManaged': False,\n",
      "                                                                  'OutputName': 'processed-validation',\n",
      "                                                                  'S3Output': {'LocalPath': '/opt/ml/processing/output/processed/validation',\n",
      "                                                                               'S3UploadMode': 'EndOfJob',\n",
      "                                                                               'S3Uri': 's3://sagemaker-us-east-1-979294212144/sagemaker-scikit-learn-2021-08-25-20-56-56-768/output/processed-validation'}},\n",
      "                                                                 {'AppManaged': False,\n",
      "                                                                  'OutputName': 'processed-test',\n",
      "                                                                  'S3Output': {'LocalPath': '/opt/ml/processing/output/processed/test',\n",
      "                                                                               'S3UploadMode': 'EndOfJob',\n",
      "                                                                               'S3Uri': 's3://sagemaker-us-east-1-979294212144/sagemaker-scikit-learn-2021-08-25-20-56-56-768/output/processed-test'}}]},\n",
      "                          'ProcessingResources': {'ClusterConfig': {'InstanceCount': {'Get': 'Parameters.ProcessingInstanceCount'},\n",
      "                                                                    'InstanceType': {'Get': 'Parameters.ProcessingInstanceType'},\n",
      "                                                                    'VolumeSizeInGB': 30}},\n",
      "                          'RoleArn': 'arn:aws:iam::979294212144:role/service-role/AmazonSageMaker-ExecutionRole-20210423T122185',\n",
      "                          'StoppingCondition': {'MaxRuntimeInSeconds': 18000}},\n",
      "            'Name': 'Pre-Processing',\n",
      "            'Type': 'Processing'},\n",
      "           {'Arguments': {'AlgorithmSpecification': {'EnableSageMakerMetricsTimeSeries': True,\n",
      "                                                     'MetricDefinitions': [{'Name': 'train:loss',\n",
      "                                                                            'Regex': 'loss: '\n",
      "                                                                                     '([0-9\\\\.]+)'},\n",
      "                                                                           {'Name': 'train:accuracy',\n",
      "                                                                            'Regex': 'acc: '\n",
      "                                                                                     '([0-9\\\\.]+)'},\n",
      "                                                                           {'Name': 'validation:loss',\n",
      "                                                                            'Regex': 'val_loss: '\n",
      "                                                                                     '([0-9\\\\.]+)'},\n",
      "                                                                           {'Name': 'validation:accuracy',\n",
      "                                                                            'Regex': 'val_acc: '\n",
      "                                                                                     '([0-9\\\\.]+)'}],\n",
      "                                                     'TrainingImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-tensorflow-training:2.4-transformers4.4-gpu-py37-cu110-ubuntu18.04',\n",
      "                                                     'TrainingInputMode': 'File'},\n",
      "                          'DebugHookConfig': {'CollectionConfigurations': [],\n",
      "                                              'S3OutputPath': 's3://sagemaker-us-east-1-979294212144/'},\n",
      "                          'HyperParameters': {'batch_size': '16',\n",
      "                                              'epochs': '3',\n",
      "                                              'learning_rate': '5e-05',\n",
      "                                              'max_seq_length': '45',\n",
      "                                              'num_records': '6901',\n",
      "                                              'optimizer': '\"Adam\"',\n",
      "                                              'sagemaker_container_log_level': '20',\n",
      "                                              'sagemaker_job_name': '\"huggingface-tensorflow-training-2021-08-25-20-56-57-029\"',\n",
      "                                              'sagemaker_program': '\"train.py\"',\n",
      "                                              'sagemaker_region': '\"us-east-1\"',\n",
      "                                              'sagemaker_submit_directory': '\"s3://sagemaker-us-east-1-979294212144/huggingface-tensorflow-training-2021-08-25-20-56-57-029/source/sourcedir.tar.gz\"',\n",
      "                                              'steps_per_epoch': '431',\n",
      "                                              'validation_batch_size': '16',\n",
      "                                              'validation_steps': '47'},\n",
      "                          'InputDataConfig': [{'ChannelName': 'train',\n",
      "                                               'DataSource': {'S3DataSource': {'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                                               'S3DataType': 'S3Prefix',\n",
      "                                                                               'S3Uri': {'Get': \"Steps.Pre-Processing.ProcessingOutputConfig.Outputs['processed-train'].S3Output.S3Uri\"}}}},\n",
      "                                              {'ChannelName': 'validation',\n",
      "                                               'DataSource': {'S3DataSource': {'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                                               'S3DataType': 'S3Prefix',\n",
      "                                                                               'S3Uri': {'Get': \"Steps.Pre-Processing.ProcessingOutputConfig.Outputs['processed-validation'].S3Output.S3Uri\"}}}}],\n",
      "                          'OutputDataConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-979294212144/'},\n",
      "                          'ProfilerConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-979294212144/'},\n",
      "                          'ProfilerRuleConfigurations': [{'RuleConfigurationName': 'ProfilerReport-1629925017',\n",
      "                                                          'RuleEvaluatorImage': '503895931360.dkr.ecr.us-east-1.amazonaws.com/sagemaker-debugger-rules:latest',\n",
      "                                                          'RuleParameters': {'rule_to_invoke': 'ProfilerReport'}}],\n",
      "                          'ResourceConfig': {'InstanceCount': 1,\n",
      "                                             'InstanceType': 'ml.p3.2xlarge',\n",
      "                                             'VolumeSizeInGB': 50},\n",
      "                          'RoleArn': 'arn:aws:iam::979294212144:role/service-role/AmazonSageMaker-ExecutionRole-20210423T122185',\n",
      "                          'StoppingCondition': {'MaxRuntimeInSeconds': 18000}},\n",
      "            'CacheConfig': {'Enabled': True, 'ExpireAfter': 'PT1H'},\n",
      "            'Name': 'Train',\n",
      "            'Type': 'Training'},\n",
      "           {'Arguments': {'AppSpecification': {'ContainerArguments': ['--max_len',\n",
      "                                                                      '45'],\n",
      "                                               'ContainerEntrypoint': ['python3',\n",
      "                                                                       '/opt/ml/processing/input/code/evaluate_model_metrics.py'],\n",
      "                                               'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3'},\n",
      "                          'Environment': {'AWS_DEFAULT_REGION': 'us-east-1'},\n",
      "                          'ProcessingInputs': [{'AppManaged': False,\n",
      "                                                'InputName': 'input-1',\n",
      "                                                'S3Input': {'LocalPath': '/opt/ml/processing/input/model',\n",
      "                                                            'S3CompressionType': 'None',\n",
      "                                                            'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                            'S3DataType': 'S3Prefix',\n",
      "                                                            'S3InputMode': 'File',\n",
      "                                                            'S3Uri': {'Get': 'Steps.Train.ModelArtifacts.S3ModelArtifacts'}}},\n",
      "                                               {'AppManaged': False,\n",
      "                                                'InputName': 'input-2',\n",
      "                                                'S3Input': {'LocalPath': '/opt/ml/processing/input/data',\n",
      "                                                            'S3CompressionType': 'None',\n",
      "                                                            'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                            'S3DataType': 'S3Prefix',\n",
      "                                                            'S3InputMode': 'File',\n",
      "                                                            'S3Uri': {'Get': \"Steps.Pre-Processing.ProcessingOutputConfig.Outputs['processed-test'].S3Output.S3Uri\"}}},\n",
      "                                               {'AppManaged': False,\n",
      "                                                'InputName': 'code',\n",
      "                                                'S3Input': {'LocalPath': '/opt/ml/processing/input/code',\n",
      "                                                            'S3CompressionType': 'None',\n",
      "                                                            'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                            'S3DataType': 'S3Prefix',\n",
      "                                                            'S3InputMode': 'File',\n",
      "                                                            'S3Uri': 's3://sagemaker-us-east-1-979294212144/sagemaker-scikit-learn-2021-08-25-20-56-57-358/input/code/evaluate_model_metrics.py'}}],\n",
      "                          'ProcessingOutputConfig': {'Outputs': [{'AppManaged': False,\n",
      "                                                                  'OutputName': 'metrics',\n",
      "                                                                  'S3Output': {'LocalPath': '/opt/ml/processing/output/metrics/',\n",
      "                                                                               'S3UploadMode': 'EndOfJob',\n",
      "                                                                               'S3Uri': 's3://sagemaker-us-east-1-979294212144/sagemaker-scikit-learn-2021-08-25-20-56-50-199/output/metrics'}}]},\n",
      "                          'ProcessingResources': {'ClusterConfig': {'InstanceCount': {'Get': 'Parameters.ProcessingInstanceCount'},\n",
      "                                                                    'InstanceType': {'Get': 'Parameters.ProcessingInstanceType'},\n",
      "                                                                    'VolumeSizeInGB': 30}},\n",
      "                          'RoleArn': 'arn:aws:iam::979294212144:role/service-role/AmazonSageMaker-ExecutionRole-20210423T122185',\n",
      "                          'StoppingCondition': {'MaxRuntimeInSeconds': 7200}},\n",
      "            'Name': 'EvaluateModel',\n",
      "            'PropertyFiles': [{'FilePath': 'evaluation.json',\n",
      "                               'OutputName': 'metrics',\n",
      "                               'PropertyFileName': 'EvaluationReport'}],\n",
      "            'Type': 'Processing'},\n",
      "           {'Arguments': {'Conditions': [{'LeftValue': {'Std:JsonGet': {'Path': 'metrics.accuracy.value',\n",
      "                                                                        'PropertyFile': {'Get': 'Steps.EvaluateModel.PropertyFiles.EvaluationReport'}}},\n",
      "                                          'RightValue': {'Get': 'Parameters.MinAccuracyValue'},\n",
      "                                          'Type': 'GreaterThanOrEqualTo'}],\n",
      "                          'ElseSteps': [],\n",
      "                          'IfSteps': [{'Arguments': {'InferenceSpecification': {'Containers': [{'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.4.1-cpu',\n",
      "                                                                                                'ModelDataUrl': {'Get': 'Steps.Train.ModelArtifacts.S3ModelArtifacts'}}],\n",
      "                                                                                'SupportedContentTypes': ['application/jsonlines'],\n",
      "                                                                                'SupportedRealtimeInferenceInstanceTypes': [{'Get': 'Parameters.DeployInstanceType'}],\n",
      "                                                                                'SupportedResponseMIMETypes': ['application/jsonlines'],\n",
      "                                                                                'SupportedTransformInstanceTypes': [{'Get': 'Parameters.DeployInstanceType'}]},\n",
      "                                                     'ModelApprovalStatus': {'Get': 'Parameters.ModelApprovalStatus'},\n",
      "                                                     'ModelMetrics': {'ModelQuality': {'Statistics': {'ContentType': 'application/json',\n",
      "                                                                                                      'S3Uri': 's3://sagemaker-us-east-1-979294212144/sagemaker-scikit-learn-2021-08-25-20-56-50-199/output/metrics/evaluation.json'}}},\n",
      "                                                     'ModelPackageGroupName': 'BaseBERT-Injury-Coding-1629925000'},\n",
      "                                       'Name': 'RegisterModel',\n",
      "                                       'Type': 'RegisterModel'},\n",
      "                                      {'Arguments': {'ExecutionRoleArn': 'arn:aws:iam::979294212144:role/service-role/AmazonSageMaker-ExecutionRole-20210423T122185',\n",
      "                                                     'PrimaryContainer': {'Environment': {},\n",
      "                                                                          'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.4.1-cpu',\n",
      "                                                                          'ModelDataUrl': {'Get': 'Steps.Train.ModelArtifacts.S3ModelArtifacts'}}},\n",
      "                                       'Name': 'CreateModel',\n",
      "                                       'Type': 'Model'}]},\n",
      "            'Name': 'AccuracyCondition',\n",
      "            'Type': 'Condition'}],\n",
      " 'Version': '2020-12-01'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "\n",
    "pprint(definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 08/25/2021 08:57:05 PM Defaulting to the only supported framework/algorithm version: latest.\n",
      "INFO: 08/25/2021 08:57:05 PM Ignoring unnecessary instance type: None.\n",
      "WARNING: 08/25/2021 08:57:05 PM No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    }
   ],
   "source": [
    "response = pipeline.create(role_arn=role)\n",
    "\n",
    "#pipeline_arn = response[\"PipelineArn\"]\n",
    "#print(pipeline_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:sagemaker:us-east-1:979294212144:pipeline/basebert-injury-coding-pipeline-1629925000/execution/pbe0shuwfy1a\n"
     ]
    }
   ],
   "source": [
    "execution = pipeline.start(\n",
    "    parameters=dict(\n",
    "        InputData=input_data_train,\n",
    "        ProcessingInstanceCount=1,\n",
    "        ProcessingInstanceType='ml.c5.2xlarge',\n",
    "        MaxSeqLength=45,\n",
    "        SampleDataset='True',\n",
    "        TransformerModel = 'bert-based-uncased',\n",
    "        TrainPercentage=0.05,\n",
    "        Epochs=1,\n",
    "        NumRecords = 6901,\n",
    "        LearningRate=5e-5,\n",
    "        optimizer = 'Adam',\n",
    "        TrainBatchSize=16,\n",
    "        TrainStepsPerEpoch=431,\n",
    "        ValidationBatchSize=16,\n",
    "        ValidationStepsPerEpoch=47,\n",
    "        InputMode= 'File',\n",
    "        TrainInstanceCount=1,\n",
    "        TrainInstanceType='ml.p3.2xlarge',\n",
    "        MinAccuracyValue=0.75,\n",
    "        ModelApprovalStatus='PendingManualApproval', \n",
    "        DeployInstanceType='ml.m5.large',\n",
    "        DeployInstanceCount=1 \n",
    "    )\n",
    ")\n",
    "\n",
    "print(execution.arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
