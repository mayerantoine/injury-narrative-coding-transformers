{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OyjfNueYkAJK"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers\n",
    "!pip install nltk\n",
    "#!pip install tensorflow\n",
    "#!pip isntall tensoflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cNfFC26nkDLs",
    "outputId": "8641dfc3-dff3-438d-88a6-b8edf7e867c9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uhhjUcSKJs1X"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import activations, optimizers, losses\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "v0ItnLAvzjK5"
   },
   "outputs": [],
   "source": [
    "# reduce dataset size for testing\n",
    "# implement predict -save and load model\n",
    "# Add Roberta with fine tuning\n",
    "# Add BERT with fine tuning\n",
    "# Calculate f1 and precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lp0COxWoKxOC",
    "outputId": "4ec33665-60d9-4f7a-bea3-00822a64b910"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.2\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "L3k2VOVwzRbm"
   },
   "outputs": [],
   "source": [
    "class MyTriangularSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "  def __init__(self,initial_learning_rate, end_learning_rate,cycle_steps,warmup_fraction = 0.1):\n",
    "    self.initial_learning_rate = initial_learning_rate\n",
    "    self.end_learning_rate = end_learning_rate\n",
    "    self.cycle_steps = cycle_steps\n",
    "    self.warmup_fraction = warmup_fraction\n",
    "\n",
    "  def __call__(self, step):\n",
    "    if step <= self.cycle_steps * self.warmup_fraction:\n",
    "       unit_cycle = step * 1 / (self.cycle_steps * self.warmup_fraction)\n",
    "    elif step <= self.cycle_steps:\n",
    "      unit_cycle = (self.cycle_steps - step) * 1 / (self.cycle_steps * (1 - self.warmup_fraction))\n",
    "    else:\n",
    "      unit_cycle = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OHex9p3nIZfJ"
   },
   "outputs": [],
   "source": [
    "from transformers.optimization_tf import WarmUp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJw6FOzmuETy"
   },
   "source": [
    "# Extract Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2A449XQWC7DR"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "def get_samples(ratio,train):\n",
    "  great_than2_classes = train['event'].value_counts()[train['event'].value_counts() >2].index\n",
    "  train = train[train['event'].isin(great_than2_classes.to_list())]\n",
    "  train_samples, _ = train_test_split(train,train_size=ratio,random_state=42,stratify=train['event'])\n",
    "  print(\"nb classes\",train_samples['event'].nunique())\n",
    "  print(\"nb oservations:\",train_samples.shape)\n",
    "  print(train_samples['event'].value_counts())\n",
    "\n",
    "  return train_samples\n",
    "\n",
    "#get_samples(ratio = 0.05, train=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "QMZv1S7RJ_v3"
   },
   "outputs": [],
   "source": [
    "def get_data(is_sample=None,ratio=None,is_test_split=False):\n",
    "  \n",
    "    train = pd.read_csv('train.csv')\n",
    "    if is_sample:\n",
    "      train = get_samples(ratio = ratio, train=train)\n",
    "    great_than2_classes = train['event'].value_counts()[train['event'].value_counts() >7].index \n",
    "    train_filter = train[train['event'].isin(great_than2_classes.to_list())]\n",
    "\n",
    "    X = train_filter['text']\n",
    "    y = train_filter['event']\n",
    "\n",
    "    print(f\"X.shape {X.shape} y.shape : {y.shape}\")\n",
    "\n",
    "    X_train_valid,X_test,y_train_valid, y_test = train_test_split(X,y,train_size=0.9,random_state=42,stratify=y)\n",
    "    \n",
    "    if is_test_split:\n",
    "      X_train,X_valid,y_train,y_valid = train_test_split(X_train_valid,y_train_valid,train_size=0.8,random_state=42,stratify=y_train_valid)\n",
    "\n",
    "    if is_test_split :\n",
    "      print(f\"X_train shape {X_train.shape} y_train shape : {y_train.shape}\")\n",
    "      print(f\"X_valid shape {X_valid.shape} y_valid shape : {y_valid.shape}\")\n",
    "      print(f\"X_test shape {X_test.shape} y_test shape : {y_test.shape}\")\n",
    "\n",
    "      return {\n",
    "          'train': (X_train,y_train),\n",
    "          'valid': (X_valid,y_valid),\n",
    "          'test': (X_test,y_test)\n",
    "      }\n",
    "\n",
    "    else:\n",
    "      print(f\"X_train shape {X_train_valid.shape} y_train shape : {y_train_valid.shape}\")\n",
    "      print(f\"X_valid shape {X_test.shape} y_valid shape : {y_test.shape}\")\n",
    "\n",
    "\n",
    "      return {\n",
    "          'train': (X_train_valid,y_train_valid),\n",
    "          'valid': (X_test,y_test),\n",
    "      }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dOSsOuMklCV8",
    "outputId": "77423075-5c1c-4e98-e571-fbe3a16f7fe9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape (153938,) y.shape : (153938,)\n",
      "X_train shape (138544,) y_train shape : (138544,)\n",
      "X_valid shape (15394,) y_valid shape : (15394,)\n"
     ]
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 3100\n",
    "data = get_data(is_sample=False,ratio=1)\n",
    "X_train, y_train = data['train']\n",
    "X_valid,y_valid = data['valid']\n",
    "#X_test,y_test = data['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkmrF4fBt7IS"
   },
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rv6Xg007BWDa",
    "outputId": "ee8ebb0b-15aa-4094-e6d4-d42e75a89803"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes in train : 42\n",
      "classes in valid : 42\n",
      "[42, 62, 71, 55, 43, 73, 63, 31, 60, 41, 70, 64, 12, 99, 11, 53, 72, 13, 26, 52, 66, 61, 67, 27, 24, 51, 32, 23, 78, 44, 25, 79, 50, 40, 65, 49, 22, 69, 21, 20, 45, 54]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('classes in train :',len(np.unique(y_train)))\n",
    "print('classes in valid :',len(np.unique(y_valid)))\n",
    "\n",
    "CLASSES = y_train.unique().tolist()\n",
    "print(CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Niej9Xr2oYPF"
   },
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Applies some pre-processing on the given text.\n",
    "\n",
    "    Steps :\n",
    "    - Removing HTML tags\n",
    "    - Removing punctuation\n",
    "    - Lowering text\n",
    "    \"\"\"\n",
    "\n",
    "    # remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    # remove the characters [\\], ['] and [\"]\n",
    "    text = re.sub(r\"\\\\\", \"\", text)\n",
    "    text = re.sub(r\"\\'\", \"\", text)\n",
    "    text = re.sub(r\"\\\"\", \"\", text)\n",
    "\n",
    "    # convert text to lowercase\n",
    "    text = text.strip().lower()\n",
    "\n",
    "    # remove all non-ASCII characters:\n",
    "    text = re.sub(r'[^\\x00-\\x7f]', r'', text)\n",
    "\n",
    "    # replace punctuation characters with spaces\n",
    "    filters = '!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    translate_dict = dict((c, \" \") for c in filters)\n",
    "    translate_map = str.maketrans(translate_dict)\n",
    "    text = text.translate(translate_map)\n",
    "    text = \" \".join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Hbky6bE6LRSB"
   },
   "outputs": [],
   "source": [
    "def remove_useless_words(text,useless_words):\n",
    "    sentence = [word for word in word_tokenize(text)]\n",
    "    sentence_stop = [word for word in sentence if word not in useless_words]\n",
    "\n",
    "    text = \" \".join(sentence_stop)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "--BcpA6wLSNv"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    \"\"\" Preprocess : cleaning and remove stop words\"\"\"\n",
    "\n",
    "    X = X.apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "    X = X.apply(lambda x: clean_text(x))\n",
    "\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    useless_words = stopwords + list(string.punctuation) + ['yom', 'yof', 'yowm', 'yf', 'ym', 'yo']\n",
    "    # print(\"useless word : \",useless_words)\n",
    "    X = X.apply(lambda x: remove_useless_words(x,useless_words))\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6C8uN_dxLa7D",
    "outputId": "74b26581-4d93-47c6-eb8c-e8663f403a8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after preprocessing...\n",
      "59414     walking steps job missed step slipped fell hit rt knee felt pop dx quad tendon rupture\n",
      "48134                         c foot injury p back foot hit linen cart work dx right foot injury\n",
      "136481                              paramedic twisted ankle lifting patient work dx ankle sprain\n",
      "54909                                      unhooking iv line got iv fluid splashed face occ expo\n",
      "18265                                  shoulder injury fell ladder carrying bundle shingles work\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "X_train_processed = preprocess_data(X_train)\n",
    "X_valid_processed = preprocess_data(X_valid)\n",
    "\n",
    "print(\"after preprocessing...\")\n",
    "print(X_train_processed.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6L4h5P_YFj6"
   },
   "source": [
    "Save Processed data as TFRecords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_2viYtTtx29"
   },
   "source": [
    "# Tokenize and Encode Train and Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJ67aVfeYKrr"
   },
   "source": [
    "Test and Preview Tokenizer in transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "APf_28XP_GRQ",
    "outputId": "fb5a4b1b-ee79-49a0-9fc3-84d8caa77c2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len:45\n",
      "narrative: '['paramedic twisted ankle lifting patient work dx ankle sprain', 'working c metal grinder wearing gloves kicked back hittinghand lac']'\n",
      "input ids: [[101, 11498, 7583, 2594, 6389, 10792, 8783, 5776, 2147, 1040, 2595, 10792, 11867, 21166, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2551, 1039, 3384, 23088, 2121, 4147, 11875, 6476, 2067, 7294, 11774, 18749, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "attention mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import  DistilBertTokenizerFast,BertTokenizerFast,AutoTokenizer\n",
    "\n",
    "MAX_LEN = 45\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "#tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "#narrative = 'r wrist lac cut wrist yest scraped wrist piece rusty metal happened working camp'\n",
    "\n",
    "\n",
    "\n",
    "#MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "narratives = [X_train_processed.iloc[2],X_valid_processed.iloc[2]]\n",
    "inputs = tokenizer(narratives, max_length=MAX_LEN, truncation=True, padding='max_length')\n",
    "\n",
    "print(f\"max_len:{MAX_LEN}\")\n",
    "print(f'narrative: \\'{narratives}\\'')\n",
    "print(f'input ids: {inputs[\"input_ids\"]}')\n",
    "print(f'attention mask: {inputs[\"attention_mask\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "7j2E9Md0ASZA"
   },
   "outputs": [],
   "source": [
    "def construct_encodings(x, tkzr, max_len, trucation=True, padding=True):\n",
    "    return tkzr(x, max_length=max_len, truncation=trucation, padding=padding)\n",
    "\n",
    "x_train = X_train_processed.to_list()\n",
    "x_valid = X_valid_processed.to_list()\n",
    "\n",
    "#train_encodings = construct_encodings(x_train, tokenizer, max_len=MAX_LEN)\n",
    "#valid_encodings = construct_encodings(x_valid, tokenizer, max_len=MAX_LEN)\n",
    "\n",
    "train_encodings = tokenizer(x_train, max_length=MAX_LEN, truncation=True, padding='max_length',return_tensors='tf')\n",
    "valid_encodings = tokenizer(x_valid, max_length=MAX_LEN, truncation=True, padding='max_length',return_tensors='tf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Js2DJiHzPcdi",
    "outputId": "6b4e8f6c-275e-42fb-99a6-76fee2b96c1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "45\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "print(MAX_LEN)\n",
    "print(len(train_encodings['input_ids'][0]))\n",
    "print(len(valid_encodings['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FtHFsdDcPfGX",
    "outputId": "e845281b-4733-46c0-f9fb-f696f6634fe2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Labels .....\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"Encoding Labels .....\")\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "y_train_encode = np.asarray(encoder.transform(y_train))\n",
    "y_valid_encode = np.asarray(encoder.transform(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f2D8qXPkJU0p",
    "outputId": "c6094835-450d-4f11-bfc2-db3473fd3844"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes in train : 42\n",
      "classes in valid : 42\n"
     ]
    }
   ],
   "source": [
    "print('classes in train :',len(np.unique(y_train_encode)))\n",
    "print('classes in valid :',len(np.unique(y_valid_encode)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHSevTANtrxM"
   },
   "source": [
    "## Create TF Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aVuoi-aLP4hl",
    "outputId": "81b1baa4-0c74-43f7-b17d-02ad04a406d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.86 ms, sys: 155 µs, total: 3.02 ms\n",
      "Wall time: 2.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def construct_tfdataset(encodings, y=None):\n",
    "    if y is not None:\n",
    "        return tf.data.Dataset.from_tensor_slices((dict(encodings),y))\n",
    "    else:\n",
    "        # this case is used when making predictions on unseen samples after training\n",
    "        return tf.data.Dataset.from_tensor_slices(dict(encodings))\n",
    "    \n",
    "train_tfdataset = construct_tfdataset(train_encodings, y_train_encode)\n",
    "valid_tfdataset = construct_tfdataset(valid_encodings, y_valid_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8svxfCKeP4eF",
    "outputId": "61eca96c-42c4-454b-ff74-76a7595703d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: ({input_ids: (45,), attention_mask: (45,)}, ()), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.int64)>\n",
      "<TensorSliceDataset shapes: ({input_ids: (45,), attention_mask: (45,)}, ()), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "print(train_tfdataset)\n",
    "print(valid_tfdataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N_EPOCHS=3\n",
    "#BATCH_SIZE = 16 \n",
    "#steps_per_epoch = len(X_train) // BATCH_SIZE\n",
    "\n",
    "#train_tfdataset = train_tfdataset.repeat(N_EPOCHS * steps_per_epoch)\n",
    "#train_tfdataset = train_tfdataset.prefetch(tf.data.AUTOTUNE)\n",
    "#train_tfdataset = train_tfdataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid_tfdataset = valid_tfdataset.repeat(N_EPOCHS * steps_per_epoch)\n",
    "#valid_tfdataset = valid_tfdataset.prefetch(tf.data.AUTOTUNE)\n",
    "#valid_tfdataset = valid_tfdataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: ({input_ids: (45,), attention_mask: (45,)}, ()), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.int64)>\n",
      "<TensorSliceDataset shapes: ({input_ids: (45,), attention_mask: (45,)}, ()), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "print(train_tfdataset)\n",
    "print(valid_tfdataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wz1FDvTJtX16"
   },
   "source": [
    "# Fine Tune DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "cF_bToiYP4av"
   },
   "outputs": [],
   "source": [
    "from transformers import  TFDistilBertForSequenceClassification\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "from transformers.optimization_tf import WarmUp, AdamWeightDecay\n",
    "\n",
    "class DistillBERT:\n",
    "    def __init__(self, params):\n",
    "        self.num_labels = params['num_labels']\n",
    "        self.max_len = params['max_len']\n",
    "        self.learning_rate = params['learning_rate']\n",
    "        self.num_records = params['num_records']\n",
    "        self.batch_size = params['batch_size']\n",
    "        self.epochs = params['epochs']\n",
    "        \n",
    "    def build(self):\n",
    "        \n",
    "        freeze_bert_layer = False\n",
    "        \n",
    "        # Load model and tokenizer\n",
    "        transformer_model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=self.num_labels)\n",
    "        input_ids = tf.keras.Input(shape = (self.max_len,),name='input_ids',dtype='int32')\n",
    "        attention_mask = tf.keras.Input(shape = (self.max_len,),name='attention_mask',dtype='int32')\n",
    "        \n",
    "        embbeding_layer = transformer_model.distilbert(input_ids,attention_mask=attention_mask)[0]\n",
    "        #X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256,return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(embbeding_layer)\n",
    "        X = tf.keras.layers.Conv1D(512, 3, activation='relu')(embbeding_layer)\n",
    "        X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
    "        X = tf.keras.layers.Dense(256, activation='relu')(X)\n",
    "        X = tf.keras.layers.Dropout(0.2)(X)\n",
    "        X = tf.keras.layers.Dense(self.num_labels, activation='softmax')(X)\n",
    "        \n",
    "        model = tf.keras.Model(inputs=[input_ids,attention_mask], outputs = X)\n",
    "        \n",
    "        for layer in model.layers[:3]:\n",
    "            layer.trainable = not freeze_bert_layer\n",
    "\n",
    "        num_train_steps = (self.num_records // self.batch_size) * self.epochs\n",
    "        decay_schedule_fn = PolynomialDecay(\n",
    "            initial_learning_rate=self.learning_rate,\n",
    "            end_learning_rate=0.,\n",
    "            decay_steps=num_train_steps\n",
    "            )\n",
    "        \n",
    "        warmup_steps  = num_train_steps * 0.1\n",
    "        warmup_schedule = WarmUp(self.learning_rate,decay_schedule_fn,warmup_steps)\n",
    "\n",
    "          # fine optimizer and loss\n",
    "        #optimizer = tf.keras.optimizers.Adam(learning_rate=warmup_schedule,epsilon=1e-06)\n",
    "\n",
    "        optimizer = AdamWeightDecay(learning_rate=warmup_schedule, weight_decay_rate=0.01, epsilon=1e-6)\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits= False)\n",
    "        metrics = ['acc']\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qV5Qp85sRy5g",
    "outputId": "35a53441-3092-4c2a-e971-4e0ef779b45b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-08-19 22:37:22.537 tensorflow-2-3-gpu--ml-g4dn-xlarge-c85184389676cdfa7bdf06745c9b:104 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2021-08-19 22:37:22.563 tensorflow-2-3-gpu--ml-g4dn-xlarge-c85184389676cdfa7bdf06745c9b:104 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'activation_13', 'vocab_transform', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'classifier', 'dropout_19']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 45)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 45)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "distilbert (TFDistilBertMainLay TFBaseModelOutput(la 66362880    input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 43, 512)      1180160     distilbert[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 512)          0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          131328      global_max_pooling1d[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 42)           10794       dropout_20[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 67,685,162\n",
      "Trainable params: 67,685,162\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 5e-5\n",
    "params = {\n",
    "        'num_labels': len(CLASSES),\n",
    "        'max_len': MAX_LEN,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'num_records':len(X_train),\n",
    "        'batch_size':16,\n",
    "        'epochs':1\n",
    "    }\n",
    "  \n",
    "model = DistillBERT(params).build()   \n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "id": "ehVqtsrfSGo8",
    "outputId": "f8fef34e-3ffc-4791-f8aa-ca63ebc60c40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8659/8659 [==============================] - 833s 96ms/step - loss: 0.7041 - acc: 0.7893 - val_loss: 0.4528 - val_acc: 0.8509\n",
      "CPU times: user 9min 16s, sys: 3min 17s, total: 12min 33s\n",
      "Wall time: 14min 8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f242be33750>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "N_EPOCHS=1\n",
    "BATCH_SIZE = 16 \n",
    "model.fit(x=train_tfdataset.batch(BATCH_SIZE),\n",
    "          batch_size=BATCH_SIZE, \n",
    "          epochs=N_EPOCHS,\n",
    "          validation_data=valid_tfdataset.batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7bAYONdomLFs",
    "outputId": "eff9f51b-c2ad-4200-c358-5838295aa2ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ({input_ids: (45,), attention_mask: (45,)}, ()), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.int64)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tfdataset\n",
    "valid_tfdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zlYKKmNHUJnT",
    "outputId": "f849c4a3-dcb2-48da-f50a-5cd7800301be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "963/963 [==============================] - 25s 26ms/step - loss: 0.4528 - acc: 0.8509\n",
      "CPU times: user 17.4 s, sys: 3 s, total: 20.4 s\n",
      "Wall time: 25.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.45275986194610596, 'acc': 0.8508509993553162}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.evaluate(valid_tfdataset.batch(BATCH_SIZE), batch_size=BATCH_SIZE,return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Q8nruYTOMqx"
   },
   "outputs": [],
   "source": [
    "model.save(\"distillbert.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VC3ayMr9tJ2X"
   },
   "source": [
    "# Train DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5F5K6sqTH8g",
    "outputId": "80188bd6-a9a2-4046-f38c-c10c78aaae8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_layer_norm', 'vocab_transform', 'vocab_projector', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_376', 'classifier', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "distilbert (TFDistilBertMain multiple                  66362880  \n",
      "_________________________________________________________________\n",
      "pre_classifier (Dense)       multiple                  590592    \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  21532     \n",
      "_________________________________________________________________\n",
      "dropout_376 (Dropout)        multiple                  0         \n",
      "=================================================================\n",
      "Total params: 66,975,004\n",
      "Trainable params: 66,975,004\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "BATCH_SIZE = 16 \n",
    "print(MODEL_NAME)\n",
    "\n",
    "model2 = TFDistilBertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(CLASSES))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model2.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ecYSHEd-qYuU",
    "outputId": "52bec047-4f9f-4f06-b1a2-69a7bebfcc09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fc560dd1d70>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fc560dd1d70>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7fc57bf86170> and will run it as-is.\n",
      "Cause: while/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function wrap at 0x7fc57bf86170> and will run it as-is.\n",
      "Cause: while/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "384/384 [==============================] - ETA: 0s - loss: 1.5577 - accuracy: 0.5737WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "384/384 [==============================] - 117s 194ms/step - loss: 1.5577 - accuracy: 0.5737 - val_loss: 1.0163 - val_accuracy: 0.7125\n",
      "Epoch 2/5\n",
      "384/384 [==============================] - 73s 189ms/step - loss: 0.7287 - accuracy: 0.7835 - val_loss: 0.8780 - val_accuracy: 0.7477\n",
      "Epoch 3/5\n",
      "384/384 [==============================] - 73s 189ms/step - loss: 0.4786 - accuracy: 0.8525 - val_loss: 0.9493 - val_accuracy: 0.7360\n",
      "Epoch 4/5\n",
      "384/384 [==============================] - 73s 189ms/step - loss: 0.3224 - accuracy: 0.8986 - val_loss: 0.8742 - val_accuracy: 0.7621\n",
      "Epoch 5/5\n",
      "384/384 [==============================] - 73s 189ms/step - loss: 0.2164 - accuracy: 0.9333 - val_loss: 0.9448 - val_accuracy: 0.7627\n",
      "CPU times: user 5min 7s, sys: 14.2 s, total: 5min 21s\n",
      "Wall time: 6min 56s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc45c72e690>"
      ]
     },
     "execution_count": 96,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model2.fit(train_tfdataset.shuffle(1000).batch(BATCH_SIZE), batch_size=BATCH_SIZE, epochs=N_EPOCHS,validation_data=valid_tfdataset.batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLwXY4AvtidP"
   },
   "source": [
    "# Train Base BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Uj_61ZGez16o"
   },
   "outputs": [],
   "source": [
    "from transformers import TFRobertaForSequenceClassification,TFBertForSequenceClassification\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "from transformers.optimization_tf import WarmUp, AdamWeightDecay\n",
    "class BaseBERT:\n",
    "    def __init__(self, params):\n",
    "        self.num_labels = params['num_labels']\n",
    "        self.max_len = params['max_len']\n",
    "        self.learning_rate = params['learning_rate']\n",
    "        self.num_records = params['num_records']\n",
    "        self.batch_size = params['batch_size']\n",
    "        self.epochs = params['epochs']\n",
    "        \n",
    "    def build(self):\n",
    "        \n",
    "        freeze_bert_layer = False\n",
    "        \n",
    "        # Load model and tokenizer\n",
    "        transformer_model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=self.num_labels)\n",
    "        input_ids = tf.keras.Input(shape = (self.max_len,),name='input_ids',dtype='int32')\n",
    "        attention_mask = tf.keras.Input(shape = (self.max_len,),name='attention_mask',dtype='int32')\n",
    "        \n",
    "        embbeding_layer = transformer_model.bert(input_ids,attention_mask=attention_mask)[0]\n",
    "        #X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256,return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(embbeding_layer)\n",
    "        X = tf.keras.layers.Conv1D(512, 3, activation='relu')(embbeding_layer)\n",
    "        X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
    "        X = tf.keras.layers.Dense(256, activation='relu')(X)\n",
    "        X = tf.keras.layers.Dropout(0.2)(X)\n",
    "        X = tf.keras.layers.Dense(self.num_labels, activation='softmax')(X)\n",
    "        \n",
    "        model = tf.keras.Model(inputs=[input_ids,attention_mask], outputs = X)\n",
    "        \n",
    "        for layer in model.layers[:3]:\n",
    "            layer.trainable = not freeze_bert_layer\n",
    "\n",
    "        num_train_steps = (self.num_records // self.batch_size) * self.epochs\n",
    "        decay_schedule_fn = PolynomialDecay(\n",
    "            initial_learning_rate=self.learning_rate,\n",
    "            end_learning_rate=0.,\n",
    "            decay_steps=num_train_steps\n",
    "            )\n",
    "        \n",
    "        warmup_steps  = num_train_steps * 0.1\n",
    "        warmup_schedule = WarmUp(self.learning_rate,decay_schedule_fn,warmup_steps)\n",
    "\n",
    "          # fine optimizer and loss\n",
    "        #optimizer = tf.keras.optimizers.Adam(learning_rate=warmup_schedule,epsilon=1e-06)\n",
    "\n",
    "        optimizer = AdamWeightDecay(learning_rate=warmup_schedule, weight_decay_rate=0.01, epsilon=1e-6)\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits= False)\n",
    "        metrics = ['acc']\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F5V9_ef40YUT",
    "outputId": "0485897e-38dd-4e9f-ca06-eafe117887e6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 45)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 45)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (TFBertMainLayer)          TFBaseModelOutputWit 109482240   input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 43, 512)      1180160     bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 512)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          131328      global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_59 (Dropout)            (None, 256)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 42)           10794       dropout_59[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 110,804,522\n",
      "Trainable params: 110,804,522\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 5e-5\n",
    "params = {\n",
    "        'num_labels': len(CLASSES),\n",
    "        'max_len': MAX_LEN,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'num_records':len(X_train),\n",
    "        'batch_size':16,\n",
    "        'epochs':1\n",
    "    }\n",
    "  \n",
    "model4 = BaseBERT(params).build()   \n",
    "print(model4.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "id": "Izo2WvZP0hSo",
    "outputId": "03158a0a-cf82-4803-f0f3-0a98f511e945"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification/bert/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8659/8659 [==============================] - 1519s 175ms/step - loss: 0.7114 - acc: 0.7879 - val_loss: 0.4517 - val_acc: 0.8508\n",
      "CPU times: user 17min 3s, sys: 6min 8s, total: 23min 12s\n",
      "Wall time: 25min 33s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f24c2fd9250>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "N_EPOCHS=1\n",
    "BATCH_SIZE = 16 \n",
    "model4.fit(x=train_tfdataset.shuffle(len(X_train)).batch(BATCH_SIZE),\n",
    "           batch_size=BATCH_SIZE,\n",
    "           epochs=N_EPOCHS,\n",
    "           validation_data=valid_tfdataset.batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2RWEnVIptvj3",
    "outputId": "19a01533-3193-4c9f-91e6-2670c4ec03db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_309 (Dropout)        multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  32298     \n",
      "=================================================================\n",
      "Total params: 109,514,538\n",
      "Trainable params: 109,514,538\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFRobertaForSequenceClassification,TFBertForSequenceClassification\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "\n",
    "N_EPOCHS = 4\n",
    "BATCH_SIZE = 16 \n",
    "\n",
    "print(MODEL_NAME)\n",
    "\n",
    "model3 = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(CLASSES),output_attentions = False,output_hidden_states= False )\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model3.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "model3.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nMl1ZZmVjOX2",
    "outputId": "de7b108b-3d9e-44df-b30a-7d46e499e640"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7f2c3cf8eec0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7f2c3cf8eec0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7f2c58144950> and will run it as-is.\n",
      "Cause: while/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function wrap at 0x7f2c58144950> and will run it as-is.\n",
      "Cause: while/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "384/384 [==============================] - ETA: 0s - loss: 1.6397 - accuracy: 0.5466WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "384/384 [==============================] - 197s 372ms/step - loss: 1.6397 - accuracy: 0.5466 - val_loss: 1.0385 - val_accuracy: 0.7066\n",
      "Epoch 2/4\n",
      "384/384 [==============================] - 139s 363ms/step - loss: 0.7999 - accuracy: 0.7705 - val_loss: 0.9014 - val_accuracy: 0.7516\n",
      "Epoch 3/4\n",
      "384/384 [==============================] - 140s 364ms/step - loss: 0.5315 - accuracy: 0.8430 - val_loss: 0.8953 - val_accuracy: 0.7601\n",
      "Epoch 4/4\n",
      "384/384 [==============================] - 141s 366ms/step - loss: 0.3835 - accuracy: 0.8856 - val_loss: 0.8874 - val_accuracy: 0.7562\n",
      "CPU times: user 7min 31s, sys: 46.1 s, total: 8min 17s\n",
      "Wall time: 10min 17s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2b39308a10>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model3.fit(train_tfdataset.shuffle(1000).batch(BATCH_SIZE), batch_size=BATCH_SIZE, epochs=N_EPOCHS,validation_data=valid_tfdataset.batch(BATCH_SIZE))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aajphvIYqbuI"
   },
   "source": [
    "Using the fine-tuned model to predict new samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xTBSbz7TqeLt",
    "outputId": "db117678-55fd-4a6f-dd19-01e9845937c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023528205\n"
     ]
    }
   ],
   "source": [
    "def distilBert_create_predictor(model, model_name, max_len):\n",
    "  tkzr = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "  def predict_proba(text):\n",
    "      x = [text]\n",
    "    \n",
    "      encodings =  tkzr(x, max_length=max_len, truncation=True, padding='max_length',return_tensors='tf')\n",
    "      tfdataset = construct_tfdataset(encodings)\n",
    "      tfdataset = tfdataset.batch(1)\n",
    "\n",
    "      preds = model.predict(tfdataset)\n",
    "      preds = activations.softmax(tf.convert_to_tensor(preds)).numpy()\n",
    "      return preds[0][0]\n",
    "    \n",
    "  return predict_proba\n",
    "\n",
    "clf = distilBert_create_predictor(model, MODEL_NAME, MAX_LEN)\n",
    "print(clf('reports sus laceration rt palm lost footing whilecoming ladder dx palm laceration'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LmjONuqprqvg",
    "outputId": "538de265-6791-42fd-cf22-fe0369f1eae3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "43\n",
      "0.023528205\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "def bert_create_predictor(model, model_name, max_len):\n",
    "  tkzr = BertTokenizerFast.from_pretrained(model_name)\n",
    "  def predict_proba(text):\n",
    "      x = [text]\n",
    "    \n",
    "      encodings =  tkzr(x, max_length=max_len, truncation=True, padding='max_length',return_tensors='tf')\n",
    "      tfdataset = construct_tfdataset(encodings)\n",
    "      tfdataset = tfdataset.batch(1)\n",
    "\n",
    "      preds = model.predict(tfdataset)\n",
    "      preds = activations.softmax(tf.convert_to_tensor(preds)).numpy()\n",
    "      print(preds[0])\n",
    "      return preds[0][0]\n",
    "    \n",
    "  return predict_proba\n",
    "\n",
    "clf = bert_create_predictor(model, MODEL_NAME, MAX_LEN)\n",
    "print(clf('reports sus laceration rt palm lost footing whilecoming ladder dx palm laceration'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e-K1x44xr1Mp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Injury narrative classification with transformers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.3 Python 3.7 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.3-gpu-py37-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
