{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Injury narrative classification using BERT, DistilBERT and Roberta with Hugging Face transformer library\n",
    "This notebook shows how to use the tansformer library to fine tune BERT for Text classification on the Injury narrative dataset\n",
    "___________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview and Problem statement\n",
    "\n",
    "The National Institute for Occupational Safety and Health (NIOSH) is responsible for conducting research to reduce worker injuries and illnesses in the United States. Every year millions of Americans are injured on the job. The data from those events are collected in medical records and surveillance system as unstructured text heavy injury narratives. A better understanding of the characteristics of these incidents can help us prevent them in the future. However, having human read and manually code these injury narratives to analyze the data is expensive, time consuming, and error-prone, thus the idea to use NLP to auto-code the narratives.\n",
    "\n",
    "In 2018 National Institute for Occupational Safety and Health (NIOSH), as part of the Centers for Disease Control and Prevention (CDC), along with NASA, worked with Topcoder to organize an intramural (within CDC ) and extramural (international) Natural language processing competition to classify unstructured free-text \"injury narratives\" recorded in surveillance system into injury codes from the Occupational Injuries and Illnesses Classification System (OIICS). This is known as a Multi-class text classification problem. For example the text 'DOING UNSPECIFIED LIFTING AT WORK AND DEVELOPED PAIN ACROSS CHEST CHEST PAIN' is coded by 71 which means 'Overexertion involving outside sources'. More details on the categories and event codes can be found [here](https://wwwn.cdc.gov/wisards/oiics/Trees/MultiTree.aspx?TreeType=Event).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***In this notebook we will use the dataset available on Topcoder and [Github](https://github.com/NASA-Tournament-Lab/CDC-NLP-Occ-Injury-Coding) to experiment text classification using BERT, DistilBERT and Roberta***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our workflow will follow the generic pipeline for modern-day, data-driven NLP system development as describe in the book \"Practical Natural Language Processing\". The key stages in the pipeline are as follows:\n",
    "\n",
    "1. Data acquisition\n",
    "2. Text cleaning\n",
    "3. Pre-processing\n",
    "4. Feature engineering\n",
    "5. Modeling\n",
    "6. Evaluation\n",
    "6. Deployment\n",
    "7. Monitoring and model updating\n",
    "\n",
    "However, we will only go from step 1 to step 6 evaluating 3 transformer algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NLP Pipeline](./assets/pnlp_0201r.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 -  Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OyjfNueYkAJK"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers\n",
    "!pip install nltk\n",
    "!pip install -U tensorflow\n",
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please ignore warning messages during the installation\n",
    "#!pip install --disable-pip-version-check -q sagemaker==2.35.0\n",
    "#!pip install --disable-pip-version-check -q transformers==3.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cNfFC26nkDLs",
    "outputId": "8641dfc3-dff3-438d-88a6-b8edf7e867c9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "id": "uhhjUcSKJs1X"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import activations, optimizers, losses\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "import numpy as np\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lp0COxWoKxOC",
    "outputId": "4ec33665-60d9-4f7a-bea3-00822a64b910"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "4.10.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJw6FOzmuETy"
   },
   "source": [
    "## Step 1 - Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The training dataset used in the competition can be downloaded from here and the test dataset from here. You can also download the complete dataset from Github. The training dataset includes 48 classifiable event codes distributed across 7 categories:\n",
    "* Violence and other injuries by persons and animals\n",
    "* Transportation incidents\n",
    "* Fires and explosions\n",
    "* Falls, slips, and trips\n",
    "* Exposure to harmful substances or environments\n",
    "* Contact with objects and equipment\n",
    "* Overexertion and bodily reaction\n",
    "\n",
    "More details on the categories and event codes can be found here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57YOM WITH CONTUSION TO FACE AFTER STRIKING IT WITH A POST POUNDER WHILE SETTING A FENCE POST</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A 45YOM FELL ON ARM WHILE WORKING HAD SLIPPED ON WATER FX WRIST</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58YOM WITH CERVICAL STRAIN  BACK PAIN S P RESTRAINED TAXI DRIVER IN LOW SPEED REAR END MVC NO LOC NO AB DEPLOYED</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33 YOM LAC TO HAND FROM A RAZOR KNIFE</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53YOM AT WORK IN A WAREHOUSE DOING UNSPECIFIED LIFTING AND STRAINED LO WER BACK</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153951</th>\n",
       "      <td>19YOF DOING UNSPECIFIED LIFTING AT WORK AND DEVELOPED PAIN ACROSS CHES T CHEST PAIN</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153952</th>\n",
       "      <td>58 YOM ACCIDENTAL CONTACT WITH AN ELECTRIC SAW AT WORK BLEEDING FROM LEFT HAND WAS A MITER SAW DX FRACTURE HAND OPEN LACERATION HAND</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153953</th>\n",
       "      <td>X 18 YOM GOT HIS HAND CAUGHT IN A DOUGH PRESS AT WORK DX FINGER CONTUSION</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153954</th>\n",
       "      <td>53YOM WAS DOING SOME KIND OF WORK IN THE DRIVEWAY OF A PARKING LOT WAS HIT BY CAR COMING OUT OF PARKING LOT DX  LT FOOT FX</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153955</th>\n",
       "      <td>38YOM GOT METAL SHAVING IN EYE WHILE CO WORKER WAS GRINDING METAL DX LT CORNEAL AB</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153956 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                        text  \\\n",
       "0                                              57YOM WITH CONTUSION TO FACE AFTER STRIKING IT WITH A POST POUNDER WHILE SETTING A FENCE POST   \n",
       "1                                                                            A 45YOM FELL ON ARM WHILE WORKING HAD SLIPPED ON WATER FX WRIST   \n",
       "2                           58YOM WITH CERVICAL STRAIN  BACK PAIN S P RESTRAINED TAXI DRIVER IN LOW SPEED REAR END MVC NO LOC NO AB DEPLOYED   \n",
       "3                                                                                                      33 YOM LAC TO HAND FROM A RAZOR KNIFE   \n",
       "4                                                            53YOM AT WORK IN A WAREHOUSE DOING UNSPECIFIED LIFTING AND STRAINED LO WER BACK   \n",
       "...                                                                                                                                      ...   \n",
       "153951                                                   19YOF DOING UNSPECIFIED LIFTING AT WORK AND DEVELOPED PAIN ACROSS CHES T CHEST PAIN   \n",
       "153952  58 YOM ACCIDENTAL CONTACT WITH AN ELECTRIC SAW AT WORK BLEEDING FROM LEFT HAND WAS A MITER SAW DX FRACTURE HAND OPEN LACERATION HAND   \n",
       "153953                                                             X 18 YOM GOT HIS HAND CAUGHT IN A DOUGH PRESS AT WORK DX FINGER CONTUSION   \n",
       "153954            53YOM WAS DOING SOME KIND OF WORK IN THE DRIVEWAY OF A PARKING LOT WAS HIT BY CAR COMING OUT OF PARKING LOT DX  LT FOOT FX   \n",
       "153955                                                    38YOM GOT METAL SHAVING IN EYE WHILE CO WORKER WAS GRINDING METAL DX LT CORNEAL AB   \n",
       "\n",
       "        sex  age  event  \n",
       "0         1   57     62  \n",
       "1         1   45     42  \n",
       "2         1   58     26  \n",
       "3         1   33     60  \n",
       "4         1   53     71  \n",
       "...     ...  ...    ...  \n",
       "153951    2   19     71  \n",
       "153952    1   58     63  \n",
       "153953    1   18     64  \n",
       "153954    1   53     24  \n",
       "153955    1   38     66  \n",
       "\n",
       "[153956 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Set print limits\n",
    "pd.options.display.max_colwidth = 3000\n",
    "## Import Data\n",
    "df_injury = pd.read_csv('./data/raw/train.csv')\n",
    "df_injury"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2A449XQWC7DR"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_samples(ratio,train):\n",
    "    great_than2_classes = train['event'].value_counts()[train['event'].value_counts() >2].index\n",
    "    train = train[train['event'].isin(great_than2_classes.to_list())]\n",
    "    train_samples, _ = train_test_split(train,train_size=ratio,random_state=42,stratify=train['event'])\n",
    "    print(\"nb classes\",train_samples['event'].nunique())\n",
    "    print(\"nb oservations:\",train_samples.shape)\n",
    "    print(train_samples['event'].value_counts())\n",
    "    \n",
    "    return train_samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "QMZv1S7RJ_v3"
   },
   "outputs": [],
   "source": [
    "def get_data(data_file,is_sample=None,ratio=None,is_test_split=False):\n",
    "  \n",
    "    train = pd.read_csv(data_file)\n",
    "    if is_sample:\n",
    "        train = get_samples(ratio = ratio, train=train)\n",
    "    great_than2_classes = train['event'].value_counts()[train['event'].value_counts() >5].index \n",
    "    train_filter = train[train['event'].isin(great_than2_classes.to_list())]\n",
    "\n",
    "    X = train_filter['text']\n",
    "    y = train_filter['event']\n",
    "\n",
    "    print(f\"X.shape {X.shape} y.shape : {y.shape}\")\n",
    "\n",
    "    X_train_valid,X_test,y_train_valid, y_test = train_test_split(X,y,train_size=0.9,random_state=42,stratify=y)\n",
    "    \n",
    "    if is_test_split:\n",
    "        X_train,X_valid,y_train,y_valid = train_test_split(X_train_valid,y_train_valid,train_size=0.8,random_state=42,stratify=y_train_valid)\n",
    "\n",
    "    if is_test_split :\n",
    "        print(f\"X_train shape {X_train.shape} y_train shape : {y_train.shape}\")\n",
    "        print(f\"X_valid shape {X_valid.shape} y_valid shape : {y_valid.shape}\")\n",
    "        print(f\"X_test shape {X_test.shape} y_test shape : {y_test.shape}\")\n",
    "        \n",
    "        return {\n",
    "          'train': (X_train,y_train),\n",
    "          'valid': (X_valid,y_valid),\n",
    "          'test': (X_test,y_test)\n",
    "      }\n",
    "\n",
    "    else:\n",
    "        print(f\"X_train shape {X_train_valid.shape} y_train shape : {y_train_valid.shape}\")\n",
    "        print(f\"X_valid shape {X_test.shape} y_valid shape : {y_test.shape}\")\n",
    "        \n",
    "        return {\n",
    "          'train': (X_train_valid,y_train_valid),\n",
    "          'valid': (X_test,y_test),\n",
    "      }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and spliting the training data\n",
    "\n",
    "The dataset includes 153,956 records and 48 classes.For this experiment we did not use any class imbalanced methods , we only kept classes with more than five records and split the train data: 90% for training and 10 % for validation.This reduces the number of classes to 43."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dOSsOuMklCV8",
    "outputId": "77423075-5c1c-4e98-e571-fbe3a16f7fe9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb classes 41\n",
      "nb oservations: (7697, 4)\n",
      "71    1295\n",
      "62    1220\n",
      "42     781\n",
      "55     584\n",
      "63     453\n",
      "60     449\n",
      "11     447\n",
      "73     416\n",
      "43     327\n",
      "70     266\n",
      "64     219\n",
      "53     195\n",
      "13     163\n",
      "66     144\n",
      "26     134\n",
      "12     112\n",
      "41      76\n",
      "99      69\n",
      "24      51\n",
      "31      45\n",
      "78      44\n",
      "27      42\n",
      "72      38\n",
      "51      25\n",
      "52      24\n",
      "44      19\n",
      "32      16\n",
      "23      14\n",
      "25       5\n",
      "69       5\n",
      "61       3\n",
      "67       3\n",
      "21       2\n",
      "65       2\n",
      "40       2\n",
      "22       2\n",
      "45       1\n",
      "49       1\n",
      "79       1\n",
      "20       1\n",
      "50       1\n",
      "Name: event, dtype: int64\n",
      "X.shape (7668,) y.shape : (7668,)\n",
      "X_train shape (6901,) y_train shape : (6901,)\n",
      "X_valid shape (767,) y_valid shape : (767,)\n"
     ]
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 3100\n",
    "data = get_data('./data/raw/train.csv',is_sample=True,ratio=0.05)\n",
    "X_train, y_train = data['train']\n",
    "X_valid,y_valid = data['valid']\n",
    "#X_test,y_test = data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rv6Xg007BWDa",
    "outputId": "ee8ebb0b-15aa-4094-e6d4-d42e75a89803"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes in train : 28\n",
      "classes in valid : 28\n",
      "CLASSSES :  [62, 71, 63, 11, 43, 55, 42, 52, 60, 73, 13, 66, 12, 53, 64, 27, 24, 99, 26, 72, 70, 51, 44, 41, 31, 78, 32, 23]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('classes in train :',len(np.unique(y_train)))\n",
    "print('classes in valid :',len(np.unique(y_valid)))\n",
    "\n",
    "CLASSES = y_train.unique().tolist()\n",
    "print('CLASSSES : ',CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkmrF4fBt7IS"
   },
   "source": [
    "##  Step 3 and 4 -  Cleaning and Pre-processing\n",
    "\n",
    "We clean each narrative in training and validation dataset by removing HTML , tags, punctuation, stop words using NLTK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Niej9Xr2oYPF"
   },
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Applies some pre-processing on the given text.\n",
    "\n",
    "    Steps :\n",
    "    - Removing HTML tags\n",
    "    - Removing punctuation\n",
    "    - Lowering text\n",
    "    \"\"\"\n",
    "\n",
    "    # remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    # remove the characters [\\], ['] and [\"]\n",
    "    text = re.sub(r\"\\\\\", \"\", text)\n",
    "    text = re.sub(r\"\\'\", \"\", text)\n",
    "    text = re.sub(r\"\\\"\", \"\", text)\n",
    "\n",
    "    # convert text to lowercase\n",
    "    text = text.strip().lower()\n",
    "\n",
    "    # remove all non-ASCII characters:\n",
    "    text = re.sub(r'[^\\x00-\\x7f]', r'', text)\n",
    "\n",
    "    # replace punctuation characters with spaces\n",
    "    filters = '!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    translate_dict = dict((c, \" \") for c in filters)\n",
    "    translate_map = str.maketrans(translate_dict)\n",
    "    text = text.translate(translate_map)\n",
    "    text = \" \".join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Hbky6bE6LRSB"
   },
   "outputs": [],
   "source": [
    "def remove_useless_words(text,useless_words):\n",
    "    sentence = [word for word in word_tokenize(text)]\n",
    "    sentence_stop = [word for word in sentence if word not in useless_words]\n",
    "\n",
    "    text = \" \".join(sentence_stop)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "--BcpA6wLSNv"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    \"\"\" Preprocess : cleaning and remove stop words\"\"\"\n",
    "\n",
    "    X = X.apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "    X = X.apply(lambda x: clean_text(x))\n",
    "\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    useless_words = stopwords + list(string.punctuation) + ['yom', 'yof', 'yowm', 'yf', 'ym', 'yo']\n",
    "    # print(\"useless word : \",useless_words)\n",
    "    X = X.apply(lambda x: remove_useless_words(x,useless_words))\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6C8uN_dxLa7D",
    "outputId": "74b26581-4d93-47c6-eb8c-e8663f403a8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after preprocessing...\n",
      "78811                                                                yomcontusion foot metal fell foot work\n",
      "29443                            c hip pain pulling heavy trash bags work friday dx strain right hip flexor\n",
      "146033                                                  hurt chest leaning fish aquarium dx contusion chest\n",
      "92679                                                   assaulted work punched face customer contusion face\n",
      "124907                    reports sus laceration rt palm lost footing whilecoming ladder dx palm laceration\n",
      "110259                                                                    stuck self needle dx ppunc finger\n",
      "62601                                                          unspecified lifting work strained lower back\n",
      "152848                                h ms worsening leg numbness fell tdy work lle weakness ms excerbation\n",
      "113306                            corneal keratitis person welding work pt denies fb eyes woke eyes burning\n",
      "111206    work room divider struck pt wall divider dx comminuted fx r index finger open pain r idnex finger\n",
      "61040                                                acc bumped face door frame mail truck loading work lac\n",
      "97397                                                        headache work slipped fell headache since fall\n",
      "61813                                    f hit leg peice metal carrying wood farm days ago dx lle contusion\n",
      "148140                      carrying heavy pipe work yesterday extreme low back pain dx acute low back pain\n",
      "42556                                                                hand lac cut wrench working truck work\n",
      "76072                                slipped fell hitting lower leg plastic barracade dx abrasion lower leg\n",
      "42304                                                installs carpet living kneeling constantly c knee pain\n",
      "152431                                 work handling dirty syringe stuck self finger accidentally pw finger\n",
      "146732                        male stepped unspecified type pipe work twisted ankle dx right ankle sprain b\n",
      "122473                 twisted lt ankle mud construction site work wks agoswelling numbness dx ankle sprain\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "X_train_processed = preprocess_data(X_train)\n",
    "X_valid_processed = preprocess_data(X_valid)\n",
    "\n",
    "print(\"after preprocessing...\")\n",
    "print(X_train_processed.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 5 - Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_2viYtTtx29"
   },
   "source": [
    "### Tokenize Train and Validation data for DistilBERT\n",
    "To get our features we tokenize our train and validation data using the DistilBertTokenizerFast. To use a BERT like model we need to tokenize the data in the format requested by BERT. Fortunately the transformers library provides the function to do so. DistilBertTokenizerFast converts each text into token_ids and attention_mask using the BERT vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "APf_28XP_GRQ",
    "outputId": "fb5a4b1b-ee79-49a0-9fc3-84d8caa77c2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len:45\n",
      "narrative: '['hurt chest leaning fish aquarium dx contusion chest', 'lbp rad post leg p mech fall parking lot work sciatica']'\n",
      "input ids: [[101, 3480, 3108, 6729, 3869, 18257, 1040, 2595, 9530, 5809, 3258, 3108, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 6053, 2361, 10958, 2094, 2695, 4190, 1052, 2033, 2818, 2991, 5581, 2843, 2147, 16596, 12070, 2050, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "attention mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import  DistilBertTokenizerFast,BertTokenizerFast,AutoTokenizer\n",
    "\n",
    "MAX_LEN = 45\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "narratives = [X_train_processed.iloc[2],X_valid_processed.iloc[2]]\n",
    "inputs = tokenizer(narratives, max_length=MAX_LEN, truncation=True, padding='max_length')\n",
    "\n",
    "print(f\"max_len:{MAX_LEN}\")\n",
    "print(f'narrative: \\'{narratives}\\'')\n",
    "print(f'input ids: {inputs[\"input_ids\"]}')\n",
    "print(f'attention mask: {inputs[\"attention_mask\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "7j2E9Md0ASZA"
   },
   "outputs": [],
   "source": [
    "def construct_encodings(x, tkzr, max_len, trucation=True, padding=True):\n",
    "    return tkzr(x, max_length=max_len, truncation=trucation, padding=padding)\n",
    "\n",
    "x_train = X_train_processed.to_list()\n",
    "x_valid = X_valid_processed.to_list()\n",
    "\n",
    "\n",
    "train_encodings = tokenizer(x_train, max_length=MAX_LEN, truncation=True, padding='max_length',return_tensors='tf')\n",
    "valid_encodings = tokenizer(x_valid, max_length=MAX_LEN, truncation=True, padding='max_length',return_tensors='tf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Js2DJiHzPcdi",
    "outputId": "6b4e8f6c-275e-42fb-99a6-76fee2b96c1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "45\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "print(MAX_LEN)\n",
    "print(len(train_encodings['input_ids'][0]))\n",
    "print(len(valid_encodings['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Labels for training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FtHFsdDcPfGX",
    "outputId": "e845281b-4733-46c0-f9fb-f696f6634fe2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Labels .....\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"Encoding Labels .....\")\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "y_train_encode = np.asarray(encoder.transform(y_train))\n",
    "y_valid_encode = np.asarray(encoder.transform(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f2D8qXPkJU0p",
    "outputId": "c6094835-450d-4f11-bfc2-db3473fd3844"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes in train : 28\n",
      "classes in valid : 28\n"
     ]
    }
   ],
   "source": [
    "print('classes in train :',len(np.unique(y_train_encode)))\n",
    "print('classes in valid :',len(np.unique(y_valid_encode)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHSevTANtrxM"
   },
   "source": [
    "### Convert our data into TensorFlow Dataset\n",
    "To improve training performance we convert the data to Tensorflow dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aVuoi-aLP4hl",
    "outputId": "81b1baa4-0c74-43f7-b17d-02ad04a406d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.56 ms, sys: 94 µs, total: 2.65 ms\n",
      "Wall time: 2.08 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def construct_tfdataset(encodings, y=None):\n",
    "    if y is not None:\n",
    "        return tf.data.Dataset.from_tensor_slices((dict(encodings),y))\n",
    "    else:\n",
    "        # this case is used when making predictions on unseen samples after training\n",
    "        return tf.data.Dataset.from_tensor_slices(dict(encodings))\n",
    "    \n",
    "train_tfdataset = construct_tfdataset(train_encodings, y_train_encode)\n",
    "valid_tfdataset = construct_tfdataset(valid_encodings, y_valid_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8svxfCKeP4eF",
    "outputId": "61eca96c-42c4-454b-ff74-76a7595703d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: ({input_ids: (45,), attention_mask: (45,)}, ()), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.int64)>\n",
      "<TensorSliceDataset shapes: ({input_ids: (45,), attention_mask: (45,)}, ()), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "print(train_tfdataset)\n",
    "print(valid_tfdataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS= 1\n",
    "BATCH_SIZE = 16 \n",
    "steps_per_epoch = len(X_train) // BATCH_SIZE\n",
    "\n",
    "train_tfdataset = train_tfdataset.repeat(N_EPOCHS * steps_per_epoch)\n",
    "train_tfdataset = train_tfdataset.prefetch(tf.data.AUTOTUNE)\n",
    "train_tfdataset = train_tfdataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_batch_size = 16\n",
    "validation_steps = len(X_valid) // valid_batch_size\n",
    "\n",
    "valid_tfdataset = valid_tfdataset.repeat(N_EPOCHS * validation_steps)\n",
    "valid_tfdataset = valid_tfdataset.prefetch(tf.data.AUTOTUNE)\n",
    "valid_tfdataset = valid_tfdataset.batch(validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ({input_ids: (None, 45), attention_mask: (None, 45)}, (None,)), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.int64)>\n",
      "<BatchDataset shapes: ({input_ids: (None, 45), attention_mask: (None, 45)}, (None,)), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "print(train_tfdataset)\n",
    "print(valid_tfdataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wz1FDvTJtX16"
   },
   "source": [
    "## Step 6 - Modeling and Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  6.1 Fine Tune DistilBERT\n",
    "Fine-tuning is the process of leveraging the knowledge from pre-trained weights by initializing your model with those weights and training it for your specific downstream task. In our case we used the 67M pre-trained parameters from DistilBERT and added on top of it a classifier using Convolutional 1D Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "cF_bToiYP4av"
   },
   "outputs": [],
   "source": [
    "from transformers import  TFDistilBertForSequenceClassification\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "from transformers.optimization_tf import WarmUp, AdamWeightDecay\n",
    "\n",
    "class DistillBERT:\n",
    "    def __init__(self, params):\n",
    "        self.num_labels = params['num_labels']\n",
    "        self.max_len = params['max_len']\n",
    "        self.learning_rate = params['learning_rate']\n",
    "        self.num_records = params['num_records']\n",
    "        self.batch_size = params['batch_size']\n",
    "        self.epochs = params['epochs']\n",
    "        \n",
    "    def build(self):\n",
    "        \n",
    "        freeze_bert_layer = False\n",
    "        \n",
    "        # Load model and tokenizer\n",
    "        transformer_model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=self.num_labels)\n",
    "        input_ids = tf.keras.Input(shape = (self.max_len,),name='input_ids',dtype='int32')\n",
    "        attention_mask = tf.keras.Input(shape = (self.max_len,),name='attention_mask',dtype='int32')\n",
    "        \n",
    "        embbeding_layer = transformer_model.distilbert(input_ids,attention_mask=attention_mask)[0]\n",
    "        #X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256,return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(embbeding_layer)\n",
    "        X = tf.keras.layers.Conv1D(512, 3, activation='relu')(embbeding_layer)\n",
    "        X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
    "        X = tf.keras.layers.Dense(256, activation='relu')(X)\n",
    "        X = tf.keras.layers.Dropout(0.2)(X)\n",
    "        X = tf.keras.layers.Dense(self.num_labels, activation='softmax')(X)\n",
    "        \n",
    "        model = tf.keras.Model(inputs=[input_ids,attention_mask], outputs = X)\n",
    "        \n",
    "        for layer in model.layers[:3]:\n",
    "            layer.trainable = not freeze_bert_layer\n",
    "\n",
    "        num_train_steps = (self.num_records // self.batch_size) * self.epochs\n",
    "        decay_schedule_fn = PolynomialDecay(\n",
    "            initial_learning_rate=self.learning_rate,\n",
    "            end_learning_rate=0.,\n",
    "            decay_steps=num_train_steps\n",
    "            )\n",
    "        \n",
    "        warmup_steps  = num_train_steps * 0.1\n",
    "        warmup_schedule = WarmUp(self.learning_rate,decay_schedule_fn,warmup_steps)\n",
    "\n",
    "          # fine optimizer and loss\n",
    "        #optimizer = tf.keras.optimizers.Adam(learning_rate=warmup_schedule,epsilon=1e-06)\n",
    "\n",
    "        optimizer = AdamWeightDecay(learning_rate=warmup_schedule, weight_decay_rate=0.01, epsilon=1e-6)\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits= False)\n",
    "        metrics = ['acc']\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Summary( DistilBERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the TFDistilBertForSequenceClassification class provided by the transformer library. We also used recommended hyperparameters (after several tuning experiments) suggested by Chi Sun et al 2019 and Mosbash et al 2020. We trained for 5 epochs with a batch size 16. We used slanted triangular learning rates (Howard and Ruder, 2018) with a base learning rate of 5e-5 and a warmup portion 0.1. We used AdamWeightDecay optimizer with a weight_decay_rate = 0.01 and epsilon 1e-6. We trained all layers which includes the complete 67M+ parameters of our model.\n",
    "\n",
    "\n",
    "***We will use same configuration for BERT and Roberta***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qV5Qp85sRy5g",
    "outputId": "35a53441-3092-4c2a-e971-4e0ef779b45b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'activation_13', 'vocab_layer_norm', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'classifier', 'dropout_261']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 45)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 45)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "distilbert (TFDistilBertMainLay TFBaseModelOutput(la 66362880    input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 43, 512)      1180160     distilbert[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 512)          0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 256)          131328      global_max_pooling1d_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_262 (Dropout)           (None, 256)          0           dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 28)           7196        dropout_262[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 67,681,564\n",
      "Trainable params: 67,681,564\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 5e-5\n",
    "num_records = len(X_train)\n",
    "num_valid_records = len(X_valid)\n",
    "max_len = MAX_LEN\n",
    "epochs = 1\n",
    "batch_size = 16\n",
    "valid_batch_size = 16\n",
    "steps_per_epoch = num_records // batch_size\n",
    "validation_steps = num_valid_records // valid_batch_size\n",
    "\n",
    "params = {\n",
    "        'num_labels': len(CLASSES),\n",
    "        'max_len': MAX_LEN,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'num_records':len(X_train),\n",
    "        'batch_size':batch_size,\n",
    "        'epochs':epochs\n",
    "    }\n",
    "  \n",
    "model = DistillBERT(params).build()   \n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training / Finetune DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431/431 [==============================] - 598s 1s/step - loss: 1.4631 - acc: 0.5727 - val_loss: 0.8852 - val_acc: 0.7316\n",
      "CPU times: user 1h 1min 28s, sys: 4min 17s, total: 1h 5min 46s\n",
      "Wall time: 9min 58s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5d32090790>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(x=train_tfdataset,\n",
    "          steps_per_epoch = steps_per_epoch,\n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs,\n",
    "          validation_steps = validation_steps,\n",
    "          validation_data=valid_tfdataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate validation DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "767\n",
      "431\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "num_valid_records = len(X_valid)\n",
    "steps_per_epoch = num_records // batch_size\n",
    "validation_steps = num_valid_records // valid_batch_size\n",
    "print(num_valid_records)\n",
    "print(steps_per_epoch)\n",
    "print(validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zlYKKmNHUJnT",
    "outputId": "f849c4a3-dcb2-48da-f50a-5cd7800301be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 41s 864ms/step - loss: 0.8852 - acc: 0.7316\n",
      "CPU times: user 4min 42s, sys: 1 s, total: 4min 43s\n",
      "Wall time: 40.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.8851924538612366, 'acc': 0.7315527200698853}"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "distil_bert_valid_score = model.evaluate(x=valid_tfdataset, steps = validation_steps,return_dict=True)\n",
    "distil_bert_valid_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Training DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6901\n",
      "431\n"
     ]
    }
   ],
   "source": [
    "num_records = len(X_train)\n",
    "steps_per_epoch = num_records // batch_size\n",
    "print(num_records)\n",
    "print(steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431/431 [==============================] - 140s 324ms/step - loss: 0.6925 - acc: 0.7942\n",
      "CPU times: user 15min 6s, sys: 6.89 s, total: 15min 13s\n",
      "Wall time: 2min 19s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.6924773454666138, 'acc': 0.7942285537719727}"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "distil_bert_train_score = model.evaluate(x=train_tfdataset,steps = steps_per_epoch, return_dict=True)\n",
    "distil_bert_train_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('./output/',exist_ok = True)\n",
    "model.save(\"./output/distilbert.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create dict to store all these results:\n",
    "result_scores = {}\n",
    "\n",
    "## Score the Model on Training and Testing Set\n",
    "result_scores['DistilBERT'] =(distil_bert_train_score,distil_bert_valid_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DistilBERT': ({'loss': 0.6924773454666138, 'acc': 0.7942285537719727},\n",
       "  {'loss': 0.8851924538612366, 'acc': 0.7315527200698853})}"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Function to Print Results\n",
    "def get_results(x1):\n",
    "    print(\"\\n{0:20}   {1:4}    {2:4} \".format('Model','Train','Validation'))\n",
    "    print('-------------------------------------------')\n",
    "    for key in x1.keys():\n",
    "        print(\"{0:20}   {1:<6.4}   {2:<6.4}\".format(key,x1[key][0]['acc'],x1[key][1]['acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model                  Train    Validation \n",
      "-------------------------------------------\n",
      "DistilBERT             0.7942   0.7316\n"
     ]
    }
   ],
   "source": [
    "get_results(result_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLwXY4AvtidP"
   },
   "source": [
    "### Step 6.2 - FineTune  BaseBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "id": "Uj_61ZGez16o"
   },
   "outputs": [],
   "source": [
    "from transformers import TFRobertaForSequenceClassification,TFBertForSequenceClassification\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "from transformers.optimization_tf import WarmUp, AdamWeightDecay\n",
    "class BaseBERT:\n",
    "    def __init__(self, params):\n",
    "        self.num_labels = params['num_labels']\n",
    "        self.max_len = params['max_len']\n",
    "        self.learning_rate = params['learning_rate']\n",
    "        self.num_records = params['num_records']\n",
    "        self.batch_size = params['batch_size']\n",
    "        self.epochs = params['epochs']\n",
    "        \n",
    "    def build(self):\n",
    "        \n",
    "        freeze_bert_layer = False\n",
    "        \n",
    "        # Load model and tokenizer\n",
    "        transformer_model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=self.num_labels)\n",
    "        input_ids = tf.keras.Input(shape = (self.max_len,),name='input_ids',dtype='int32')\n",
    "        attention_mask = tf.keras.Input(shape = (self.max_len,),name='attention_mask',dtype='int32')\n",
    "        \n",
    "        embbeding_layer = transformer_model.bert(input_ids,attention_mask=attention_mask)[0]\n",
    "        #X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256,return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(embbeding_layer)\n",
    "        X = tf.keras.layers.Conv1D(512, 3, activation='relu')(embbeding_layer)\n",
    "        X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
    "        X = tf.keras.layers.Dense(256, activation='relu')(X)\n",
    "        X = tf.keras.layers.Dropout(0.2)(X)\n",
    "        X = tf.keras.layers.Dense(self.num_labels, activation='softmax')(X)\n",
    "        \n",
    "        model = tf.keras.Model(inputs=[input_ids,attention_mask], outputs = X)\n",
    "        \n",
    "        for layer in model.layers[:3]:\n",
    "            layer.trainable = not freeze_bert_layer\n",
    "\n",
    "        num_train_steps = (self.num_records // self.batch_size) * self.epochs\n",
    "        decay_schedule_fn = PolynomialDecay(\n",
    "            initial_learning_rate=self.learning_rate,\n",
    "            end_learning_rate=0.,\n",
    "            decay_steps=num_train_steps\n",
    "            )\n",
    "        \n",
    "        warmup_steps  = num_train_steps * 0.1\n",
    "        warmup_schedule = WarmUp(self.learning_rate,decay_schedule_fn,warmup_steps)\n",
    "\n",
    "          # fine optimizer and loss\n",
    "        #optimizer = tf.keras.optimizers.Adam(learning_rate=warmup_schedule,epsilon=1e-06)\n",
    "\n",
    "        optimizer = AdamWeightDecay(learning_rate=warmup_schedule, weight_decay_rate=0.01, epsilon=1e-6)\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits= False)\n",
    "        metrics = ['acc']\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Model Summary (BaseBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F5V9_ef40YUT",
    "outputId": "0485897e-38dd-4e9f-ca06-eafe117887e6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 45)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 45)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (TFBertMainLayer)          TFBaseModelOutputWit 109482240   input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 43, 512)      1180160     bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 512)          0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 256)          131328      global_max_pooling1d_10[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_339 (Dropout)           (None, 256)          0           dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 28)           7196        dropout_339[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 110,800,924\n",
      "Trainable params: 110,800,924\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 5e-5\n",
    "num_records = len(X_train)\n",
    "num_valid_records = len(X_valid)\n",
    "max_len = MAX_LEN\n",
    "epochs = 1\n",
    "batch_size = 16\n",
    "valid_batch_size = 16\n",
    "steps_per_epoch = num_records // batch_size\n",
    "validation_steps = num_valid_records // valid_batch_size\n",
    "params = {\n",
    "        'num_labels': len(CLASSES),\n",
    "        'max_len': MAX_LEN,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'num_records':len(X_train),\n",
    "        'batch_size':16,\n",
    "        'epochs':1\n",
    "    }\n",
    "  \n",
    "model2 = BaseBERT(params).build()   \n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training BaseBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification_2/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification_2/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification_2/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification_2/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "431/431 [==============================] - 1092s 2s/step - loss: 1.5026 - acc: 0.5737 - val_loss: 0.8823 - val_acc: 0.7352\n",
      "CPU times: user 1h 57min 13s, sys: 5min 11s, total: 2h 2min 24s\n",
      "Wall time: 18min 11s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5e02f33610>"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model2.fit(x=train_tfdataset,\n",
    "          steps_per_epoch = steps_per_epoch,\n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs,\n",
    "          validation_steps = validation_steps,\n",
    "          validation_data=valid_tfdataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Evaluate Validation BaseBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 80s 2s/step - loss: 0.8823 - acc: 0.7352\n",
      "CPU times: user 9min 18s, sys: 1.3 s, total: 9min 19s\n",
      "Wall time: 1min 20s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.8822953701019287, 'acc': 0.735174298286438}"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "bert_valid_score = model2.evaluate(x=valid_tfdataset, steps = validation_steps,return_dict=True)\n",
    "bert_valid_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Training BaseBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431/431 [==============================] - 273s 633ms/step - loss: 0.6527 - acc: 0.7996\n",
      "CPU times: user 29min 40s, sys: 12 s, total: 29min 52s\n",
      "Wall time: 4min 32s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.6526983380317688, 'acc': 0.799593985080719}"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "bert_train_score = model2.evaluate(x=train_tfdataset,steps = steps_per_epoch, return_dict=True)\n",
    "bert_train_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./output/',exist_ok = True)\n",
    "model2.save(\"./output/bert.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Score the Model on Training and Testing Set\n",
    "result_scores['BERT'] =(bert_train_score,bert_valid_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model                  Train    Validation \n",
      "-------------------------------------------\n",
      "DistilBERT             0.7942   0.7316\n",
      "BERT                   0.7996   0.7352\n"
     ]
    }
   ],
   "source": [
    "get_results(result_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 6.3 -  FineTune  Base Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import logging\n",
    "#transformers.logging.get_verbosity = lambda: logging.NOTSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len:45\n",
      "narrative: '['hurt chest leaning fish aquarium dx contusion chest', 'lbp rad post leg p mech fall parking lot work sciatica']'\n",
      "input ids: [[0, 298, 7363, 7050, 19146, 3539, 33734, 49386, 8541, 15727, 7050, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 17243, 642, 13206, 618, 2985, 181, 46833, 1136, 2932, 319, 173, 19974, 5183, 102, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "attention mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import  DistilBertTokenizerFast,BertTokenizerFast,AutoTokenizer,RobertaTokenizer\n",
    "\n",
    "MAX_LEN = 45\n",
    "MODEL_NAME = 'roberta-base'\n",
    "#MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "narratives = [X_train_processed.iloc[2],X_valid_processed.iloc[2]]\n",
    "inputs = tokenizer(narratives, max_length=MAX_LEN, truncation=True, padding='max_length')\n",
    "\n",
    "print(f\"max_len:{MAX_LEN}\")\n",
    "print(f'narrative: \\'{narratives}\\'')\n",
    "print(f'input ids: {inputs[\"input_ids\"]}')\n",
    "print(f'attention mask: {inputs[\"attention_mask\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfdataset = construct_tfdataset(train_encodings, y_train_encode)\n",
    "valid_tfdataset = construct_tfdataset(valid_encodings, y_valid_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS= 1\n",
    "BATCH_SIZE = 16 \n",
    "steps_per_epoch = len(X_train) // BATCH_SIZE\n",
    "\n",
    "train_tfdataset = train_tfdataset.repeat(N_EPOCHS * steps_per_epoch)\n",
    "train_tfdataset = train_tfdataset.prefetch(tf.data.AUTOTUNE)\n",
    "train_tfdataset = train_tfdataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_batch_size = 16\n",
    "validation_steps = len(X_valid) // valid_batch_size\n",
    "\n",
    "valid_tfdataset = valid_tfdataset.repeat(N_EPOCHS * validation_steps)\n",
    "valid_tfdataset = valid_tfdataset.prefetch(tf.data.AUTOTUNE)\n",
    "valid_tfdataset = valid_tfdataset.batch(validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFRobertaForSequenceClassification,TFBertForSequenceClassification\n",
    "\n",
    "class BaseRoberta:\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        self.num_labels = params['num_labels']\n",
    "        self.max_len = params['max_len']\n",
    "        self.learning_rate = params['learning_rate']\n",
    "        self.batch_size = params['batch_size']\n",
    "        self.epochs = params['epochs']\n",
    "        self.num_records = params['num_records']\n",
    "        \n",
    "    def build(self):      \n",
    "        \n",
    "        freeze_bert_layer = False\n",
    "        \n",
    "        transformer_model = TFRobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=self.num_labels)\n",
    "        input_ids = tf.keras.Input(shape = (self.max_len,),name='input_ids',dtype='int32')\n",
    "        attention_mask = tf.keras.Input(shape = (self.max_len,),name='attention_mask',dtype='int32')\n",
    "        \n",
    "        embbeding_layer = transformer_model.roberta(input_ids,attention_mask=attention_mask)[0]\n",
    "        X = tf.keras.layers.GlobalMaxPool1D()(embbeding_layer)\n",
    "        X = tf.keras.layers.Dense(self.num_labels, activation='softmax')(X)\n",
    "        \n",
    "        model = tf.keras.Model(inputs=[input_ids,attention_mask], outputs = X)\n",
    "        \n",
    "        for layer in model.layers[:3]:\n",
    "            layer.trainable = not freeze_bert_layer\n",
    "        \n",
    "        num_train_steps = (self.num_records // self.batch_size) * self.epochs\n",
    "        decay_schedule_fn = PolynomialDecay(\n",
    "            initial_learning_rate=self.learning_rate,\n",
    "            end_learning_rate=0.,\n",
    "            decay_steps=num_train_steps\n",
    "            )\n",
    "        \n",
    "        warmup_steps = num_train_steps * 0.1\n",
    "        warmup_schedule  = WarmUp(self.learning_rate,decay_schedule_fn,warmup_steps)\n",
    "        \n",
    "        #optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        optimizer = AdamWeightDecay (learning_rate=warmup_schedule, weight_decay_rate=0.01, epsilon=1e-6)\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "        \n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Summary ( Base Roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 45)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 45)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "roberta (TFRobertaMainLayer)    TFBaseModelOutputWit 124055040   input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 768)          0           roberta[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 28)           21532       global_max_pooling1d_11[0][0]    \n",
      "==================================================================================================\n",
      "Total params: 124,076,572\n",
      "Trainable params: 124,076,572\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 5e-5\n",
    "num_records = len(X_train)\n",
    "num_valid_records = len(X_valid)\n",
    "max_len = MAX_LEN\n",
    "epochs = 1\n",
    "batch_size = 16\n",
    "valid_batch_size = 16\n",
    "steps_per_epoch = num_records // batch_size\n",
    "validation_steps = num_valid_records // valid_batch_size\n",
    "params = {\n",
    "        'num_labels': len(CLASSES),\n",
    "        'max_len': MAX_LEN,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'num_records':len(X_train),\n",
    "        'batch_size':batch_size,\n",
    "        'epochs':epochs\n",
    "    }\n",
    "  \n",
    "model3 = BaseRoberta(params).build()   \n",
    "print(model3.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training BaseRoberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431/431 [==============================] - 1141s 3s/step - loss: 2.1707 - acc: 0.3624 - val_loss: 1.5880 - val_acc: 0.5215\n",
      "CPU times: user 1h 58min 39s, sys: 7min 41s, total: 2h 6min 20s\n",
      "Wall time: 19min 1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5e28ba3290>"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model3.fit(x=train_tfdataset,\n",
    "          steps_per_epoch = steps_per_epoch,\n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs,\n",
    "          validation_steps = validation_steps,\n",
    "          validation_data=valid_tfdataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate validation Base Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 80s 2s/step - loss: 1.5880 - acc: 0.5215\n",
      "CPU times: user 9min 17s, sys: 1.98 s, total: 9min 19s\n",
      "Wall time: 1min 20s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 1.5880324840545654, 'acc': 0.5215029716491699}"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "roberta_valid_score = model3.evaluate(x=valid_tfdataset, steps = validation_steps,return_dict=True)\n",
    "roberta_valid_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Training data Base Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431/431 [==============================] - 265s 615ms/step - loss: 1.4413 - acc: 0.5293\n",
      "CPU times: user 28min 52s, sys: 12.6 s, total: 29min 5s\n",
      "Wall time: 4min 25s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 1.441288709640503, 'acc': 0.5292923450469971}"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "roberta_train_score = model3.evaluate(x=train_tfdataset,steps = steps_per_epoch, return_dict=True)\n",
    "roberta_train_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./output/',exist_ok = True)\n",
    "model.save('./output/roberta.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Score the Model on Training and Testing Set\n",
    "result_scores['Roberta'] =(roberta_train_score,roberta_valid_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model                  Train    Validation \n",
      "-------------------------------------------\n",
      "DistilBERT             0.7942   0.7316\n",
      "BERT                   0.7996   0.7352\n",
      "Roberta                0.5293   0.5215\n"
     ]
    }
   ],
   "source": [
    "get_results(result_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Evaluation on Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(test_data_file, is_sample, ratio):\n",
    "    \n",
    "    test_data = pd.read_csv(test_data_file)\n",
    "    test_data = test_data[~test_data['event'].isin([10,29,30,59,74])]\n",
    "    test_data = test_data[['text','event']]\n",
    "    \n",
    "    if is_sample:\n",
    "        test_data = get_samples(ratio = ratio, train=test_data) \n",
    "        great_than5_classes = test_data['event'].value_counts()[test_data['event'].value_counts() >5].index\n",
    "        test_data = test_data[test_data['event'].isin(great_than5_classes.to_list())]\n",
    "    \n",
    "    print(\"nb classes in final data:\",test_data['event'].nunique())\n",
    "    print(f\"test_data_small.shape {test_data.shape}\")\n",
    "    \n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb classes 39\n",
      "nb oservations: (3784, 2)\n",
      "71    634\n",
      "62    601\n",
      "42    385\n",
      "55    284\n",
      "63    223\n",
      "60    221\n",
      "11    220\n",
      "73    205\n",
      "43    161\n",
      "70    130\n",
      "64    108\n",
      "53     96\n",
      "13     80\n",
      "66     71\n",
      "26     66\n",
      "12     55\n",
      "41     37\n",
      "99     34\n",
      "24     25\n",
      "31     22\n",
      "78     22\n",
      "27     21\n",
      "72     19\n",
      "51     12\n",
      "52     12\n",
      "44      9\n",
      "32      8\n",
      "23      7\n",
      "69      3\n",
      "25      3\n",
      "67      2\n",
      "40      1\n",
      "22      1\n",
      "65      1\n",
      "50      1\n",
      "61      1\n",
      "49      1\n",
      "45      1\n",
      "21      1\n",
      "Name: event, dtype: int64\n",
      "nb classes in final data: 28\n",
      "test_data_small.shape (3768, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37427                                                              male hurt bending work dx knee pain b\n",
       "3526                                    works construction door fell hitting head loc c neck pain chi ms\n",
       "8292       c l finger pain work l th digit removing door panel crushed dx finger contu subungal hematoma\n",
       "63604                                                 wks lows heavy lifting h worsening lbp atypical cp\n",
       "11228    f pt work yesterday slipped fell onto floor hitting head loc altered mental status today dx chi\n",
       "                                                      ...                                               \n",
       "36432                               drives subject bus lots lifting pushing people wheelchairs back pain\n",
       "16967                                          work handling concrete got rash hands contact dermat itis\n",
       "49205                                                                                sexual assault work\n",
       "2439                               work hit open freezer door c r shoulder pain dx acute r shoulder pain\n",
       "9297                                              dx knee abrasion p pt assaulted work kicked x days ago\n",
       "Name: text, Length: 3768, dtype: object"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = get_test_data('./data/raw/test_data.csv',is_sample=True, ratio=0.05)\n",
    "X_test, y_test = test_data['text'], test_data['event']\n",
    "X_test_processed = preprocess_data(X_test)\n",
    "X_test_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 12, 13, 23, 24, 26, 27, 31, 32, 41, 42, 43, 44, 51, 52, 53, 55, 60, 62, 63, 64, 66, 70, 71, 72, 73, 78, 99]\n",
      "[11, 12, 13, 23, 24, 26, 27, 31, 32, 41, 42, 43, 44, 51, 52, 53, 55, 60, 62, 63, 64, 66, 70, 71, 72, 73, 78, 99]\n"
     ]
    }
   ],
   "source": [
    "train_classes =y_train.unique().tolist()\n",
    "test_classes = y_test.unique().tolist()\n",
    "\n",
    "print(sorted(train_classes))\n",
    "print(sorted(test_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model('./output/roberta.h5',custom_objects={'AdamWeightDecay':AdamWeightDecay})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.41 ms, sys: 57 µs, total: 1.47 ms\n",
      "Wall time: 1.16 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "MAX_LEN = 45\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "x_test = X_test_processed.reset_index(drop=True).tolist()\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Test data...\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " indices[14,11] = 49386 is not in [0, 30522)\n\t [[node model_9/distilbert/embeddings/Gather (defined at /usr/local/lib/python3.7/site-packages/transformers/models/distilbert/modeling_tf_distilbert.py:112) ]] [Op:__inference_test_function_299355]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node model_9/distilbert/embeddings/Gather:\n IteratorGetNext (defined at <ipython-input-261-6e8d573a5fa9>:9)\n\nFunction call stack:\ntest_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-261-6e8d573a5fa9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluating Test data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfdataset_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test loss: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test accuracy: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1501\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    955\u001b[0m       \u001b[0;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m       return self._concrete_stateful_fn._call_flat(\n\u001b[0;32m--> 957\u001b[0;31m           filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfn_with_cond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_kwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_filtered_flat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  indices[14,11] = 49386 is not in [0, 30522)\n\t [[node model_9/distilbert/embeddings/Gather (defined at /usr/local/lib/python3.7/site-packages/transformers/models/distilbert/modeling_tf_distilbert.py:112) ]] [Op:__inference_test_function_299355]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node model_9/distilbert/embeddings/Gather:\n IteratorGetNext (defined at <ipython-input-261-6e8d573a5fa9>:9)\n\nFunction call stack:\ntest_function\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "encodings_x_test =  tokenizer(x_test, max_length=MAX_LEN, truncation=True, padding='max_length',return_tensors='tf')\n",
    "y_test_encode = np.asarray(encoder.transform(y_test))\n",
    "tfdataset_test = construct_tfdataset(encodings_x_test,y_test_encode).batch(16)\n",
    "\n",
    "print(\"Evaluating Test data...\")\n",
    "test_score = loaded_model.evaluate(tfdataset_test, steps = validation_steps,batch_size=16)\n",
    "print(\"Test loss: \", test_score[0])\n",
    "print(\"Test accuracy: \", test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Injury narrative classification with transformers.ipynb",
   "provenance": []
  },
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.3 Python 3.7 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.3-gpu-py37-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
