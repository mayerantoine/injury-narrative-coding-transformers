{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Injury classification with SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://wandb.ai/jack-morris/david-vs-goliath/reports/Does-Model-Size-Matter-A-Comparison-of-BERT-and-DistilBERT--VmlldzoxMDUxNzU\n",
    "#https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a\n",
    "#https://towardsdatascience.com/scale-neural-network-training-with-sagemaker-distributed-8cf3aefcff51\n",
    "#https://towardsdatascience.com/how-to-reduce-training-time-for-a-deep-learning-model-using-tf-data-43e1989d2961\n",
    "#https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
    "#https://towardsdatascience.com/to-distil-or-not-to-distil-bert-roberta-and-xlnet-c777ad92f8\n",
    "#https://ymeadows.com/articles/fine-tuning-transformer-based-language-models\n",
    "\n",
    "### Smart Batching\n",
    "#https://towardsdatascience.com/divide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e\n",
    "#https://www.youtube.com/watch?v=ynOZUNnbEWU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.10\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install tensorflow\n",
    "!pip install transformers\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import nltk\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import activations, optimizers, losses\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import joblib \n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name AmazonSageMaker-ExecutionRole-20210423T122185 to get Role path.\n",
      "Assuming role was created in SageMaker AWS console, as the name contains `AmazonSageMaker-ExecutionRole`. Defaulting to Role ARN with service-role in path. If this Role ARN is incorrect, please add IAM read permissions to your role or supply the Role Arn directly.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sagemaker-us-east-1-979294212144'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n"
     ]
    }
   ],
   "source": [
    "%store -z\n",
    "%store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Import and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(ratio,train):\n",
    "    great_than2_classes = train['event'].value_counts()[train['event'].value_counts() >2].index\n",
    "    train = train[train['event'].isin(great_than2_classes.to_list())]\n",
    "    train_samples, _ = train_test_split(train,train_size=ratio,random_state=42,stratify=train['event'])\n",
    "    print(\"nb classes in samples\",train_samples['event'].nunique())\n",
    "    print(\"nb oservations:\",train_samples.shape)\n",
    "    #print(train_samples['event'].value_counts())\n",
    "\n",
    "    return train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_file,is_sample=None,ratio=None,is_test_split=False):\n",
    "  \n",
    "    train = pd.read_csv(data_file)\n",
    "    if is_sample:\n",
    "        train = get_samples(ratio = ratio, train=train)\n",
    "    great_than2_classes = train['event'].value_counts()[train['event'].value_counts() >5].index \n",
    "    train_filter = train[train['event'].isin(great_than2_classes.to_list())]\n",
    "    \n",
    "    print(\"nb classes in final data:\",train_filter['event'].nunique())\n",
    "\n",
    "    X = train_filter['text']\n",
    "    y = train_filter['event']\n",
    "\n",
    "    print(f\"X.shape {X.shape} y.shape : {y.shape}\")\n",
    "\n",
    "    X_train_valid,X_test,y_train_valid, y_test = train_test_split(X,y,train_size=0.9,random_state=42,stratify=y)\n",
    "    \n",
    "    if is_test_split:\n",
    "        X_train,X_valid,y_train,y_valid = train_test_split(X_train_valid,y_train_valid,train_size=0.8,random_state=42,stratify=y_train_valid)\n",
    "\n",
    "    if is_test_split :\n",
    "        print(f\"X_train shape {X_train.shape} y_train shape : {y_train.shape}\")\n",
    "        print(f\"X_valid shape {X_valid.shape} y_valid shape : {y_valid.shape}\")\n",
    "        print(f\"X_test shape {X_test.shape} y_test shape : {y_test.shape}\")\n",
    "        \n",
    "        return {\n",
    "          'train': (X_train,y_train),\n",
    "          'valid': (X_valid,y_valid),\n",
    "          'test': (X_test,y_test)\n",
    "      }\n",
    "\n",
    "    else:\n",
    "        print(f\"X_train shape {X_train_valid.shape} y_train shape : {y_train_valid.shape}\")\n",
    "        print(f\"X_valid shape {X_test.shape} y_valid shape : {y_test.shape}\")\n",
    "        \n",
    "        return {\n",
    "          'train': (X_train_valid,y_train_valid),\n",
    "          'valid': (X_test,y_test),\n",
    "      }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb classes in samples 41\n",
      "nb oservations: (7697, 4)\n",
      "nb classes in final data: 28\n",
      "X.shape (7668,) y.shape : (7668,)\n",
      "X_train shape (6901,) y_train shape : (6901,)\n",
      "X_valid shape (767,) y_valid shape : (767,)\n"
     ]
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 3100\n",
    "data = get_data(data_file='./data/raw/train.csv',is_sample=True,ratio=0.05)\n",
    "#data = get_data(is_test_split=True)\n",
    "X_train, y_train = data['train']\n",
    "X_valid,y_valid = data['valid']\n",
    "#X_test,y_test = data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes in train : 28\n",
      "classes in valid : 28\n",
      "[62, 71, 63, 11, 43, 55, 42, 52, 60, 73, 13, 66, 12, 53, 64, 27, 24, 99, 26, 72, 70, 51, 44, 41, 31, 78, 32, 23]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('classes in train :',len(np.unique(y_train)))\n",
    "print('classes in valid :',len(np.unique(y_valid)))\n",
    "\n",
    "CLASSES = y_train.unique().tolist()\n",
    "print(CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Applies some pre-processing on the given text.\n",
    "\n",
    "    Steps :\n",
    "    - Removing HTML tags\n",
    "    - Removing punctuation\n",
    "    - Lowering text\n",
    "    \"\"\"\n",
    "\n",
    "    # remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    # remove the characters [\\], ['] and [\"]\n",
    "    text = re.sub(r\"\\\\\", \"\", text)\n",
    "    text = re.sub(r\"\\'\", \"\", text)\n",
    "    text = re.sub(r\"\\\"\", \"\", text)\n",
    "\n",
    "    # convert text to lowercase\n",
    "    text = text.strip().lower()\n",
    "\n",
    "    # remove all non-ASCII characters:\n",
    "    text = re.sub(r'[^\\x00-\\x7f]', r'', text)\n",
    "\n",
    "    # replace punctuation characters with spaces\n",
    "    filters = '!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    translate_dict = dict((c, \" \") for c in filters)\n",
    "    translate_map = str.maketrans(translate_dict)\n",
    "    text = text.translate(translate_map)\n",
    "    text = \" \".join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_useless_words(text,useless_words):\n",
    "    sentence = [word for word in word_tokenize(text)]\n",
    "    sentence_stop = [word for word in sentence if word not in useless_words]\n",
    "\n",
    "    text = \" \".join(sentence_stop)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    \"\"\" Preprocess : cleaning and remove stop words\"\"\"\n",
    "\n",
    "    X = X.apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "    X = X.apply(lambda x: clean_text(x))\n",
    "\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    useless_words = stopwords + list(string.punctuation) + ['yom', 'yof', 'yowm', 'yf', 'ym', 'yo']\n",
    "    # print(\"useless word : \",useless_words)\n",
    "    X = X.apply(lambda x: remove_useless_words(x,useless_words))\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after preprocessing...\n",
      "78811                                                yomcontusion foot metal fell foot work\n",
      "29443            c hip pain pulling heavy trash bags work friday dx strain right hip flexor\n",
      "146033                                  hurt chest leaning fish aquarium dx contusion chest\n",
      "92679                                   assaulted work punched face customer contusion face\n",
      "124907    reports sus laceration rt palm lost footing whilecoming ladder dx palm laceration\n",
      "Name: text, dtype: object\n",
      "CPU times: user 1.45 s, sys: 2.89 ms, total: 1.46 s\n",
      "Wall time: 1.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_processed = preprocess_data(X_train)\n",
    "X_valid_processed = preprocess_data(X_valid)\n",
    "\n",
    "print(\"after preprocessing...\")\n",
    "print(X_train_processed.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##TODO Needs to fix sampling strategy to work for percentage/ no har coded classes\n",
    "\n",
    "def get_test_data(test_data_file, is_sample, ratio):\n",
    "    \n",
    "    test_data = pd.read_csv(test_data_file)\n",
    "    test_data = test_data[~test_data['event'].isin([10,29,30,59,74])]\n",
    "    test_data = test_data[['text','event']]\n",
    "    \n",
    "    if is_sample:\n",
    "        test_data = get_samples(ratio = ratio, train=test_data) \n",
    "        great_than5_classes = test_data['event'].value_counts()[test_data['event'].value_counts() >5].index\n",
    "        test_data = test_data[test_data['event'].isin(great_than5_classes.to_list())]\n",
    "    \n",
    "    print(\"nb classes in final data:\",test_data['event'].nunique())\n",
    "    print(f\"test_data_small.shape {test_data.shape}\")\n",
    "    \n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb classes in samples 39\n",
      "nb oservations: (3784, 2)\n",
      "nb classes in final data: 28\n",
      "test_data_small.shape (3768, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37427                                                              male hurt bending work dx knee pain b\n",
       "3526                                    works construction door fell hitting head loc c neck pain chi ms\n",
       "8292       c l finger pain work l th digit removing door panel crushed dx finger contu subungal hematoma\n",
       "63604                                                 wks lows heavy lifting h worsening lbp atypical cp\n",
       "11228    f pt work yesterday slipped fell onto floor hitting head loc altered mental status today dx chi\n",
       "                                                      ...                                               \n",
       "36432                               drives subject bus lots lifting pushing people wheelchairs back pain\n",
       "16967                                          work handling concrete got rash hands contact dermat itis\n",
       "49205                                                                                sexual assault work\n",
       "2439                               work hit open freezer door c r shoulder pain dx acute r shoulder pain\n",
       "9297                                              dx knee abrasion p pt assaulted work kicked x days ago\n",
       "Name: text, Length: 3768, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = get_test_data('./data/raw/test_data.csv',is_sample=True, ratio=0.05)\n",
    "X_test, y_test = test_data['text'], test_data['event']\n",
    "X_test_processed = preprocess_data(X_test)\n",
    "X_test_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 12, 13, 23, 24, 26, 27, 31, 32, 41, 42, 43, 44, 51, 52, 53, 55, 60, 62, 63, 64, 66, 70, 71, 72, 73, 78, 99]\n",
      "[11, 12, 13, 23, 24, 26, 27, 31, 32, 41, 42, 43, 44, 51, 52, 53, 55, 60, 62, 63, 64, 66, 70, 71, 72, 73, 78, 99]\n"
     ]
    }
   ],
   "source": [
    "train_classes =y_train.unique().tolist()\n",
    "test_classes = y_test.unique().tolist()\n",
    "\n",
    "print(sorted(train_classes))\n",
    "print(sorted(test_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_processed = pd.DataFrame({'text':X_test_processed,'event':y_test})\n",
    "text_processed.to_csv('./data/test/test_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and Encode Train and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.11 s, sys: 512 ms, total: 1.63 s\n",
      "Wall time: 1.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import  AutoTokenizer, DistilBertTokenizerFast,BertTokenizerFast,RobertaTokenizerFast\n",
    "MAX_LEN = 45\n",
    "\n",
    "\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "x_train = X_train_processed.to_list()\n",
    "x_valid = X_valid_processed.to_list()\n",
    "\n",
    "\n",
    "train_encodings = tokenizer(x_train, max_length=MAX_LEN, truncation=True, padding='max_length',return_tensors='tf')\n",
    "valid_encodings = tokenizer(x_valid, max_length=MAX_LEN, truncation=True, padding='max_length',return_tensors='tf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[  101 10930 12458 12162 14499  3329  3384  3062  3329  2147   102     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0], shape=(45,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0], shape=(45,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(train_encodings['input_ids'][0])\n",
    "print(train_encodings['attention_mask'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Labels .....\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"Encoding Labels .....\")\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "y_train_encode = np.asarray(encoder.transform(y_train))\n",
    "y_valid_encode = np.asarray(encoder.transform(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = { id:str(label) for id, label in enumerate(encoder.classes_)}\n",
    "label2id = { str(label):id for id, label in enumerate(encoder.classes_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes in train : 28\n",
      "classes in valid : 28\n"
     ]
    }
   ],
   "source": [
    "print('classes in train :',len(np.unique(y_train_encode)))\n",
    "print('classes in valid :',len(np.unique(y_valid_encode)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_path='./data/train/'\n",
    "test_data_path = './data/test'\n",
    "os.makedirs(data_path,exist_ok=True)\n",
    "\n",
    "from pickle import dump\n",
    "#for training process\n",
    "dump(encoder,open(os.path.join(data_path,'encode.pkl'),'wb'))\n",
    "\n",
    "#for evaluation process\n",
    "dump(encoder,open(os.path.join(test_data_path,'encode.pkl'),'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 12, 13, 23, 24, 26, 27, 31, 32, 41, 42, 43, 44, 51, 52, 53, 55, 60, 62, 63, 64, 66, 70, 71, 72, 73, 78, 99]\n"
     ]
    }
   ],
   "source": [
    "print(encoder.classes_.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TF Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.15 ms, sys: 20 µs, total: 4.17 ms\n",
      "Wall time: 3.63 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def construct_tfdataset(encodings, y=None):\n",
    "    if y is not None:\n",
    "        return tf.data.Dataset.from_tensor_slices((dict(encodings),y))\n",
    "    else:\n",
    "        # this case is used when making predictions on unseen samples after training\n",
    "        return tf.data.Dataset.from_tensor_slices(dict(encodings))\n",
    "    \n",
    "train_tfdataset = construct_tfdataset(train_encodings, y_train_encode)\n",
    "valid_tfdataset = construct_tfdataset(valid_encodings, y_valid_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TF Records File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _save_feature_as_tfrecord(tfdataset,file_path):\n",
    "    \"\"\" Helper function to save the tf dataset as tfrecords\"\"\"\n",
    "    \n",
    "\n",
    "    def single_example_data(encoding,label_id):\n",
    "\n",
    "        input_ids =encoding['input_ids']\n",
    "        attention_mask = encoding['attention_mask']\n",
    "\n",
    "        tfrecord_features = collections.OrderedDict()\n",
    "\n",
    "        def _int64_feature(value):\n",
    "            \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "            return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "        tfrecord_features['input_ids'] = _int64_feature(input_ids)\n",
    "        tfrecord_features['attention_mask'] = _int64_feature(attention_mask)\n",
    "        tfrecord_features['label_ids'] =  _int64_feature([label_id])\n",
    "\n",
    "        _example = tf.train.Example(features=tf.train.Features(feature=tfrecord_features))\n",
    "\n",
    "        return _example.SerializeToString()\n",
    "\n",
    "    def data_generator():\n",
    "        for features in tfdataset:\n",
    "            yield single_example_data(*features)\n",
    "\n",
    "    serialized_tfdataset = tf.data.Dataset.from_generator(\n",
    "        data_generator, output_types=tf.string, output_shapes=())        \n",
    "\n",
    "    writer = tf.data.experimental.TFRecordWriter(file_path)\n",
    "    writer.write(serialized_tfdataset)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-29-a2e748b32298>:31: TFRecordWriter.__init__ (from tensorflow.python.data.experimental.ops.writers) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To write TFRecords to disk, use `tf.io.TFRecordWriter`. To save and load the contents of a dataset, use `tf.data.experimental.save` and `tf.data.experimental.load`\n",
      "CPU times: user 1min 26s, sys: 224 ms, total: 1min 26s\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training data\n",
    "_save_feature_as_tfrecord(train_tfdataset,'./data/train/train.tfrecord')\n",
    "\n",
    "#validation data\n",
    "_save_feature_as_tfrecord(valid_tfdataset,'./data/valid/valid.tfrecord')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-979294212144/injury-data/training'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = 'injury-data/training'\n",
    "tfrecord_train_location = sagemaker_session.upload_data(path = './data/train',\n",
    "                                                      bucket = bucket,\n",
    "                                                      key_prefix = prefix)\n",
    "tfrecord_train_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-979294212144/injury-data/validation'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = 'injury-data/validation'\n",
    "tfrecord_valid_location = sagemaker_session.upload_data(path = './data/valid',\n",
    "                                                      bucket = bucket,\n",
    "                                                      key_prefix = prefix)\n",
    "tfrecord_valid_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Run BERT Training Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training output path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-979294212144/output/'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = 'output'\n",
    "output_path = 's3://{}/{}/'.format(bucket, prefix )\n",
    "output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\n",
      "    \n",
      "\u001b[37m# implement pip as a subprocess:\u001b[39;49;00m\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtransformers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mnltk\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtensorflow\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m  TFDistilBertForSequenceClassification,DistilBertConfig\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TFBertForSequenceClassification, BertConfig\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TFRobertaForSequenceClassification, RobertaConfig\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptimizers\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mschedules\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m PolynomialDecay\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptimization_tf\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m WarmUp, AdamWeightDecay\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m load\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_parse_args\u001b[39;49;00m():\n",
      "    parser = argparse.ArgumentParser()\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output-data-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--sm-model-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,default=os.environ.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,default=os.environ.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--validation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,default=os.environ.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_VALIDATION\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    \n",
      "    \u001b[37m## Hyperparams\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,default=\u001b[33m'\u001b[39;49;00m\u001b[33mBaseBERT\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--num_records\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--optimizer\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,default=\u001b[33m'\u001b[39;49;00m\u001b[33madam\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--max_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,default=\u001b[34m45\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,default=\u001b[34m5e-5\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,default=\u001b[34m1\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,default=\u001b[34m32\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--valid_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,default=\u001b[34m8\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--steps_per_epoch\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--validation_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m  parser.parse_known_args()\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mDistilBERT\u001b[39;49;00m:\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, params):\n",
      "        \u001b[36mself\u001b[39;49;00m.num_labels = params[\u001b[33m'\u001b[39;49;00m\u001b[33mnum_labels\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.max_len = params[\u001b[33m'\u001b[39;49;00m\u001b[33mmax_len\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.learning_rate = params[\u001b[33m'\u001b[39;49;00m\u001b[33mlearning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.id2label = params[\u001b[33m'\u001b[39;49;00m\u001b[33mid2label\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.label2id = params[\u001b[33m'\u001b[39;49;00m\u001b[33mlabel2id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.batch_size = params[\u001b[33m'\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.epochs = params[\u001b[33m'\u001b[39;49;00m\u001b[33mepochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.num_records = params[\u001b[33m'\u001b[39;49;00m\u001b[33mnum_records\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mbuild\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\n",
      "        \n",
      "        freeze_bert_layer = \u001b[34mFalse\u001b[39;49;00m\n",
      "        \n",
      "        \n",
      "        transformer_config = DistilBertConfig(num_labels=\u001b[36mself\u001b[39;49;00m.num_labels,\n",
      "                                   id2label = \u001b[36mself\u001b[39;49;00m.id2label,\n",
      "                                   label2id  = \u001b[36mself\u001b[39;49;00m.label2id,\n",
      "                                   output_attentions = \u001b[34mFalse\u001b[39;49;00m,\n",
      "                                   output_hidden_states= \u001b[34mFalse\u001b[39;49;00m)\n",
      "                                   \n",
      "        transformer_model = TFDistilBertForSequenceClassification.from_pretrained(\u001b[33m\"\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,config=transformer_config)\n",
      "        input_ids = tf.keras.Input(shape = (\u001b[36mself\u001b[39;49;00m.max_len,),name=\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,dtype=\u001b[33m'\u001b[39;49;00m\u001b[33mint32\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        attention_mask = tf.keras.Input(shape = (\u001b[36mself\u001b[39;49;00m.max_len,),name=\u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,dtype=\u001b[33m'\u001b[39;49;00m\u001b[33mint32\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \n",
      "        embbeding_layer = transformer_model.distilbert(input_ids,attention_mask=attention_mask)[\u001b[34m0\u001b[39;49;00m]\n",
      "        X = tf.keras.layers.Conv1D(\u001b[34m512\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(embbeding_layer)\n",
      "        X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
      "        X = tf.keras.layers.Dense(\u001b[34m256\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(X)\n",
      "        X = tf.keras.layers.Dropout(\u001b[34m0.2\u001b[39;49;00m)(X)\n",
      "        X = tf.keras.layers.Dense(\u001b[36mself\u001b[39;49;00m.num_labels, activation=\u001b[33m'\u001b[39;49;00m\u001b[33msoftmax\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(X)\n",
      "        \n",
      "        model = tf.keras.Model(inputs=[input_ids,attention_mask], outputs = X)\n",
      "        \n",
      "        \u001b[34mfor\u001b[39;49;00m layer \u001b[35min\u001b[39;49;00m model.layers[:\u001b[34m3\u001b[39;49;00m]:\n",
      "            layer.trainable = \u001b[35mnot\u001b[39;49;00m freeze_bert_layer\n",
      "            \n",
      "        num_train_steps = (\u001b[36mself\u001b[39;49;00m.num_records // \u001b[36mself\u001b[39;49;00m.batch_size) * \u001b[36mself\u001b[39;49;00m.epochs\n",
      "        decay_schedule_fn = PolynomialDecay(\n",
      "            initial_learning_rate=\u001b[36mself\u001b[39;49;00m.learning_rate,\n",
      "            end_learning_rate=\u001b[34m0.\u001b[39;49;00m,\n",
      "            decay_steps=num_train_steps\n",
      "            )\n",
      "        \n",
      "        warmup_steps = num_train_steps * \u001b[34m0.1\u001b[39;49;00m\n",
      "        warmup_schedule  = WarmUp(\u001b[36mself\u001b[39;49;00m.learning_rate,decay_schedule_fn,warmup_steps)\n",
      "        \n",
      "        \u001b[37m# fine optimizer and loss\u001b[39;49;00m\n",
      "        \u001b[37m#optimizer = tf.keras.optimizers.Adam(learning_rate=lr_scheduler)\u001b[39;49;00m\n",
      "        optimizer = AdamWeightDecay (learning_rate=warmup_schedule, weight_decay_rate=\u001b[34m0.01\u001b[39;49;00m, epsilon=\u001b[34m1e-6\u001b[39;49;00m)\n",
      "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits= \u001b[34mFalse\u001b[39;49;00m)\n",
      "        metrics = [\u001b[33m'\u001b[39;49;00m\u001b[33macc\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
      "\n",
      "        \u001b[34mreturn\u001b[39;49;00m model\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mBaseBERT\u001b[39;49;00m:\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, params):\n",
      "        \u001b[36mself\u001b[39;49;00m.num_labels = params[\u001b[33m'\u001b[39;49;00m\u001b[33mnum_labels\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.max_len = params[\u001b[33m'\u001b[39;49;00m\u001b[33mmax_len\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.learning_rate = params[\u001b[33m'\u001b[39;49;00m\u001b[33mlearning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.id2label = params[\u001b[33m'\u001b[39;49;00m\u001b[33mid2label\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.label2id = params[\u001b[33m'\u001b[39;49;00m\u001b[33mlabel2id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.batch_size = params[\u001b[33m'\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.epochs = params[\u001b[33m'\u001b[39;49;00m\u001b[33mepochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.num_records = params[\u001b[33m'\u001b[39;49;00m\u001b[33mnum_records\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mbuild\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):      \n",
      "        \n",
      "        freeze_bert_layer = \u001b[34mFalse\u001b[39;49;00m\n",
      "        \n",
      "        transformer_model = TFBertForSequenceClassification.from_pretrained(\u001b[33m\"\u001b[39;49;00m\u001b[33mbert-base-uncased\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,num_labels=\u001b[36mself\u001b[39;49;00m.num_labels, \n",
      "                 output_attentions = \u001b[34mFalse\u001b[39;49;00m,\n",
      "                output_hidden_states= \u001b[34mFalse\u001b[39;49;00m )\n",
      "        input_ids = tf.keras.Input(shape = (\u001b[36mself\u001b[39;49;00m.max_len,),name=\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,dtype=\u001b[33m'\u001b[39;49;00m\u001b[33mint32\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        attention_mask = tf.keras.Input(shape = (\u001b[36mself\u001b[39;49;00m.max_len,),name=\u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,dtype=\u001b[33m'\u001b[39;49;00m\u001b[33mint32\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \n",
      "        embbeding_layer = transformer_model.bert(input_ids,attention_mask=attention_mask)[\u001b[34m0\u001b[39;49;00m]\n",
      "        X = tf.keras.layers.Conv1D(\u001b[34m512\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(embbeding_layer)\n",
      "        X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
      "        X = tf.keras.layers.Dense(\u001b[34m256\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(X)\n",
      "        X = tf.keras.layers.Dropout(\u001b[34m0.2\u001b[39;49;00m)(X)\n",
      "        X = tf.keras.layers.Dense(\u001b[36mself\u001b[39;49;00m.num_labels, activation=\u001b[33m'\u001b[39;49;00m\u001b[33msoftmax\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(X)\n",
      "\n",
      "        model = tf.keras.Model(inputs=[input_ids,attention_mask], outputs = X)\n",
      "        \n",
      "        \u001b[34mfor\u001b[39;49;00m layer \u001b[35min\u001b[39;49;00m model.layers[:\u001b[34m3\u001b[39;49;00m]:\n",
      "            layer.trainable = \u001b[35mnot\u001b[39;49;00m freeze_bert_layer\n",
      "        \n",
      "        num_train_steps = (\u001b[36mself\u001b[39;49;00m.num_records // \u001b[36mself\u001b[39;49;00m.batch_size) * \u001b[36mself\u001b[39;49;00m.epochs\n",
      "        decay_schedule_fn = PolynomialDecay(\n",
      "            initial_learning_rate=\u001b[36mself\u001b[39;49;00m.learning_rate,\n",
      "            end_learning_rate=\u001b[34m0.\u001b[39;49;00m,\n",
      "            decay_steps=num_train_steps\n",
      "            )\n",
      "        \n",
      "        warmup_steps = num_train_steps * \u001b[34m0.1\u001b[39;49;00m\n",
      "        warmup_schedule  = WarmUp(\u001b[36mself\u001b[39;49;00m.learning_rate,decay_schedule_fn,warmup_steps)\n",
      "        \n",
      "        \u001b[37m#optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\u001b[39;49;00m\n",
      "        optimizer = AdamWeightDecay (learning_rate=warmup_schedule)\n",
      "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    \n",
      "        model.compile(optimizer=optimizer, loss=loss, metrics=[\u001b[33m'\u001b[39;49;00m\u001b[33macc\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "        \n",
      "        \u001b[34mreturn\u001b[39;49;00m model\n",
      "    \n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mBaseRoberta\u001b[39;49;00m:\n",
      "    \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, params):\n",
      "        \u001b[36mself\u001b[39;49;00m.num_labels = params[\u001b[33m'\u001b[39;49;00m\u001b[33mnum_labels\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.max_len = params[\u001b[33m'\u001b[39;49;00m\u001b[33mmax_len\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.learning_rate = params[\u001b[33m'\u001b[39;49;00m\u001b[33mlearning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.id2label = params[\u001b[33m'\u001b[39;49;00m\u001b[33mid2label\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.label2id = params[\u001b[33m'\u001b[39;49;00m\u001b[33mlabel2id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.batch_size = params[\u001b[33m'\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.epochs = params[\u001b[33m'\u001b[39;49;00m\u001b[33mepochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.num_records = params[\u001b[33m'\u001b[39;49;00m\u001b[33mnum_records\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mbuild\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):      \n",
      "        \n",
      "        freeze_bert_layer = \u001b[34mFalse\u001b[39;49;00m\n",
      "        \n",
      "        transformer_model = TFRobertaForSequenceClassification.from_pretrained(\u001b[33m\"\u001b[39;49;00m\u001b[33mroberta-base\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, num_labels=\u001b[36mself\u001b[39;49;00m.num_labels, \n",
      "                 output_attentions = \u001b[34mFalse\u001b[39;49;00m,\n",
      "                output_hidden_states= \u001b[34mFalse\u001b[39;49;00m )\n",
      "        input_ids = tf.keras.Input(shape = (\u001b[36mself\u001b[39;49;00m.max_len,),name=\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,dtype=\u001b[33m'\u001b[39;49;00m\u001b[33mint32\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        attention_mask = tf.keras.Input(shape = (\u001b[36mself\u001b[39;49;00m.max_len,),name=\u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,dtype=\u001b[33m'\u001b[39;49;00m\u001b[33mint32\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \n",
      "        embbeding_layer = transformer_model.roberta(input_ids,attention_mask=attention_mask)[\u001b[34m0\u001b[39;49;00m]\n",
      "        X = tf.keras.layers.GlobalMaxPool1D()(embbeding_layer)\n",
      "        X = tf.keras.layers.Dense(\u001b[36mself\u001b[39;49;00m.num_labels, activation=\u001b[33m'\u001b[39;49;00m\u001b[33msoftmax\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(X)\n",
      "        \n",
      "        model = tf.keras.Model(inputs=[input_ids,attention_mask], outputs = X)\n",
      "        \n",
      "        \u001b[34mfor\u001b[39;49;00m layer \u001b[35min\u001b[39;49;00m model.layers[:\u001b[34m3\u001b[39;49;00m]:\n",
      "            layer.trainable = \u001b[35mnot\u001b[39;49;00m freeze_bert_layer\n",
      "        \n",
      "        num_train_steps = (\u001b[36mself\u001b[39;49;00m.num_records // \u001b[36mself\u001b[39;49;00m.batch_size) * \u001b[36mself\u001b[39;49;00m.epochs\n",
      "        decay_schedule_fn = PolynomialDecay(\n",
      "            initial_learning_rate=\u001b[36mself\u001b[39;49;00m.learning_rate,\n",
      "            end_learning_rate=\u001b[34m0.\u001b[39;49;00m,\n",
      "            decay_steps=num_train_steps\n",
      "            )\n",
      "        \n",
      "        warmup_steps = num_train_steps * \u001b[34m0.1\u001b[39;49;00m\n",
      "        warmup_schedule  = WarmUp(\u001b[36mself\u001b[39;49;00m.learning_rate,decay_schedule_fn,warmup_steps)\n",
      "        \n",
      "        \u001b[37m#optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\u001b[39;49;00m\n",
      "        optimizer = AdamWeightDecay (learning_rate=warmup_schedule, weight_decay_rate=\u001b[34m0.01\u001b[39;49;00m, epsilon=\u001b[34m1e-6\u001b[39;49;00m)\n",
      "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=\u001b[34mFalse\u001b[39;49;00m)\n",
      "        \n",
      "        model.compile(optimizer=optimizer, loss=loss, metrics=[\u001b[33m'\u001b[39;49;00m\u001b[33macc\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "        \n",
      "        \u001b[34mreturn\u001b[39;49;00m model\n",
      "        \n",
      "        \n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_load_data\u001b[39;49;00m(train_dir,valid_dir,MAX_LEN,epochs,batch_size,valid_batch_size,steps_per_epoch,validation_steps):\n",
      "          \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain_dir : \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,train_dir)    \n",
      "    train_file = os.path.join(train_dir,\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain.tfrecord\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain_file : \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,train_file)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mvalid_dir:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,valid_dir)\n",
      "    valid_file = os.path.join(valid_dir,\u001b[33m\"\u001b[39;49;00m\u001b[33mvalid.tfrecord\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mvalid_file:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,valid_file)\n",
      "    \n",
      "    \u001b[37m# Create a description of the features.\u001b[39;49;00m\n",
      "    feature_description = {\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: tf.io.FixedLenFeature([MAX_LEN], tf.int64),\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: tf.io.FixedLenFeature([MAX_LEN], tf.int64),\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: tf.io.FixedLenFeature([], tf.int64),\n",
      "    }\n",
      "        \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m_parse_function\u001b[39;49;00m(example_proto):\n",
      "\n",
      "        \u001b[37m# Parse the input `tf.train.Example` proto using the dictionary above.\u001b[39;49;00m\n",
      "        parsed  = tf.io.parse_single_example(example_proto, feature_description)\n",
      "\n",
      "        \u001b[34mreturn\u001b[39;49;00m {\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:parsed[\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:parsed[\u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]},parsed[\u001b[33m'\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \n",
      "    \n",
      "    train_dataset = tf.data.TFRecordDataset(train_file)\n",
      "    train_dataset = train_dataset.repeat(epochs * steps_per_epoch)\n",
      "    train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
      "    train_dataset = train_dataset.map(_parse_function,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
      "    train_dataset = train_dataset.batch(batch_size)\n",
      "    \n",
      "    train_dataset.cache()\n",
      "    \n",
      "    valid_dataset = tf.data.TFRecordDataset(valid_file)\n",
      "    valid_dataset = valid_dataset.repeat(epochs * validation_steps)\n",
      "    valid_dataset = valid_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
      "    valid_dataset = valid_dataset.map(_parse_function,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
      "    valid_dataset = valid_dataset.batch(valid_batch_size)\n",
      "    \n",
      "    valid_dataset.cache()\n",
      "   \n",
      "    \u001b[34mreturn\u001b[39;49;00m train_dataset, valid_dataset\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_load_encoder\u001b[39;49;00m(train_dir):\n",
      "    \u001b[37m# load the encoder\u001b[39;49;00m\n",
      "    encoder_file = os.path.join(train_dir,\u001b[33m'\u001b[39;49;00m\u001b[33mencode.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \n",
      "    encoder = load(\u001b[36mopen\u001b[39;49;00m(encoder_file, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m encoder\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m():\n",
      "    args, unknown = _parse_args()\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33minput train: \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,args.train)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33minput valid: \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,args.validation)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mloading data...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    train_dataset, valid_dataset = _load_data(args.train,\n",
      "                                              args.validation,\n",
      "                                              args.max_len,\n",
      "                                              args.epochs,\n",
      "                                              args.batch_size,\n",
      "                                              args.valid_batch_size,\n",
      "                                              args.steps_per_epoch,\n",
      "                                              args.validation_steps)    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mloading encoder...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    encoder = _load_encoder(args.train)\n",
      "    \n",
      "    CLASSES = encoder.classes_\n",
      "    id2label = { \u001b[36mid\u001b[39;49;00m:\u001b[36mstr\u001b[39;49;00m(label) \u001b[34mfor\u001b[39;49;00m \u001b[36mid\u001b[39;49;00m, label \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(encoder.classes_)}\n",
      "    label2id = { \u001b[36mstr\u001b[39;49;00m(label):\u001b[36mid\u001b[39;49;00m \u001b[34mfor\u001b[39;49;00m \u001b[36mid\u001b[39;49;00m, label \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(encoder.classes_)}\n",
      "    \n",
      "    params = {\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mnum_labels\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[36mlen\u001b[39;49;00m(CLASSES),\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mmax_len\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.max_len,\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mlearning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.learning_rate,\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mid2label\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: id2label,\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mlabel2id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: label2id,\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.batch_size,\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mepochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.epochs,\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mnum_records\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:args.num_records\n",
      "        }\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mBuilding model...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mif\u001b[39;49;00m args.model_name == \u001b[33m'\u001b[39;49;00m\u001b[33mDistilBERT\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m :\n",
      "        model = DistilBERT(params).build()   \n",
      "    \u001b[34melif\u001b[39;49;00m args.model_name == \u001b[33m'\u001b[39;49;00m\u001b[33mBaseBERT\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "        model = BaseBERT(params).build()\n",
      "    \u001b[34melif\u001b[39;49;00m args.model_name == \u001b[33m'\u001b[39;49;00m\u001b[33mBaseRoberta\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "        model = BaseRoberta(params).build()\n",
      "        \n",
      "    \u001b[36mprint\u001b[39;49;00m(model.summary())\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation steps:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,args.validation_steps)\n",
      "        \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mTraining....\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    history = model.fit(x=train_dataset.shuffle(args.num_records),\n",
      "                        steps_per_epoch = args.steps_per_epoch,\n",
      "                        batch_size=args.batch_size,\n",
      "                        epochs=args.epochs,\n",
      "                        validation_data=valid_dataset,\n",
      "                        validation_steps=args.validation_steps)\n",
      "        \n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33msaving model...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[37m#model.save(os.path.join(args.sm_model_dir,f\"{args.model_name}.h5\"))\u001b[39;49;00m\n",
      "    model.save(os.path.join(args.sm_model_dir,\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.model_name\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    \u001b[37m#model.save(f\"./output/model/{args.model_name}/{args.model_name}_test.h5\")\u001b[39;49;00m\n",
      "\n",
      "    \n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    main()\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow 2.3 script\n",
    "!pygmentize './src/train.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records = len(X_train)\n",
    "num_valid_records = len(X_valid)\n",
    "max_len = MAX_LEN\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "valid_batch_size = 16\n",
    "steps_per_epoch = num_records // batch_size\n",
    "validation_steps = num_valid_records // valid_batch_size\n",
    "learning_rate = 5e-5\n",
    "optimizer = 'adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6901\n",
      "431\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "print(num_records)\n",
    "print(steps_per_epoch)\n",
    "print(validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 8.11 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#%run ./src/train.py --train ./data/train --validation ./data/valid --epochs 1 --num_records 6901 --steps_per_epoch 431 --validation_steps 47"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Hugging Face Estimator to start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from time import gmtime, strftime\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "model_name = 'BaseBERT'\n",
    "job_name_prefix = f\"training-{model_name}\"\n",
    "timestamp = strftime(\"-%m-%d-%M-%S\", gmtime())\n",
    "\n",
    "job_name = job_name_prefix + timestamp\n",
    "\n",
    "_estimator = HuggingFace(\n",
    "        base_job_name  = job_name,\n",
    "        entry_point=\"train.py\",\n",
    "        source_dir = \"./src/\",\n",
    "        role=role,\n",
    "        instance_count=1,\n",
    "        volume_size = 20,\n",
    "        max_run = 18000,\n",
    "        instance_type='ml.p3.2xlarge',\n",
    "        transformers_version = \"4.4\",\n",
    "        tensorflow_version  = \"2.4\",\n",
    "        py_version=\"py37\",\n",
    "        output_path = output_path,\n",
    "        hyperparameters = {\n",
    "                \"model_name\": model_name,\n",
    "                \"num_records\":  num_records,\n",
    "                \"max_len\":max_len,\n",
    "                \"epochs\":int(epochs),\n",
    "                \"learning_rate\":float(learning_rate),\n",
    "                \"batch_size\":int(batch_size),\n",
    "                \"valid_batch_size\":valid_batch_size,\n",
    "                \"steps_per_epoch\": steps_per_epoch,\n",
    "                \"validation_steps\": validation_steps,\n",
    "                \"optimizer\":optimizer\n",
    "                },\n",
    "        metric_definitions = [{'Name':'train:loss','Regex':'loss: ([0-9\\\\.]+)'},\n",
    "                                    {'Name':'train:accuracy','Regex':'acc: ([0-9\\\\.]+)'},\n",
    "                                    {'Name':'validation:loss','Regex':'val_loss: ([0-9\\\\.]+)'},\n",
    "                                    {'Name':'validation:accuracy','Regex':'val_acc: ([0-9\\\\.]+)'}],\n",
    "        enable_sagemaker_metrics = True\n",
    "    )\n",
    "\n",
    "\n",
    "train_data = sagemaker.inputs.TrainingInput(\n",
    "    tfrecord_train_location, # Replace None\n",
    "    distribution='FullyReplicated'\n",
    ")\n",
    "\n",
    "validation_data = sagemaker.inputs.TrainingInput(\n",
    "    tfrecord_valid_location, # Replace None\n",
    "    distribution='FullyReplicated'\n",
    ")\n",
    "\n",
    "_estimator.fit({'train':train_data,'validation':validation_data}, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the trained distilbert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training-BaseBERT-08-25-44-56-2021-08-25-15-44-56-919\n"
     ]
    }
   ],
   "source": [
    "print(_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3  import S3Downloader\n",
    "import os\n",
    "\n",
    "def download_model(sagemaker_session,job_name):\n",
    "    \n",
    "    if job_name is not None :\n",
    "        modeldesc = sagemaker_session.describe_training_job(job_name)\n",
    "        s3_model_path = modeldesc['ModelArtifacts']['S3ModelArtifacts']\n",
    "    \n",
    "    os.makedirs(f\"./output/model/{job_name}/\",exist_ok=True)\n",
    "\n",
    "    S3Downloader.download(\n",
    "        s3_uri=s3_model_path, # s3 uri where the trained model is located\n",
    "        local_path=f\"./output/model/{job_name}/\", # local path where *.targ.gz is saved\n",
    "        sagemaker_session=sagemaker_session # sagemaker session used for training the model\n",
    "    )\n",
    "    \n",
    "    return s3_model_path, modeldesc['HyperParameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-979294212144/output/training-BaseBERT-08-25-44-56-2021-08-25-15-44-56-919/output/model.tar.gz\n",
      "{'batch_size': '16', 'epochs': '5', 'learning_rate': '5e-05', 'max_len': '45', 'model_name': '\"BaseBERT\"', 'num_records': '6901', 'optimizer': '\"adam\"', 'sagemaker_container_log_level': '20', 'sagemaker_job_name': '\"training-BaseBERT-08-25-44-56-2021-08-25-15-44-56-919\"', 'sagemaker_program': '\"train.py\"', 'sagemaker_region': '\"us-east-1\"', 'sagemaker_submit_directory': '\"s3://sagemaker-us-east-1-979294212144/training-BaseBERT-08-25-44-56-2021-08-25-15-44-56-919/source/sourcedir.tar.gz\"', 'steps_per_epoch': '431', 'valid_batch_size': '16', 'validation_steps': '47'}\n"
     ]
    }
   ],
   "source": [
    "#downloading model\n",
    "job_name = 'training-BaseBERT-08-25-44-56-2021-08-25-15-44-56-919'\n",
    "s3_model_path, hp = download_model(sagemaker_session,job_name)\n",
    "\n",
    "print(s3_model_path)\n",
    "print(hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract and load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "from transformers.optimization_tf import AdamWeightDecay\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "\n",
    "def extract_data_load_model(job_name,model_name):\n",
    "\n",
    "    t = tarfile.open(f'./output/model/{job_name}/model.tar.gz', 'r:gz')\n",
    "    t.extractall(path=f'./output/model/{job_name}')\n",
    "    _model = tf.keras.models.load_model(f\"./output/model/{job_name}/{model_name}\",custom_objects={'AdamWeightDecay':AdamWeightDecay})\n",
    "    \n",
    "    return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'BaseBERT'\n",
    "job_name = 'training-BaseBERT-08-25-44-56-2021-08-25-15-44-56-919'\n",
    "loaded_model = extract_data_load_model(job_name,model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_data(train_dir,valid_dir,MAX_LEN,epochs,batch_size,valid_batch_size,steps_per_epoch,validation_steps):\n",
    "    \"\"\" Helper function to load,parse and create input data pipeline from TFRecords\"\"\"\n",
    "          \n",
    "    train_file = os.path.join(train_dir,\"train.tfrecord\") \n",
    "    valid_file = os.path.join(valid_dir,\"valid.tfrecord\")\n",
    "    \n",
    "    # Create a description of the features.\n",
    "    feature_description = {\n",
    "        'input_ids': tf.io.FixedLenFeature([MAX_LEN], tf.int64),\n",
    "        'attention_mask': tf.io.FixedLenFeature([MAX_LEN], tf.int64),\n",
    "        'label_ids': tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "        \n",
    "    def _parse_function(example_proto):\n",
    "\n",
    "        # Parse the input `tf.train.Example` proto using the dictionary above.\n",
    "        parsed  = tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "        return {'input_ids':parsed['input_ids'],'attention_mask':parsed['attention_mask']},parsed['label_ids']\n",
    "        \n",
    "    \n",
    "    train_dataset = tf.data.TFRecordDataset(train_file)\n",
    "    train_dataset = train_dataset.repeat(epochs * steps_per_epoch)\n",
    "    train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    train_dataset = train_dataset.map(_parse_function,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    train_dataset = train_dataset.batch(batch_size)\n",
    "    \n",
    "    \n",
    "    \n",
    "    valid_dataset = tf.data.TFRecordDataset(valid_file)\n",
    "    valid_dataset = valid_dataset.repeat(epochs * validation_steps)\n",
    "    valid_dataset = valid_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    valid_dataset = valid_dataset.map(_parse_function,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    valid_dataset = valid_dataset.batch(valid_batch_size)\n",
    "    \n",
    "   \n",
    "    return train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = './data/train'\n",
    "valid_dir= './data/valid'\n",
    "train_dataset,valid_dataset = _load_data(train_dir,valid_dir,MAX_LEN,epochs,batch_size,valid_batch_size,steps_per_epoch,validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Training data...\n",
      "431/431 [==============================] - 245s 565ms/step - loss: 0.0793 - acc: 0.9796\n",
      "Training loss:  0.07934041321277618\n",
      "Training accuracy:  0.9795533418655396\n",
      "Evaluating Validation data...\n",
      "47/47 [==============================] - 27s 568ms/step - loss: 0.8604 - acc: 0.7646\n",
      "Validation loss:  0.860435426235199\n",
      "Validation accuracy:  0.7646276354789734\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def _evaluate_model(loaded_model):\n",
    "    print(\"Evaluating Training data...\")\n",
    "    train_score = loaded_model.evaluate(train_dataset,\n",
    "                                     steps = steps_per_epoch,\n",
    "                                     batch_size=batch_size)\n",
    "\n",
    "    print(\"Training loss: \", train_score[0])\n",
    "    print(\"Training accuracy: \", train_score[1])\n",
    "\n",
    "    print(\"Evaluating Validation data...\")\n",
    "    valid_score = loaded_model.evaluate(valid_dataset, steps = validation_steps,batch_size=valid_batch_size)\n",
    "    print(\"Validation loss: \", valid_score[0])\n",
    "    print(\"Validation accuracy: \", valid_score[1])\n",
    "    \n",
    "_evaluate_model(loaded_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the fine-tuned model to predict new samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'bert-base-uncased'\n",
    "\n",
    "def _create_predictor(model, encoder,model_name, max_len,text):\n",
    "    tkzr = AutoTokenizer.from_pretrained(model_name)\n",
    "    x = [text]\n",
    "    encodings =  tkzr(x, max_length=max_len, truncation=True, padding='max_length',return_tensors='tf')\n",
    "    tfdataset = construct_tfdataset(encodings)\n",
    "    tfdataset = tfdataset.batch(1)\n",
    "    preds = model.predict(tfdataset)\n",
    "    categories = encoder.classes_.tolist()\n",
    "    enc = np.argmax(preds[0])\n",
    "    \n",
    "    return {     'text' : x,\n",
    "                 'predict_proba' : preds[0][np.argmax(preds[0])],\n",
    "                 'predicted_class' : categories[np.argmax(preds)]                             \n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'sp lt ankle going steps twisted ankle work'\n",
    "_create_predictor(loaded_model, encoder,MODEL_NAME, MAX_LEN,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Save Predictions and calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "767\n",
      "CPU times: user 1min 28s, sys: 1.93 s, total: 1min 30s\n",
      "Wall time: 25.5 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>true_event</th>\n",
       "      <th>preds_encode</th>\n",
       "      <th>preds_event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>work lifting heavy objects work strauined upper arm</td>\n",
       "      <td>71</td>\n",
       "      <td>23</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>slipped fell ice hitting back head pavement work dx chi</td>\n",
       "      <td>42</td>\n",
       "      <td>10</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lbp rad post leg p mech fall parking lot work sciatica</td>\n",
       "      <td>42</td>\n",
       "      <td>10</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>j slipped fell wet floor neck shoulder pain loc dx cervical strain</td>\n",
       "      <td>42</td>\n",
       "      <td>10</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>work c low back pain</td>\n",
       "      <td>70</td>\n",
       "      <td>22</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>breaking fight work pulling person another thumb pain sprain</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ago delivering papers walking back car jumped twisted foot fx</td>\n",
       "      <td>73</td>\n",
       "      <td>25</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>acc shot nail nail gun thigh fb removal</td>\n",
       "      <td>62</td>\n",
       "      <td>18</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>neck back pain rearended mva sb driver mailbox work</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>unspecified lifting work felt pull lower back lumbar strain</td>\n",
       "      <td>71</td>\n",
       "      <td>23</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 text  \\\n",
       "0                 work lifting heavy objects work strauined upper arm   \n",
       "1             slipped fell ice hitting back head pavement work dx chi   \n",
       "2              lbp rad post leg p mech fall parking lot work sciatica   \n",
       "3  j slipped fell wet floor neck shoulder pain loc dx cervical strain   \n",
       "4                                                work c low back pain   \n",
       "5        breaking fight work pulling person another thumb pain sprain   \n",
       "6       ago delivering papers walking back car jumped twisted foot fx   \n",
       "7                             acc shot nail nail gun thigh fb removal   \n",
       "8                 neck back pain rearended mva sb driver mailbox work   \n",
       "9         unspecified lifting work felt pull lower back lumbar strain   \n",
       "\n",
       "   true_event  preds_encode  preds_event  \n",
       "0          71            23           71  \n",
       "1          42            10           42  \n",
       "2          42            10           42  \n",
       "3          42            10           42  \n",
       "4          70            22           70  \n",
       "5          11             0           11  \n",
       "6          73            25           73  \n",
       "7          62            18           62  \n",
       "8          26             5           26  \n",
       "9          71            23           71  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "x = X_valid_processed.tolist()\n",
    "y = y_valid.tolist()\n",
    "\n",
    "def _batch_predict(model, encoder,model_name, max_len,x,y) :\n",
    "    tkzr = AutoTokenizer.from_pretrained(model_name)\n",
    "    encodings_x =  tkzr(x, max_length=MAX_LEN, truncation=True, padding='max_length',return_tensors='tf')\n",
    "    tfdataset = construct_tfdataset(encodings_x).batch(32)\n",
    "    preds = loaded_model.predict(tfdataset)\n",
    "    predictions_encode = pd.DataFrame(data=preds).apply(lambda x: np.argmax(x),axis=1)\n",
    "    categories = encoder.classes_.tolist()\n",
    "    predictions_event= predictions_encode.apply(lambda x:categories[x])\n",
    "\n",
    "\n",
    "    print(len(predictions_event))\n",
    "    results = pd.DataFrame({'text': x,\n",
    "                      'true_event':y,\n",
    "                      'preds_encode': predictions_encode,\n",
    "                      'preds_event':predictions_event\n",
    "                            })\n",
    "\n",
    "\n",
    "    return results\n",
    "\n",
    "results = _batch_predict(loaded_model,encoder,model_name,MAX_LEN,x,y)\n",
    "results.to_csv('solution.csv')\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7679269882659713,\n",
       " 'balanced_accuracy': 0.6554404070118658,\n",
       " 'f1': 0.6423576584345757,\n",
       " 'precision': 0.6657096900587514,\n",
       " 'recall': 0.6554404070118658}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score,balanced_accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.true_event\n",
    "    preds = pred.preds_event\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    bal_acc = balanced_accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels,preds,average='macro')\n",
    "    recall = recall_score(labels,preds,average='macro')\n",
    "    f1 = f1_score(labels,preds,average='macro')\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'balanced_accuracy':bal_acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "compute_metrics(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([37427,  3526,  8292, 63604, 11228, 52758,  4024,  3022, 68461,\n",
       "            15150,\n",
       "            ...\n",
       "            48480, 66479, 48145, 13776, 38542, 36432, 16967, 49205,  2439,\n",
       "             9297],\n",
       "           dtype='int64', length=3768)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 54s, sys: 8.02 s, total: 9min 2s\n",
      "Wall time: 2min 30s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.7098183e-04, 2.0497383e-04, 1.5861157e-04, ..., 9.5997047e-01,\n",
       "        6.7987824e-03, 1.0380523e-03],\n",
       "       [2.1427622e-06, 1.9612321e-06, 1.1960501e-05, ..., 1.4669994e-06,\n",
       "        5.5458358e-06, 8.3575196e-06],\n",
       "       [1.6551414e-04, 2.6177961e-04, 1.9675975e-04, ..., 1.6934944e-05,\n",
       "        1.6356539e-04, 6.3731556e-04],\n",
       "       ...,\n",
       "       [7.9736876e-01, 1.1166642e-01, 1.4137506e-03, ..., 3.2624155e-03,\n",
       "        2.2344638e-04, 5.7809362e-03],\n",
       "       [6.6859531e-05, 1.3940266e-04, 2.6153182e-04, ..., 1.2606778e-04,\n",
       "        2.8488834e-05, 1.4067366e-04],\n",
       "       [9.9847263e-01, 8.1823656e-04, 5.6473287e-05, ..., 5.2038631e-05,\n",
       "        2.2533372e-06, 5.7162630e-05]], dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "MAX_LEN = 45\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "x_test = X_test_processed.reset_index(drop=True).tolist()\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "tkzr = AutoTokenizer.from_pretrained(model_name)\n",
    "encodings_x =  tkzr(x_test, max_length=MAX_LEN, truncation=True, padding='max_length',return_tensors='tf')\n",
    "tfdataset = construct_tfdataset(encodings_x).batch(32)\n",
    "preds = loaded_model.predict(tfdataset)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3768, 28)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_encode.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3768,)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = encoder.classes_.tolist()\n",
    "predictions_event= predictions_encode.apply(lambda x:categories[x])\n",
    "predictions_event.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3768"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(predictions_event))\n",
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame.from_dict({\n",
    "                       'text': x_test,\n",
    "                      'true_event':y_test,\n",
    "                      'preds_encode': predictions_encode,\n",
    "                      'preds_event':predictions_event})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true_event</th>\n",
       "      <th>preds_encode</th>\n",
       "      <th>preds_event</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73</td>\n",
       "      <td>25</td>\n",
       "      <td>73</td>\n",
       "      <td>male hurt bending work dx knee pain b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62</td>\n",
       "      <td>18</td>\n",
       "      <td>62</td>\n",
       "      <td>works construction door fell hitting head loc c neck pain chi ms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64</td>\n",
       "      <td>18</td>\n",
       "      <td>62</td>\n",
       "      <td>c l finger pain work l th digit removing door panel crushed dx finger contu subungal hematoma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71</td>\n",
       "      <td>23</td>\n",
       "      <td>71</td>\n",
       "      <td>wks lows heavy lifting h worsening lbp atypical cp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42</td>\n",
       "      <td>10</td>\n",
       "      <td>42</td>\n",
       "      <td>f pt work yesterday slipped fell onto floor hitting head loc altered mental status today dx chi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3763</th>\n",
       "      <td>71</td>\n",
       "      <td>23</td>\n",
       "      <td>71</td>\n",
       "      <td>drives subject bus lots lifting pushing people wheelchairs back pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3764</th>\n",
       "      <td>55</td>\n",
       "      <td>16</td>\n",
       "      <td>55</td>\n",
       "      <td>work handling concrete got rash hands contact dermat itis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3765</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>sexual assault work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3766</th>\n",
       "      <td>63</td>\n",
       "      <td>19</td>\n",
       "      <td>63</td>\n",
       "      <td>work hit open freezer door c r shoulder pain dx acute r shoulder pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3767</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>dx knee abrasion p pt assaulted work kicked x days ago</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3768 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      true_event  preds_encode  preds_event  \\\n",
       "0             73            25           73   \n",
       "1             62            18           62   \n",
       "2             64            18           62   \n",
       "3             71            23           71   \n",
       "4             42            10           42   \n",
       "...          ...           ...          ...   \n",
       "3763          71            23           71   \n",
       "3764          55            16           55   \n",
       "3765          11             0           11   \n",
       "3766          63            19           63   \n",
       "3767          11             0           11   \n",
       "\n",
       "                                                                                                 text  \n",
       "0                                                               male hurt bending work dx knee pain b  \n",
       "1                                    works construction door fell hitting head loc c neck pain chi ms  \n",
       "2       c l finger pain work l th digit removing door panel crushed dx finger contu subungal hematoma  \n",
       "3                                                  wks lows heavy lifting h worsening lbp atypical cp  \n",
       "4     f pt work yesterday slipped fell onto floor hitting head loc altered mental status today dx chi  \n",
       "...                                                                                               ...  \n",
       "3763                             drives subject bus lots lifting pushing people wheelchairs back pain  \n",
       "3764                                        work handling concrete got rash hands contact dermat itis  \n",
       "3765                                                                              sexual assault work  \n",
       "3766                            work hit open freezer door c r shoulder pain dx acute r shoulder pain  \n",
       "3767                                           dx knee abrasion p pt assaulted work kicked x days ago  \n",
       "\n",
       "[3768 rows x 4 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3768\n",
      "CPU times: user 8min 53s, sys: 7.24 s, total: 9min\n",
      "Wall time: 2min 30s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>true_event</th>\n",
       "      <th>preds_encode</th>\n",
       "      <th>preds_event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male hurt bending work dx knee pain b</td>\n",
       "      <td>73</td>\n",
       "      <td>25</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>works construction door fell hitting head loc c neck pain chi ms</td>\n",
       "      <td>62</td>\n",
       "      <td>18</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c l finger pain work l th digit removing door panel crushed dx finger contu subungal hematoma</td>\n",
       "      <td>64</td>\n",
       "      <td>18</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wks lows heavy lifting h worsening lbp atypical cp</td>\n",
       "      <td>71</td>\n",
       "      <td>23</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f pt work yesterday slipped fell onto floor hitting head loc altered mental status today dx chi</td>\n",
       "      <td>42</td>\n",
       "      <td>10</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              text  \\\n",
       "0                                                            male hurt bending work dx knee pain b   \n",
       "1                                 works construction door fell hitting head loc c neck pain chi ms   \n",
       "2    c l finger pain work l th digit removing door panel crushed dx finger contu subungal hematoma   \n",
       "3                                               wks lows heavy lifting h worsening lbp atypical cp   \n",
       "4  f pt work yesterday slipped fell onto floor hitting head loc altered mental status today dx chi   \n",
       "\n",
       "   true_event  preds_encode  preds_event  \n",
       "0          73            25           73  \n",
       "1          62            18           62  \n",
       "2          64            18           62  \n",
       "3          71            23           71  \n",
       "4          42            10           42  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "MAX_LEN = 45\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "x_test = X_test_processed.reset_index(drop=True).tolist()\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "results_test = _batch_predict(loaded_model,encoder,model_name,MAX_LEN,x_test,y_test)\n",
    "results_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8118365180467091,\n",
       " 'balanced_accuracy': 0.7090264179661682,\n",
       " 'f1': 0.7124110587776313,\n",
       " 'precision': 0.7296497658918498,\n",
       " 'recall': 0.7090264179661682}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(results_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/keras/engine/functional.py:585: UserWarning: Input dict contained keys ['token_type_ids'] which did not match any model input. They will be ignored by the model.\n",
      "  [n for n in tensors.keys() if n not in ref_input_names])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 33s 677ms/step - loss: 0.7126 - acc: 0.8125\n",
      "Test loss:  0.7126179337501526\n",
      "Test accuracy:  0.8125\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "encodings_x_test =  tokenizer(x_test, max_length=MAX_LEN, truncation=True, padding='max_length',return_tensors='tf')\n",
    "y_test_encode = np.asarray(encoder.transform(y_test))\n",
    "tfdataset_test = construct_tfdataset(encodings_x_test,y_test_encode).batch(16)\n",
    "\n",
    "print(\"Evaluating Test data...\")\n",
    "test_score = loaded_model.evaluate(tfdataset_test, steps = validation_steps,batch_size=16)\n",
    "print(\"Test loss: \", test_score[0])\n",
    "print(\"Test accuracy: \", test_score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.3 Python 3.7 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.3-gpu-py37-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
