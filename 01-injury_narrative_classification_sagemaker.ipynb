{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Injury classification with SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://wandb.ai/jack-morris/david-vs-goliath/reports/Does-Model-Size-Matter-A-Comparison-of-BERT-and-DistilBERT--VmlldzoxMDUxNzU\n",
    "#https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a\n",
    "#https://towardsdatascience.com/scale-neural-network-training-with-sagemaker-distributed-8cf3aefcff51\n",
    "#https://towardsdatascience.com/how-to-reduce-training-time-for-a-deep-learning-model-using-tf-data-43e1989d2961\n",
    "#https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
    "#https://towardsdatascience.com/to-distil-or-not-to-distil-bert-roberta-and-xlnet-c777ad92f8\n",
    "#https://ymeadows.com/articles/fine-tuning-transformer-based-language-models\n",
    "\n",
    "### Smart Batching\n",
    "#https://towardsdatascience.com/divide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e\n",
    "#https://www.youtube.com/watch?v=ynOZUNnbEWU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.10\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install tensorflow\n",
    "!pip install transformers\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import nltk\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import activations, optimizers, losses\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import joblib \n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cdc-cdh-sagemaker-s3fs-dev'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket = 'cdc-cdh-sagemaker-s3fs-dev'\n",
    "sagemaker_session = sagemaker.Session(default_bucket=bucket)\n",
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "default_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n"
     ]
    }
   ],
   "source": [
    "%store -z\n",
    "%store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Import and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(ratio,train):\n",
    "    great_than2_classes = train['event'].value_counts()[train['event'].value_counts() >2].index\n",
    "    train = train[train['event'].isin(great_than2_classes.to_list())]\n",
    "    train_samples, _ = train_test_split(train,train_size=ratio,random_state=42,stratify=train['event'])\n",
    "    print(\"nb classes in samples\",train_samples['event'].nunique())\n",
    "    print(\"nb oservations:\",train_samples.shape)\n",
    "    #print(train_samples['event'].value_counts())\n",
    "\n",
    "    return train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(is_sample=None,ratio=None,is_test_split=False):\n",
    "  \n",
    "    train = pd.read_csv('train.csv')\n",
    "    if is_sample:\n",
    "        train = get_samples(ratio = ratio, train=train)\n",
    "    great_than2_classes = train['event'].value_counts()[train['event'].value_counts() >5].index \n",
    "    train_filter = train[train['event'].isin(great_than2_classes.to_list())]\n",
    "    \n",
    "    print(\"nb classes in final data:\",train_filter['event'].nunique())\n",
    "\n",
    "    X = train_filter['text']\n",
    "    y = train_filter['event']\n",
    "\n",
    "    print(f\"X.shape {X.shape} y.shape : {y.shape}\")\n",
    "\n",
    "    X_train_valid,X_test,y_train_valid, y_test = train_test_split(X,y,train_size=0.9,random_state=42,stratify=y)\n",
    "    \n",
    "    if is_test_split:\n",
    "        X_train,X_valid,y_train,y_valid = train_test_split(X_train_valid,y_train_valid,train_size=0.8,random_state=42,stratify=y_train_valid)\n",
    "\n",
    "    if is_test_split :\n",
    "        print(f\"X_train shape {X_train.shape} y_train shape : {y_train.shape}\")\n",
    "        print(f\"X_valid shape {X_valid.shape} y_valid shape : {y_valid.shape}\")\n",
    "        print(f\"X_test shape {X_test.shape} y_test shape : {y_test.shape}\")\n",
    "        \n",
    "        return {\n",
    "          'train': (X_train,y_train),\n",
    "          'valid': (X_valid,y_valid),\n",
    "          'test': (X_test,y_test)\n",
    "      }\n",
    "\n",
    "    else:\n",
    "        print(f\"X_train shape {X_train_valid.shape} y_train shape : {y_train_valid.shape}\")\n",
    "        print(f\"X_valid shape {X_test.shape} y_valid shape : {y_test.shape}\")\n",
    "        \n",
    "        return {\n",
    "          'train': (X_train_valid,y_train_valid),\n",
    "          'valid': (X_test,y_test),\n",
    "      }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb classes in final data: 43\n",
      "X.shape (153944,) y.shape : (153944,)\n",
      "X_train shape (138549,) y_train shape : (138549,)\n",
      "X_valid shape (15395,) y_valid shape : (15395,)\n"
     ]
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 3100\n",
    "data = get_data(is_sample=False,ratio=1)\n",
    "#data = get_data(is_test_split=True)\n",
    "X_train, y_train = data['train']\n",
    "X_valid,y_valid = data['valid']\n",
    "#X_test,y_test = data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes in train : 43\n",
      "classes in valid : 43\n",
      "[62, 13, 71, 42, 60, 64, 55, 70, 31, 43, 63, 73, 24, 50, 53, 41, 66, 26, 78, 11, 99, 12, 65, 72, 23, 27, 44, 52, 69, 22, 51, 25, 32, 21, 40, 61, 56, 54, 49, 79, 67, 45, 20]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('classes in train :',len(np.unique(y_train)))\n",
    "print('classes in valid :',len(np.unique(y_valid)))\n",
    "\n",
    "CLASSES = y_train.unique().tolist()\n",
    "print(CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Applies some pre-processing on the given text.\n",
    "\n",
    "    Steps :\n",
    "    - Removing HTML tags\n",
    "    - Removing punctuation\n",
    "    - Lowering text\n",
    "    \"\"\"\n",
    "\n",
    "    # remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    # remove the characters [\\], ['] and [\"]\n",
    "    text = re.sub(r\"\\\\\", \"\", text)\n",
    "    text = re.sub(r\"\\'\", \"\", text)\n",
    "    text = re.sub(r\"\\\"\", \"\", text)\n",
    "\n",
    "    # convert text to lowercase\n",
    "    text = text.strip().lower()\n",
    "\n",
    "    # remove all non-ASCII characters:\n",
    "    text = re.sub(r'[^\\x00-\\x7f]', r'', text)\n",
    "\n",
    "    # replace punctuation characters with spaces\n",
    "    filters = '!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    translate_dict = dict((c, \" \") for c in filters)\n",
    "    translate_map = str.maketrans(translate_dict)\n",
    "    text = text.translate(translate_map)\n",
    "    text = \" \".join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_useless_words(text,useless_words):\n",
    "    sentence = [word for word in word_tokenize(text)]\n",
    "    sentence_stop = [word for word in sentence if word not in useless_words]\n",
    "\n",
    "    text = \" \".join(sentence_stop)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    \"\"\" Preprocess : cleaning and remove stop words\"\"\"\n",
    "\n",
    "    X = X.apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "    X = X.apply(lambda x: clean_text(x))\n",
    "\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    useless_words = stopwords + list(string.punctuation) + ['yom', 'yof', 'yowm', 'yf', 'ym', 'yo']\n",
    "    # print(\"useless word : \",useless_words)\n",
    "    X = X.apply(lambda x: remove_useless_words(x,useless_words))\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after preprocessing...\n",
      "69539                     work hammering metal pole hit finger dx r index finger pain communited intraarticular fx pip\n",
      "76664     work pet groomer grooming dog another dog jumped upand dog bit pt r cheek abrasions jawline dx dog bite face\n",
      "71708                                                                                     hit chisel eyebrown lac face\n",
      "3165                      c upper arm shoulder pain w movement since yest think strained work lifting dx shoulder pain\n",
      "136917                                                           fracture pubic rami semi load pvc pipe fell unloading\n",
      "Name: text, dtype: object\n",
      "CPU times: user 24 s, sys: 11.8 ms, total: 24 s\n",
      "Wall time: 24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_processed = preprocess_data(X_train)\n",
    "X_valid_processed = preprocess_data(X_valid)\n",
    "\n",
    "print(\"after preprocessing...\")\n",
    "print(X_train_processed.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                   f puncture wound fiinger attaching cap insulin syring used home care patient\n",
       "1               contusion lt lower leg p mvc hit car guiding car gas pedal got stuck pushing door work yesterday\n",
       "2                        pt works quarry attempting dislodge large rock developed chest pains dx chest wall pain\n",
       "3                                       walking work twisted lt ankle later right knee dx left ankle knee sprain\n",
       "4                                             c low back pain lifting box work today dx left sided low back pain\n",
       "                                                          ...                                                   \n",
       "75692    coaching football collided player pain rt leg diff breathing dx fx rib left side cld fx sprain rt ankle\n",
       "75693                                               male using wire brush work piece got eye dx foreign body eye\n",
       "75694                                                                    lifting work back px dx thoracic strain\n",
       "75695                       f pt work poked lt thumb wth needle drawing blood patient dx puncture wound lt thumb\n",
       "75696                                                                picking car hurt back work dx low back pain\n",
       "Name: text, Length: 75687, dtype: object"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('./data/raw/test_data.csv')\n",
    "test_data = test_data[~test_data['event'].isin([10,29,30,59,74])]\n",
    "X_test, y_test = test_data['text'], test_data['event']\n",
    "X_test_processed = preprocess_data(X_test)\n",
    "X_test_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75687"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and Encode Train and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.7 s, sys: 1.16 s, total: 16.9 s\n",
      "Wall time: 4.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import  AutoTokenizer, DistilBertTokenizerFast,BertTokenizerFast,RobertaTokenizerFast\n",
    "MAX_LEN = 45\n",
    "\n",
    "\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "x_train = X_train_processed.to_list()\n",
    "x_valid = X_valid_processed.to_list()\n",
    "\n",
    "\n",
    "train_encodings = tokenizer(x_train, max_length=MAX_LEN, truncation=True, padding='max_length',return_tensors='tf')\n",
    "valid_encodings = tokenizer(x_valid, max_length=MAX_LEN, truncation=True, padding='max_length',return_tensors='tf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[  101  2147 27883  3384  6536  2718  4344  1040  2595  1054  5950  4344\n",
      "  3255  4012 23041 17572 26721  8445 21412 23292 28315   102     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0], shape=(45,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0], shape=(45,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(train_encodings['input_ids'][0])\n",
    "print(train_encodings['attention_mask'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Labels .....\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"Encoding Labels .....\")\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "y_train_encode = np.asarray(encoder.transform(y_train))\n",
    "y_valid_encode = np.asarray(encoder.transform(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = { id:str(label) for id, label in enumerate(encoder.classes_)}\n",
    "label2id = { str(label):id for id, label in enumerate(encoder.classes_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes in train : 43\n",
      "classes in valid : 43\n"
     ]
    }
   ],
   "source": [
    "print('classes in train :',len(np.unique(y_train_encode)))\n",
    "print('classes in valid :',len(np.unique(y_valid_encode)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_path='./data/train/'\n",
    "os.makedirs(data_path,exist_ok=True)\n",
    "\n",
    "from pickle import dump\n",
    "dump(encoder,open(os.path.join(data_path,'encode.pkl'),'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TF Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.64 ms, sys: 0 ns, total: 3.64 ms\n",
      "Wall time: 3.39 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def construct_tfdataset(encodings, y=None):\n",
    "    if y is not None:\n",
    "        return tf.data.Dataset.from_tensor_slices((dict(encodings),y))\n",
    "    else:\n",
    "        # this case is used when making predictions on unseen samples after training\n",
    "        return tf.data.Dataset.from_tensor_slices(dict(encodings))\n",
    "    \n",
    "train_tfdataset = construct_tfdataset(train_encodings, y_train_encode)\n",
    "valid_tfdataset = construct_tfdataset(valid_encodings, y_valid_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TF Records File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _save_feature_as_tfrecord(tfdataset,file_path):\n",
    "    \"\"\" Helper function to save the tf dataset as tfrecords\"\"\"\n",
    "    \n",
    "\n",
    "    def single_example_data(encoding,label_id):\n",
    "\n",
    "        input_ids =encoding['input_ids']\n",
    "        attention_mask = encoding['attention_mask']\n",
    "\n",
    "        tfrecord_features = collections.OrderedDict()\n",
    "\n",
    "        def _int64_feature(value):\n",
    "            \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "            return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "        tfrecord_features['input_ids'] = _int64_feature(input_ids)\n",
    "        tfrecord_features['attention_mask'] = _int64_feature(attention_mask)\n",
    "        tfrecord_features['label_ids'] =  _int64_feature([label_id])\n",
    "\n",
    "        _example = tf.train.Example(features=tf.train.Features(feature=tfrecord_features))\n",
    "\n",
    "        return _example.SerializeToString()\n",
    "\n",
    "    def data_generator():\n",
    "        for features in tfdataset:\n",
    "            yield single_example_data(*features)\n",
    "\n",
    "    serialized_tfdataset = tf.data.Dataset.from_generator(\n",
    "        data_generator, output_types=tf.string, output_shapes=())        \n",
    "\n",
    "    writer = tf.data.experimental.TFRecordWriter(file_path)\n",
    "    writer.write(serialized_tfdataset)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "# training data\n",
    "_save_feature_as_tfrecord(train_tfdataset,'./data/train/train.tfrecord')\n",
    "\n",
    "#validation data\n",
    "_save_feature_as_tfrecord(valid_tfdataset,'./data/valid/valid.tfrecord')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://cdc-cdh-sagemaker-s3fs-dev/projects/project006/injury-data/training'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = 'projects/project006/injury-data/training'\n",
    "tfrecord_train_location = sagemaker_session.upload_data(path = './data/train',\n",
    "                                                      bucket = bucket,\n",
    "                                                      key_prefix = prefix)\n",
    "tfrecord_train_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://cdc-cdh-sagemaker-s3fs-dev/projects/project006/injury-data/validation'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = 'projects/project006/injury-data/validation'\n",
    "tfrecord_valid_location = sagemaker_session.upload_data(path = './data/valid',\n",
    "                                                      bucket = bucket,\n",
    "                                                      key_prefix = prefix)\n",
    "tfrecord_valid_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Run BERT Training Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training output path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://cdc-cdh-sagemaker-s3fs-dev/projects/project006/output/'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = 'projects/project006/output'\n",
    "output_path = 's3://{}/{}/'.format(bucket, prefix )\n",
    "output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m  TFDistilBertForSequenceClassification,DistilBertConfig\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TFBertForSequenceClassification, BertConfig\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TFRobertaForSequenceClassification, RobertaConfig\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptimizers\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mschedules\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m PolynomialDecay\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptimization_tf\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m WarmUp, AdamWeightDecay\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m load\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_parse_args\u001b[39;49;00m():\n",
      "    parser = argparse.ArgumentParser()\n",
      "    \u001b[37m#parser.add_argument(\"--model-dir\",type=str)\u001b[39;49;00m\n",
      "    \u001b[37m#parser.add_argument('--output-data-dir', type=str,default=os.environ['SM_OUTPUT_DATA_DIR'])\u001b[39;49;00m\n",
      "    \u001b[37m#parser.add_argument(\"--sm-model-dir\",type=str,default=os.environ.get(\"SM_MODEL_DIR\"))\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,default=os.environ.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--validation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,default=os.environ.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_VALIDATION\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    \n",
      "    \u001b[37m## Hyperparams\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,default=\u001b[33m'\u001b[39;49;00m\u001b[33mBaseBERT\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--num_records\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--optimizer\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,default=\u001b[33m'\u001b[39;49;00m\u001b[33madam\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--max_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,default=\u001b[34m45\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,default=\u001b[34m5e-5\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,default=\u001b[34m1\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,default=\u001b[34m32\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--valid_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,default=\u001b[34m8\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--steps_per_epoch\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--validation_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m  parser.parse_known_args()\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mDistilBERT\u001b[39;49;00m:\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, params):\n",
      "        \u001b[36mself\u001b[39;49;00m.num_labels = params[\u001b[33m'\u001b[39;49;00m\u001b[33mnum_labels\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.max_len = params[\u001b[33m'\u001b[39;49;00m\u001b[33mmax_len\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.learning_rate = params[\u001b[33m'\u001b[39;49;00m\u001b[33mlearning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.id2label = params[\u001b[33m'\u001b[39;49;00m\u001b[33mid2label\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.label2id = params[\u001b[33m'\u001b[39;49;00m\u001b[33mlabel2id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.batch_size = params[\u001b[33m'\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.epochs = params[\u001b[33m'\u001b[39;49;00m\u001b[33mepochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.num_records = params[\u001b[33m'\u001b[39;49;00m\u001b[33mnum_records\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mbuild\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\n",
      "        \n",
      "        freeze_bert_layer = \u001b[34mFalse\u001b[39;49;00m\n",
      "        \n",
      "        \n",
      "        transformer_config = DistilBertConfig(num_labels=\u001b[36mself\u001b[39;49;00m.num_labels,\n",
      "                                   id2label = \u001b[36mself\u001b[39;49;00m.id2label,\n",
      "                                   label2id  = \u001b[36mself\u001b[39;49;00m.label2id,\n",
      "                                   output_attentions = \u001b[34mFalse\u001b[39;49;00m,\n",
      "                                   output_hidden_states= \u001b[34mFalse\u001b[39;49;00m)\n",
      "                                   \n",
      "        transformer_model = TFDistilBertForSequenceClassification.from_pretrained(\u001b[33m\"\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,config=transformer_config)\n",
      "        input_ids = tf.keras.Input(shape = (\u001b[36mself\u001b[39;49;00m.max_len,),name=\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,dtype=\u001b[33m'\u001b[39;49;00m\u001b[33mint32\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        attention_mask = tf.keras.Input(shape = (\u001b[36mself\u001b[39;49;00m.max_len,),name=\u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,dtype=\u001b[33m'\u001b[39;49;00m\u001b[33mint32\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \n",
      "        embbeding_layer = transformer_model.distilbert(input_ids,attention_mask=attention_mask)[\u001b[34m0\u001b[39;49;00m]\n",
      "        X = tf.keras.layers.Conv1D(\u001b[34m512\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(embbeding_layer)\n",
      "        X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
      "        X = tf.keras.layers.Dense(\u001b[34m256\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(X)\n",
      "        X = tf.keras.layers.Dropout(\u001b[34m0.2\u001b[39;49;00m)(X)\n",
      "        X = tf.keras.layers.Dense(\u001b[36mself\u001b[39;49;00m.num_labels, activation=\u001b[33m'\u001b[39;49;00m\u001b[33msoftmax\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(X)\n",
      "        \n",
      "        model = tf.keras.Model(inputs=[input_ids,attention_mask], outputs = X)\n",
      "        \n",
      "        \u001b[34mfor\u001b[39;49;00m layer \u001b[35min\u001b[39;49;00m model.layers[:\u001b[34m3\u001b[39;49;00m]:\n",
      "            layer.trainable = \u001b[35mnot\u001b[39;49;00m freeze_bert_layer\n",
      "            \n",
      "        num_train_steps = (\u001b[36mself\u001b[39;49;00m.num_records // \u001b[36mself\u001b[39;49;00m.batch_size) * \u001b[36mself\u001b[39;49;00m.epochs\n",
      "        decay_schedule_fn = PolynomialDecay(\n",
      "            initial_learning_rate=\u001b[36mself\u001b[39;49;00m.learning_rate,\n",
      "            end_learning_rate=\u001b[34m0.\u001b[39;49;00m,\n",
      "            decay_steps=num_train_steps\n",
      "            )\n",
      "        \n",
      "        warmup_steps = num_train_steps * \u001b[34m0.1\u001b[39;49;00m\n",
      "        warmup_schedule  = WarmUp(\u001b[36mself\u001b[39;49;00m.learning_rate,decay_schedule_fn,warmup_steps)\n",
      "        \n",
      "        \u001b[37m# fine optimizer and loss\u001b[39;49;00m\n",
      "        \u001b[37m#optimizer = tf.keras.optimizers.Adam(learning_rate=lr_scheduler)\u001b[39;49;00m\n",
      "        optimizer = AdamWeightDecay (learning_rate=warmup_schedule, weight_decay_rate=\u001b[34m0.01\u001b[39;49;00m, epsilon=\u001b[34m1e-6\u001b[39;49;00m)\n",
      "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits= \u001b[34mFalse\u001b[39;49;00m)\n",
      "        metrics = [\u001b[33m'\u001b[39;49;00m\u001b[33macc\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
      "\n",
      "        \u001b[34mreturn\u001b[39;49;00m model\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mBaseBERT\u001b[39;49;00m:\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, params):\n",
      "        \u001b[36mself\u001b[39;49;00m.num_labels = params[\u001b[33m'\u001b[39;49;00m\u001b[33mnum_labels\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.max_len = params[\u001b[33m'\u001b[39;49;00m\u001b[33mmax_len\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.learning_rate = params[\u001b[33m'\u001b[39;49;00m\u001b[33mlearning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.id2label = params[\u001b[33m'\u001b[39;49;00m\u001b[33mid2label\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.label2id = params[\u001b[33m'\u001b[39;49;00m\u001b[33mlabel2id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.batch_size = params[\u001b[33m'\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.epochs = params[\u001b[33m'\u001b[39;49;00m\u001b[33mepochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.num_records = params[\u001b[33m'\u001b[39;49;00m\u001b[33mnum_records\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mbuild\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):      \n",
      "        \n",
      "        freeze_bert_layer = \u001b[34mFalse\u001b[39;49;00m\n",
      "        \n",
      "        transformer_model = TFBertForSequenceClassification.from_pretrained(\u001b[33m\"\u001b[39;49;00m\u001b[33mbert-base-uncased\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,num_labels=\u001b[36mself\u001b[39;49;00m.num_labels, \n",
      "                 output_attentions = \u001b[34mFalse\u001b[39;49;00m,\n",
      "                output_hidden_states= \u001b[34mFalse\u001b[39;49;00m )\n",
      "        input_ids = tf.keras.Input(shape = (\u001b[36mself\u001b[39;49;00m.max_len,),name=\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,dtype=\u001b[33m'\u001b[39;49;00m\u001b[33mint32\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        attention_mask = tf.keras.Input(shape = (\u001b[36mself\u001b[39;49;00m.max_len,),name=\u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,dtype=\u001b[33m'\u001b[39;49;00m\u001b[33mint32\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \n",
      "        embbeding_layer = transformer_model.bert(input_ids,attention_mask=attention_mask)[\u001b[34m0\u001b[39;49;00m]\n",
      "        X = tf.keras.layers.Conv1D(\u001b[34m512\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(embbeding_layer)\n",
      "        X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
      "        X = tf.keras.layers.Dense(\u001b[34m256\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(X)\n",
      "        X = tf.keras.layers.Dropout(\u001b[34m0.2\u001b[39;49;00m)(X)\n",
      "        X = tf.keras.layers.Dense(\u001b[36mself\u001b[39;49;00m.num_labels, activation=\u001b[33m'\u001b[39;49;00m\u001b[33msoftmax\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(X)\n",
      "\n",
      "        model = tf.keras.Model(inputs=[input_ids,attention_mask], outputs = X)\n",
      "        \n",
      "        \u001b[34mfor\u001b[39;49;00m layer \u001b[35min\u001b[39;49;00m model.layers[:\u001b[34m3\u001b[39;49;00m]:\n",
      "            layer.trainable = \u001b[35mnot\u001b[39;49;00m freeze_bert_layer\n",
      "        \n",
      "        num_train_steps = (\u001b[36mself\u001b[39;49;00m.num_records // \u001b[36mself\u001b[39;49;00m.batch_size) * \u001b[36mself\u001b[39;49;00m.epochs\n",
      "        decay_schedule_fn = PolynomialDecay(\n",
      "            initial_learning_rate=\u001b[36mself\u001b[39;49;00m.learning_rate,\n",
      "            end_learning_rate=\u001b[34m0.\u001b[39;49;00m,\n",
      "            decay_steps=num_train_steps\n",
      "            )\n",
      "        \n",
      "        warmup_steps = num_train_steps * \u001b[34m0.1\u001b[39;49;00m\n",
      "        warmup_schedule  = WarmUp(\u001b[36mself\u001b[39;49;00m.learning_rate,decay_schedule_fn,warmup_steps)\n",
      "        \n",
      "        \u001b[37m#optimizer = tf.keras.optimizers.Adam(learning_rate=lr_scheduler)\u001b[39;49;00m\n",
      "        optimizer = AdamWeightDecay (learning_rate=warmup_schedule, weight_decay_rate=\u001b[34m0.01\u001b[39;49;00m, epsilon=\u001b[34m1e-6\u001b[39;49;00m)\n",
      "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    \n",
      "        model.compile(optimizer=optimizer, loss=loss, metrics=[\u001b[33m'\u001b[39;49;00m\u001b[33macc\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "        \n",
      "        \u001b[34mreturn\u001b[39;49;00m model\n",
      "    \n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mBaseRoberta\u001b[39;49;00m:\n",
      "    \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, params):\n",
      "        \u001b[36mself\u001b[39;49;00m.num_labels = params[\u001b[33m'\u001b[39;49;00m\u001b[33mnum_labels\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.max_len = params[\u001b[33m'\u001b[39;49;00m\u001b[33mmax_len\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.learning_rate = params[\u001b[33m'\u001b[39;49;00m\u001b[33mlearning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.id2label = params[\u001b[33m'\u001b[39;49;00m\u001b[33mid2label\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.label2id = params[\u001b[33m'\u001b[39;49;00m\u001b[33mlabel2id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.batch_size = params[\u001b[33m'\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.epochs = params[\u001b[33m'\u001b[39;49;00m\u001b[33mepochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.num_records = params[\u001b[33m'\u001b[39;49;00m\u001b[33mnum_records\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mbuild\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):      \n",
      "        \n",
      "        freeze_bert_layer = \u001b[34mFalse\u001b[39;49;00m\n",
      "        \n",
      "        transformer_model = TFRobertaForSequenceClassification.from_pretrained(\u001b[33m\"\u001b[39;49;00m\u001b[33mroberta-base\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, num_labels=\u001b[36mself\u001b[39;49;00m.num_labels, \n",
      "                 output_attentions = \u001b[34mFalse\u001b[39;49;00m,\n",
      "                output_hidden_states= \u001b[34mFalse\u001b[39;49;00m )\n",
      "        input_ids = tf.keras.Input(shape = (\u001b[36mself\u001b[39;49;00m.max_len,),name=\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,dtype=\u001b[33m'\u001b[39;49;00m\u001b[33mint32\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        attention_mask = tf.keras.Input(shape = (\u001b[36mself\u001b[39;49;00m.max_len,),name=\u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,dtype=\u001b[33m'\u001b[39;49;00m\u001b[33mint32\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \n",
      "        embbeding_layer = transformer_model.roberta(input_ids,attention_mask=attention_mask)[\u001b[34m0\u001b[39;49;00m]\n",
      "        X = tf.keras.layers.GlobalMaxPool1D()(embbeding_layer)\n",
      "        X = tf.keras.layers.Dense(\u001b[36mself\u001b[39;49;00m.num_labels, activation=\u001b[33m'\u001b[39;49;00m\u001b[33msoftmax\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(X)\n",
      "        \n",
      "        model = tf.keras.Model(inputs=[input_ids,attention_mask], outputs = X)\n",
      "        \n",
      "        \u001b[34mfor\u001b[39;49;00m layer \u001b[35min\u001b[39;49;00m model.layers[:\u001b[34m3\u001b[39;49;00m]:\n",
      "            layer.trainable = \u001b[35mnot\u001b[39;49;00m freeze_bert_layer\n",
      "        \n",
      "        num_train_steps = (\u001b[36mself\u001b[39;49;00m.num_records // \u001b[36mself\u001b[39;49;00m.batch_size) * \u001b[36mself\u001b[39;49;00m.epochs\n",
      "        decay_schedule_fn = PolynomialDecay(\n",
      "            initial_learning_rate=\u001b[36mself\u001b[39;49;00m.learning_rate,\n",
      "            end_learning_rate=\u001b[34m0.\u001b[39;49;00m,\n",
      "            decay_steps=num_train_steps\n",
      "            )\n",
      "        \n",
      "        warmup_steps = num_train_steps * \u001b[34m0.1\u001b[39;49;00m\n",
      "        warmup_schedule  = WarmUp(\u001b[36mself\u001b[39;49;00m.learning_rate,decay_schedule_fn,warmup_steps)\n",
      "        \n",
      "        \u001b[37m#optimizer = tf.keras.optimizers.Adam(learning_rate=lr_scheduler)\u001b[39;49;00m\n",
      "        optimizer = AdamWeightDecay (learning_rate=warmup_schedule, weight_decay_rate=\u001b[34m0.01\u001b[39;49;00m, epsilon=\u001b[34m1e-6\u001b[39;49;00m)\n",
      "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=\u001b[34mFalse\u001b[39;49;00m)\n",
      "        \n",
      "        model.compile(optimizer=optimizer, loss=loss, metrics=[\u001b[33m'\u001b[39;49;00m\u001b[33macc\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "        \n",
      "        \u001b[34mreturn\u001b[39;49;00m model\n",
      "        \n",
      "        \n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_load_data\u001b[39;49;00m(train_dir,valid_dir,MAX_LEN,epochs,batch_size,valid_batch_size,steps_per_epoch,validation_steps):\n",
      "          \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain_dir : \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,train_dir)    \n",
      "    train_file = os.path.join(train_dir,\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain.tfrecord\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain_file : \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,train_file)\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mvalid_dir:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,valid_dir)\n",
      "    valid_file = os.path.join(valid_dir,\u001b[33m\"\u001b[39;49;00m\u001b[33mvalid.tfrecord\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mvalid_file:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,valid_file)\n",
      "    \n",
      "    \u001b[37m# Create a description of the features.\u001b[39;49;00m\n",
      "    feature_description = {\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: tf.io.FixedLenFeature([MAX_LEN], tf.int64),\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: tf.io.FixedLenFeature([MAX_LEN], tf.int64),\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: tf.io.FixedLenFeature([], tf.int64),\n",
      "    }\n",
      "        \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m_parse_function\u001b[39;49;00m(example_proto):\n",
      "\n",
      "        \u001b[37m# Parse the input `tf.train.Example` proto using the dictionary above.\u001b[39;49;00m\n",
      "        parsed  = tf.io.parse_single_example(example_proto, feature_description)\n",
      "\n",
      "        \u001b[34mreturn\u001b[39;49;00m {\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:parsed[\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:parsed[\u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]},parsed[\u001b[33m'\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \n",
      "    \n",
      "    train_dataset = tf.data.TFRecordDataset(train_file)\n",
      "    train_dataset = train_dataset.repeat(epochs * steps_per_epoch)\n",
      "    train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
      "    train_dataset = train_dataset.map(_parse_function,num_parallel_calls=tf.data.AUTOTUNE)\n",
      "    train_dataset = train_dataset.batch(batch_size)\n",
      "    \n",
      "    train_dataset.cache()\n",
      "    \n",
      "    valid_dataset = tf.data.TFRecordDataset(valid_file)\n",
      "    valid_dataset = valid_dataset.repeat(epochs * validation_steps)\n",
      "    valid_dataset = valid_dataset.prefetch(tf.data.AUTOTUNE)\n",
      "    valid_dataset = valid_dataset.map(_parse_function,num_parallel_calls=tf.data.AUTOTUNE)\n",
      "    valid_dataset = valid_dataset.batch(valid_batch_size)\n",
      "    \n",
      "    valid_dataset.cache()\n",
      "   \n",
      "    \u001b[34mreturn\u001b[39;49;00m train_dataset, valid_dataset\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_load_encoder\u001b[39;49;00m(train_dir):\n",
      "    \u001b[37m# load the encoder\u001b[39;49;00m\n",
      "    encoder_file = os.path.join(train_dir,\u001b[33m'\u001b[39;49;00m\u001b[33mencode.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \n",
      "    encoder = load(\u001b[36mopen\u001b[39;49;00m(encoder_file, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m encoder\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m():\n",
      "    args, unknown = _parse_args()\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33minput train: \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,args.train)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33minput valid: \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,args.validation)\n",
      "\n",
      "    train_dataset, valid_dataset = _load_data(args.train,\n",
      "                                              args.validation,\n",
      "                                              args.max_len,\n",
      "                                              args.epochs,\n",
      "                                              args.batch_size,\n",
      "                                              args.valid_batch_size,\n",
      "                                              args.steps_per_epoch,\n",
      "                                              args.validation_steps)    \n",
      "    encoder = _load_encoder(args.train)\n",
      "    \n",
      "    CLASSES = encoder.classes_\n",
      "    id2label = { \u001b[36mid\u001b[39;49;00m:\u001b[36mstr\u001b[39;49;00m(label) \u001b[34mfor\u001b[39;49;00m \u001b[36mid\u001b[39;49;00m, label \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(encoder.classes_)}\n",
      "    label2id = { \u001b[36mstr\u001b[39;49;00m(label):\u001b[36mid\u001b[39;49;00m \u001b[34mfor\u001b[39;49;00m \u001b[36mid\u001b[39;49;00m, label \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(encoder.classes_)}\n",
      "    \n",
      "    params = {\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mnum_labels\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[36mlen\u001b[39;49;00m(CLASSES),\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mmax_len\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.max_len,\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mlearning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.learning_rate,\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mid2label\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: id2label,\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mlabel2id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: label2id,\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.batch_size,\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mepochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.epochs,\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mnum_records\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:args.num_records\n",
      "        }\n",
      "    \n",
      "    \u001b[34mif\u001b[39;49;00m args.model_name == \u001b[33m'\u001b[39;49;00m\u001b[33mDistilBERT\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m :\n",
      "        model = DistilBERT(params).build()   \n",
      "    \u001b[34melif\u001b[39;49;00m args.model_name == \u001b[33m'\u001b[39;49;00m\u001b[33mBaseBERT\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "        model = BaseBERT(params).build()\n",
      "    \u001b[34melif\u001b[39;49;00m args.model_name == \u001b[33m'\u001b[39;49;00m\u001b[33mBaseRoberta\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "        model = BaseRoberta(params).build()\n",
      "        \n",
      "    \u001b[36mprint\u001b[39;49;00m(model.summary())\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation steps:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,args.validation_steps)\n",
      "        \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mTraining....\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    history = model.fit(x=train_dataset.shuffle(args.num_records),\n",
      "                        steps_per_epoch = args.steps_per_epoch,\n",
      "                        batch_size=args.batch_size,\n",
      "                        epochs=args.epochs,\n",
      "                        validation_data=valid_dataset,\n",
      "                        validation_steps=args.validation_steps)\n",
      "        \n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33msaving model...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[37m#model.save(os.path.join(args.sm_model_dir,f\"{args.model_name}.h5\"))\u001b[39;49;00m\n",
      "    model.save(os.path.join(args.sm_model_dir,\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{args.model_name}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    \u001b[37m#odel.save(f\"./output/model/{args.model_name}/{args.model_name}_test.h5\")\u001b[39;49;00m\n",
      "\n",
      "    \n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    main()\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow 2.3 script\n",
    "!pygmentize './src/train.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records = len(X_train)\n",
    "num_valid_records = len(X_valid)\n",
    "max_len = MAX_LEN\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "valid_batch_size = 16\n",
    "steps_per_epoch = num_records // batch_size\n",
    "validation_steps = num_valid_records // valid_batch_size\n",
    "learning_rate = 5e-5\n",
    "optimizer = 'adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138549\n",
      "8659\n",
      "962\n"
     ]
    }
   ],
   "source": [
    "print(num_records)\n",
    "print(steps_per_epoch)\n",
    "print(validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./src/train.py --train ./data/train --validation ./data/validation --epochs 1 --num_records 138549 --steps_per_epoch 8659 --validation_steps 962"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Hugging Face Estimator to start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from time import gmtime, strftime\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "model_name = 'BaseBERT'\n",
    "job_name_prefix = f\"training-{model_name}\"\n",
    "timestamp = strftime(\"-%m-%d-%M-%S\", gmtime())\n",
    "\n",
    "job_name = job_name_prefix + timestamp\n",
    "\n",
    "_estimator = HuggingFace(\n",
    "        base_job_name  = job_name,\n",
    "        entry_point=\"train.py\",\n",
    "        source_dir = \"./src/\",\n",
    "        role=role,\n",
    "        instance_count=1,\n",
    "        volume_size = 5,\n",
    "        max_run = 18000,\n",
    "        instance_type='ml.p3.2xlarge',\n",
    "        transformers_version = \"4.4\",\n",
    "        tensorflow_version  = \"2.4\",\n",
    "        py_version=\"py37\",\n",
    "        output_path = output_path,\n",
    "        hyperparameters = {\n",
    "                \"model_name\": model_name,\n",
    "                \"num_records\":  num_records,\n",
    "                \"max_len\":max_len,\n",
    "                \"epochs\":int(epochs),\n",
    "                \"learning_rate\":float(learning_rate),\n",
    "                \"batch_size\":int(batch_size),\n",
    "                \"valid_batch_size\":valid_batch_size,\n",
    "                \"steps_per_epoch\": steps_per_epoch,\n",
    "                \"validation_steps\": validation_steps,\n",
    "                \"optimizer\":optimizer\n",
    "                },\n",
    "        metric_definitions = [{'Name':'train:loss','Regex':'loss: ([0-9\\\\.]+)'},\n",
    "                                    {'Name':'train:accuracy','Regex':'acc: ([0-9\\\\.]+)'},\n",
    "                                    {'Name':'validation:loss','Regex':'val_loss: ([0-9\\\\.]+)'},\n",
    "                                    {'Name':'validation:accuracy','Regex':'val_acc: ([0-9\\\\.]+)'}],\n",
    "        enable_sagemaker_metrics = True\n",
    "    )\n",
    "\n",
    "\n",
    "train_data = sagemaker.inputs.TrainingInput(\n",
    "    tfrecord_train_location, # Replace None\n",
    "    distribution='FullyReplicated'\n",
    ")\n",
    "\n",
    "validation_data = sagemaker.inputs.TrainingInput(\n",
    "    tfrecord_valid_location, # Replace None\n",
    "    distribution='FullyReplicated'\n",
    ")\n",
    "\n",
    "_estimator.fit({'train':train_data,'validation':validation_data}, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the trained distilbert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training-BaseBERT-08-10-24-42-2021-08-10-13-24-42-457\n"
     ]
    }
   ],
   "source": [
    "print(_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3  import S3Downloader\n",
    "import os\n",
    "\n",
    "def download_model(sagemaker_session,job_name):\n",
    "    \n",
    "    if job_name is not None :\n",
    "        modeldesc = sagemaker_session.describe_training_job(job_name)\n",
    "        s3_model_path = modeldesc['ModelArtifacts']['S3ModelArtifacts']\n",
    "    \n",
    "    os.makedirs(f\"./output/model/{job_name}/\",exist_ok=True)\n",
    "\n",
    "    S3Downloader.download(\n",
    "        s3_uri=s3_model_path, # s3 uri where the trained model is located\n",
    "        local_path=f\"./output/model/{job_name}/\", # local path where *.targ.gz is saved\n",
    "        sagemaker_session=sagemaker_session # sagemaker session used for training the model\n",
    "    )\n",
    "    \n",
    "    return s3_model_path, modeldesc['HyperParameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://cdc-cdh-sagemaker-s3fs-dev/projects/project006/output/training-BaseBERT-08-02-58-54-2021-08-02-19-58-55-017/output/model.tar.gz\n",
      "{'batch_size': '16', 'epochs': '5', 'learning_rate': '5e-05', 'max_len': '45', 'model_name': '\"BaseBERT\"', 'num_records': '138549', 'optimizer': '\"adam\"', 'sagemaker_container_log_level': '20', 'sagemaker_job_name': '\"training-BaseBERT-08-02-58-54-2021-08-02-19-58-55-017\"', 'sagemaker_program': '\"train_keras.py\"', 'sagemaker_region': '\"us-east-1\"', 'sagemaker_submit_directory': '\"s3://cdc-cdh-sagemaker-s3fs-dev/training-BaseBERT-08-02-58-54-2021-08-02-19-58-55-017/source/sourcedir.tar.gz\"', 'steps_per_epoch': '8659', 'valid_batch_size': '16', 'validation_steps': '962'}\n"
     ]
    }
   ],
   "source": [
    "#downloading model\n",
    "job_name = 'training-BaseBERT-08-02-58-54-2021-08-02-19-58-55-017'\n",
    "s3_model_path, hp = download_model(sagemaker_session,job_name)\n",
    "\n",
    "print(s3_model_path)\n",
    "print(hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract and load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "from transformers.optimization_tf import AdamWeightDecay\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "\n",
    "def extract_data_load_model(job_name,model_name):\n",
    "\n",
    "    t = tarfile.open(f'./output/model/{job_name}/model.tar.gz', 'r:gz')\n",
    "    t.extractall(path=f'./output/model/{job_name}')\n",
    "    _model = tf.keras.models.load_model(f\"./output/model/{job_name}/{model_name}\",custom_objects={'AdamWeightDecay':AdamWeightDecay})\n",
    "    \n",
    "    return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'BaseBERT'\n",
    "job_name = 'training-BaseBERT-08-02-58-54-2021-08-02-19-58-55-017'\n",
    "loaded_model = extract_data_load_model(job_name,model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_data(train_dir,valid_dir,MAX_LEN,epochs,batch_size,valid_batch_size,steps_per_epoch,validation_steps):\n",
    "    \"\"\" Helper function to load,parse and create input data pipeline from TFRecords\"\"\"\n",
    "          \n",
    "    train_file = os.path.join(train_dir,\"train.tfrecord\") \n",
    "    valid_file = os.path.join(valid_dir,\"valid.tfrecord\")\n",
    "    \n",
    "    # Create a description of the features.\n",
    "    feature_description = {\n",
    "        'input_ids': tf.io.FixedLenFeature([MAX_LEN], tf.int64),\n",
    "        'attention_mask': tf.io.FixedLenFeature([MAX_LEN], tf.int64),\n",
    "        'label_ids': tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "        \n",
    "    def _parse_function(example_proto):\n",
    "\n",
    "        # Parse the input `tf.train.Example` proto using the dictionary above.\n",
    "        parsed  = tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "        return {'input_ids':parsed['input_ids'],'attention_mask':parsed['attention_mask']},parsed['label_ids']\n",
    "        \n",
    "    \n",
    "    train_dataset = tf.data.TFRecordDataset(train_file)\n",
    "    train_dataset = train_dataset.repeat(epochs * steps_per_epoch)\n",
    "    train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    train_dataset = train_dataset.map(_parse_function,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    train_dataset = train_dataset.batch(batch_size)\n",
    "    \n",
    "    \n",
    "    \n",
    "    valid_dataset = tf.data.TFRecordDataset(valid_file)\n",
    "    valid_dataset = valid_dataset.repeat(epochs * validation_steps)\n",
    "    valid_dataset = valid_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    valid_dataset = valid_dataset.map(_parse_function,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    valid_dataset = valid_dataset.batch(valid_batch_size)\n",
    "    \n",
    "   \n",
    "    return train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = './data/train'\n",
    "valid_dir= './data/valid'\n",
    "train_dataset,valid_dataset = _load_data(train_dir,valid_dir,MAX_LEN,epochs,batch_size,valid_batch_size,steps_per_epoch,validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Training data...\n",
      "8659/8659 [==============================] - 3008s 347ms/step - loss: 0.1164 - acc: 0.9641\n",
      "Training loss:  0.11638274043798447\n",
      "Training accuracy:  0.9641124606132507\n",
      "Evaluating Validation data...\n",
      "962/962 [==============================] - 334s 348ms/step - loss: 0.5353 - acc: 0.8526\n",
      "Validation loss:  0.535329282283783\n",
      "Validation accuracy:  0.8525857329368591\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def _evaluate_model(loaded_model):\n",
    "    print(\"Evaluating Training data...\")\n",
    "    train_score = loaded_model.evaluate(train_dataset,\n",
    "                                     steps = steps_per_epoch,\n",
    "                                     batch_size=batch_size)\n",
    "\n",
    "    print(\"Training loss: \", train_score[0])\n",
    "    print(\"Training accuracy: \", train_score[1])\n",
    "\n",
    "    print(\"Evaluating Validation data...\")\n",
    "    valid_score = loaded_model.evaluate(valid_dataset, steps = validation_steps,batch_size=valid_batch_size)\n",
    "    print(\"Validation loss: \", valid_score[0])\n",
    "    print(\"Validation accuracy: \", valid_score[1])\n",
    "    \n",
    "_evaluate_model(loaded_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the fine-tuned model to predict new samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'bert-base-uncased'\n",
    "\n",
    "def _create_predictor(model, encoder,model_name, max_len,text):\n",
    "    tkzr = AutoTokenizer.from_pretrained(model_name)\n",
    "    x = [text]\n",
    "    encodings =  tkzr(x, max_length=max_len, truncation=True, padding='max_length',return_tensors='tf')\n",
    "    tfdataset = construct_tfdataset(encodings)\n",
    "    tfdataset = tfdataset.batch(1)\n",
    "    preds = model.predict(tfdataset)\n",
    "    categories = encoder.classes_.tolist()\n",
    "    enc = np.argmax(preds[0])\n",
    "    \n",
    "    return {     'text' : x,\n",
    "                 'predict_proba' : preds[0][np.argmax(preds[0])],\n",
    "                 'predicted_class' : categories[np.argmax(preds)]                             \n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/engine/functional.py:585: UserWarning: Input dict contained keys ['token_type_ids'] which did not match any model input. They will be ignored by the model.\n",
      "  [n for n in tensors.keys() if n not in ref_input_names])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': ['sp lt ankle going steps twisted ankle work'],\n",
       " 'predict_proba': 0.9985917,\n",
       " 'predicted_class': 73}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 'sp lt ankle going steps twisted ankle work'\n",
    "_create_predictor(loaded_model, encoder,MODEL_NAME, MAX_LEN,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Save Predictions and calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15395\n",
      "CPU times: user 28min 27s, sys: 20 s, total: 28min 47s\n",
      "Wall time: 4min 32s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>true_event</th>\n",
       "      <th>preds_encode</th>\n",
       "      <th>preds_event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>concussion cow kicked gate struck forehead</td>\n",
       "      <td>62</td>\n",
       "      <td>29</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fractured hand work</td>\n",
       "      <td>99</td>\n",
       "      <td>27</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>corneal abrasion kickback x bruising eye work</td>\n",
       "      <td>62</td>\n",
       "      <td>29</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>complains lt thumb pain slamming door last nightat work dx acute lt thumb sparin injury</td>\n",
       "      <td>62</td>\n",
       "      <td>29</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sts works microbio lab pt smelled paint thinner became dizzy lightheaded dx inhalant exposure w dizziness</td>\n",
       "      <td>55</td>\n",
       "      <td>25</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>l shoulder pain lifting pt work x shoulder strain</td>\n",
       "      <td>71</td>\n",
       "      <td>37</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pain l eye working piece wood went eye dx corneal abrasion</td>\n",
       "      <td>66</td>\n",
       "      <td>33</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dx forearm laceration p cut x work</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>c lbp lifting heavy equipment engine block work dx acute lbp</td>\n",
       "      <td>71</td>\n",
       "      <td>37</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>work tonight pt scratched pt way hospital dx superficial fingernail scratches forearm</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                        text  \\\n",
       "0                                                                 concussion cow kicked gate struck forehead   \n",
       "1                                                                                        fractured hand work   \n",
       "2                                                              corneal abrasion kickback x bruising eye work   \n",
       "3                    complains lt thumb pain slamming door last nightat work dx acute lt thumb sparin injury   \n",
       "4  sts works microbio lab pt smelled paint thinner became dizzy lightheaded dx inhalant exposure w dizziness   \n",
       "5                                                          l shoulder pain lifting pt work x shoulder strain   \n",
       "6                                                 pain l eye working piece wood went eye dx corneal abrasion   \n",
       "7                                                                         dx forearm laceration p cut x work   \n",
       "8                                               c lbp lifting heavy equipment engine block work dx acute lbp   \n",
       "9                      work tonight pt scratched pt way hospital dx superficial fingernail scratches forearm   \n",
       "\n",
       "   true_event  preds_encode  preds_event  \n",
       "0          62            29           62  \n",
       "1          99            27           60  \n",
       "2          62            29           62  \n",
       "3          62            29           62  \n",
       "4          55            25           55  \n",
       "5          71            37           71  \n",
       "6          66            33           66  \n",
       "7          62            27           60  \n",
       "8          71            37           71  \n",
       "9          12             0           11  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "x = X_valid_processed.tolist()\n",
    "y = y_valid.tolist()\n",
    "\n",
    "def _batch_predict(model, encoder,model_name, max_len,x,y) :\n",
    "    tkzr = AutoTokenizer.from_pretrained(model_name)\n",
    "    encodings_x =  tkzr(x, max_length=MAX_LEN, truncation=True, padding='max_length',return_tensors='tf')\n",
    "    tfdataset = construct_tfdataset(encodings_x).batch(32)\n",
    "    preds = loaded_model.predict(tfdataset)\n",
    "    predictions_encode = pd.DataFrame(data=preds).apply(lambda x: np.argmax(x),axis=1)\n",
    "    categories = encoder.classes_.tolist()\n",
    "    predictions_event= predictions_encode.apply(lambda x:categories[x])\n",
    "\n",
    "\n",
    "    print(len(predictions_event))\n",
    "    results = pd.DataFrame({'text': x,\n",
    "                      'true_event':y,\n",
    "                      'preds_encode': predictions_encode,\n",
    "                      'preds_event':predictions_event\n",
    "                            })\n",
    "\n",
    "\n",
    "    return results\n",
    "\n",
    "results = _batch_predict(loaded_model,encoder,model_name,MAX_LEN,x,y)\n",
    "results.to_csv('solution.csv')\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8526144852224748,\n",
       " 'balanced_accuracy': 0.6106155926140897,\n",
       " 'f1': 0.6171119378801808,\n",
       " 'precision': 0.6472400138296469,\n",
       " 'recall': 0.6106155926140897}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score,balanced_accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.true_event\n",
    "    preds = pred.preds_event\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    bal_acc = balanced_accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels,preds,average='macro')\n",
    "    recall = recall_score(labels,preds,average='macro')\n",
    "    f1 = f1_score(labels,preds,average='macro')\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'balanced_accuracy':bal_acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "compute_metrics(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75687\n",
      "CPU times: user 2h 20min 12s, sys: 1min 34s, total: 2h 21min 47s\n",
      "Wall time: 22min 11s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>true_event</th>\n",
       "      <th>preds_encode</th>\n",
       "      <th>preds_event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f puncture wound fiinger attaching cap insulin syring used home care patient</td>\n",
       "      <td>55</td>\n",
       "      <td>25</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>contusion lt lower leg p mvc hit car guiding car gas pedal got stuck pushing door work yesterday</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pt works quarry attempting dislodge large rock developed chest pains dx chest wall pain</td>\n",
       "      <td>71</td>\n",
       "      <td>37</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>walking work twisted lt ankle later right knee dx left ankle knee sprain</td>\n",
       "      <td>73</td>\n",
       "      <td>39</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c low back pain lifting box work today dx left sided low back pain</td>\n",
       "      <td>71</td>\n",
       "      <td>37</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                               text  \\\n",
       "0                      f puncture wound fiinger attaching cap insulin syring used home care patient   \n",
       "1  contusion lt lower leg p mvc hit car guiding car gas pedal got stuck pushing door work yesterday   \n",
       "2           pt works quarry attempting dislodge large rock developed chest pains dx chest wall pain   \n",
       "3                          walking work twisted lt ankle later right knee dx left ankle knee sprain   \n",
       "4                                c low back pain lifting box work today dx left sided low back pain   \n",
       "\n",
       "   true_event  preds_encode  preds_event  \n",
       "0          55            25           55  \n",
       "1          24             9           26  \n",
       "2          71            37           71  \n",
       "3          73            39           73  \n",
       "4          71            37           71  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "MAX_LEN = 45\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "x_test = X_test_processed.tolist()\n",
    "y_test = y_test.tolist()\n",
    "\n",
    "results_test = _batch_predict(loaded_model,encoder,model_name,MAX_LEN,x_test,y_test)\n",
    "results_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>true_event</th>\n",
       "      <th>preds_encode</th>\n",
       "      <th>preds_event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f puncture wound fiinger attaching cap insulin syring used home care patient</td>\n",
       "      <td>55</td>\n",
       "      <td>25</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>contusion lt lower leg p mvc hit car guiding car gas pedal got stuck pushing door work yesterday</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pt works quarry attempting dislodge large rock developed chest pains dx chest wall pain</td>\n",
       "      <td>71</td>\n",
       "      <td>37</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>walking work twisted lt ankle later right knee dx left ankle knee sprain</td>\n",
       "      <td>73</td>\n",
       "      <td>39</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c low back pain lifting box work today dx left sided low back pain</td>\n",
       "      <td>71</td>\n",
       "      <td>37</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                               text  \\\n",
       "0                      f puncture wound fiinger attaching cap insulin syring used home care patient   \n",
       "1  contusion lt lower leg p mvc hit car guiding car gas pedal got stuck pushing door work yesterday   \n",
       "2           pt works quarry attempting dislodge large rock developed chest pains dx chest wall pain   \n",
       "3                          walking work twisted lt ankle later right knee dx left ankle knee sprain   \n",
       "4                                c low back pain lifting box work today dx left sided low back pain   \n",
       "\n",
       "   true_event  preds_encode  preds_event  \n",
       "0          55            25           55  \n",
       "1          24             9           26  \n",
       "2          71            37           71  \n",
       "3          73            39           73  \n",
       "4          71            37           71  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8556687409991148,\n",
       " 'balanced_accuracy': 0.5998814422117884,\n",
       " 'f1': 0.6066925496506665,\n",
       " 'precision': 0.6256779684299058,\n",
       " 'recall': 0.5998814422117884}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(results_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/engine/functional.py:585: UserWarning: Input dict contained keys ['token_type_ids'] which did not match any model input. They will be ignored by the model.\n",
      "  [n for n in tensors.keys() if n not in ref_input_names])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962/962 [==============================] - 298s 309ms/step - loss: 0.5342 - acc: 0.8543\n",
      "Test loss:  0.5342479348182678\n",
      "Test accuracy:  0.8543398976325989\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "encodings_x_test =  tokenizer(x_test, max_length=MAX_LEN, truncation=True, padding='max_length',return_tensors='tf')\n",
    "y_test_encode = np.asarray(encoder.transform(y_test))\n",
    "tfdataset_test = construct_tfdataset(encodings_x_test,y_test_encode).batch(16)\n",
    "\n",
    "print(\"Evaluating Test data...\")\n",
    "test_score = loaded_model.evaluate(tfdataset_test, steps = validation_steps,batch_size=16)\n",
    "print(\"Test loss: \", test_score[0])\n",
    "print(\"Test accuracy: \", test_score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
