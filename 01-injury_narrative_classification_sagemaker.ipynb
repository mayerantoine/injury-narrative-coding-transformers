{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Injury classification with SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://wandb.ai/jack-morris/david-vs-goliath/reports/Does-Model-Size-Matter-A-Comparison-of-BERT-and-DistilBERT--VmlldzoxMDUxNzU\n",
    "#https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a\n",
    "#https://towardsdatascience.com/scale-neural-network-training-with-sagemaker-distributed-8cf3aefcff51\n",
    "#https://towardsdatascience.com/how-to-reduce-training-time-for-a-deep-learning-model-using-tf-data-43e1989d2961\n",
    "#https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
    "#https://towardsdatascience.com/to-distil-or-not-to-distil-bert-roberta-and-xlnet-c777ad92f8\n",
    "#https://ymeadows.com/articles/fine-tuning-transformer-based-language-models\n",
    "\n",
    "### Smart Batching\n",
    "#https://towardsdatascience.com/divide-hugging-face-transformers-training-time-by-2-or-more-21bf7129db9q-21bf7129db9e\n",
    "#https://www.youtube.com/watch?v=ynOZUNnbEWU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.10\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install tensorflow\n",
    "!pip install transformers\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.5\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import nltk\n",
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import activations, optimizers, losses\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import joblib \n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cdc-cdh-sagemaker-s3fs-dev'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket = 'cdc-cdh-sagemaker-s3fs-dev'\n",
    "sagemaker_session = sagemaker.Session(default_bucket=bucket)\n",
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "default_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n"
     ]
    }
   ],
   "source": [
    "%store -z\n",
    "%store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Import and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(ratio,train):\n",
    "    great_than2_classes = train['event'].value_counts()[train['event'].value_counts() >2].index\n",
    "    train = train[train['event'].isin(great_than2_classes.to_list())]\n",
    "    train_samples, _ = train_test_split(train,train_size=ratio,random_state=42,stratify=train['event'])\n",
    "    print(\"nb classes in samples\",train_samples['event'].nunique())\n",
    "    print(\"nb oservations:\",train_samples.shape)\n",
    "    #print(train_samples['event'].value_counts())\n",
    "\n",
    "    return train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(is_sample=None,ratio=None,is_test_split=False):\n",
    "  \n",
    "    train = pd.read_csv('train.csv')\n",
    "    if is_sample:\n",
    "        train = get_samples(ratio = ratio, train=train)\n",
    "    great_than2_classes = train['event'].value_counts()[train['event'].value_counts() >5].index \n",
    "    train_filter = train[train['event'].isin(great_than2_classes.to_list())]\n",
    "    \n",
    "    print(\"nb classes in final data:\",train_filter['event'].nunique())\n",
    "\n",
    "    X = train_filter['text']\n",
    "    y = train_filter['event']\n",
    "\n",
    "    print(f\"X.shape {X.shape} y.shape : {y.shape}\")\n",
    "\n",
    "    X_train_valid,X_test,y_train_valid, y_test = train_test_split(X,y,train_size=0.9,random_state=42,stratify=y)\n",
    "    \n",
    "    if is_test_split:\n",
    "        X_train,X_valid,y_train,y_valid = train_test_split(X_train_valid,y_train_valid,train_size=0.8,random_state=42,stratify=y_train_valid)\n",
    "\n",
    "    if is_test_split :\n",
    "        print(f\"X_train shape {X_train.shape} y_train shape : {y_train.shape}\")\n",
    "        print(f\"X_valid shape {X_valid.shape} y_valid shape : {y_valid.shape}\")\n",
    "        print(f\"X_test shape {X_test.shape} y_test shape : {y_test.shape}\")\n",
    "        \n",
    "        return {\n",
    "          'train': (X_train,y_train),\n",
    "          'valid': (X_valid,y_valid),\n",
    "          'test': (X_test,y_test)\n",
    "      }\n",
    "\n",
    "    else:\n",
    "        print(f\"X_train shape {X_train_valid.shape} y_train shape : {y_train_valid.shape}\")\n",
    "        print(f\"X_valid shape {X_test.shape} y_valid shape : {y_test.shape}\")\n",
    "        \n",
    "        return {\n",
    "          'train': (X_train_valid,y_train_valid),\n",
    "          'valid': (X_test,y_test),\n",
    "      }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb classes in final data: 43\n",
      "X.shape (153944,) y.shape : (153944,)\n",
      "X_train shape (138549,) y_train shape : (138549,)\n",
      "X_valid shape (15395,) y_valid shape : (15395,)\n"
     ]
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 3100\n",
    "data = get_data(is_sample=False,ratio=1)\n",
    "#data = get_data(is_test_split=True)\n",
    "X_train, y_train = data['train']\n",
    "X_valid,y_valid = data['valid']\n",
    "#X_test,y_test = data['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes in train : 43\n",
      "classes in valid : 43\n",
      "[62, 13, 71, 42, 60, 64, 55, 70, 31, 43, 63, 73, 24, 50, 53, 41, 66, 26, 78, 11, 99, 12, 65, 72, 23, 27, 44, 52, 69, 22, 51, 25, 32, 21, 40, 61, 56, 54, 49, 79, 67, 45, 20]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('classes in train :',len(np.unique(y_train)))\n",
    "print('classes in valid :',len(np.unique(y_valid)))\n",
    "\n",
    "CLASSES = y_train.unique().tolist()\n",
    "print(CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Applies some pre-processing on the given text.\n",
    "\n",
    "    Steps :\n",
    "    - Removing HTML tags\n",
    "    - Removing punctuation\n",
    "    - Lowering text\n",
    "    \"\"\"\n",
    "\n",
    "    # remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    # remove the characters [\\], ['] and [\"]\n",
    "    text = re.sub(r\"\\\\\", \"\", text)\n",
    "    text = re.sub(r\"\\'\", \"\", text)\n",
    "    text = re.sub(r\"\\\"\", \"\", text)\n",
    "\n",
    "    # convert text to lowercase\n",
    "    text = text.strip().lower()\n",
    "\n",
    "    # remove all non-ASCII characters:\n",
    "    text = re.sub(r'[^\\x00-\\x7f]', r'', text)\n",
    "\n",
    "    # replace punctuation characters with spaces\n",
    "    filters = '!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    translate_dict = dict((c, \" \") for c in filters)\n",
    "    translate_map = str.maketrans(translate_dict)\n",
    "    text = text.translate(translate_map)\n",
    "    text = \" \".join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_useless_words(text,useless_words):\n",
    "    sentence = [word for word in word_tokenize(text)]\n",
    "    sentence_stop = [word for word in sentence if word not in useless_words]\n",
    "\n",
    "    text = \" \".join(sentence_stop)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    \"\"\" Preprocess : cleaning and remove stop words\"\"\"\n",
    "\n",
    "    X = X.apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "    X = X.apply(lambda x: clean_text(x))\n",
    "\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    useless_words = stopwords + list(string.punctuation) + ['yom', 'yof', 'yowm', 'yf', 'ym', 'yo']\n",
    "    # print(\"useless word : \",useless_words)\n",
    "    X = X.apply(lambda x: remove_useless_words(x,useless_words))\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after preprocessing...\n",
      "69539                     work hammering metal pole hit finger dx r index finger pain communited intraarticular fx pip\n",
      "76664     work pet groomer grooming dog another dog jumped upand dog bit pt r cheek abrasions jawline dx dog bite face\n",
      "71708                                                                                     hit chisel eyebrown lac face\n",
      "3165                      c upper arm shoulder pain w movement since yest think strained work lifting dx shoulder pain\n",
      "136917                                                           fracture pubic rami semi load pvc pipe fell unloading\n",
      "Name: text, dtype: object\n",
      "CPU times: user 24.9 s, sys: 45.5 ms, total: 25 s\n",
      "Wall time: 25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_processed = preprocess_data(X_train)\n",
    "X_valid_processed = preprocess_data(X_valid)\n",
    "\n",
    "print(\"after preprocessing...\")\n",
    "print(X_train_processed.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and Encode Train and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.4 s, sys: 1.73 s, total: 18.1 s\n",
      "Wall time: 6.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import  DistilBertTokenizerFast,BertTokenizerFast,RobertaTokenizerFast\n",
    "MAX_LEN = 45\n",
    "\n",
    "#MODEL_NAME = 'distilbert-base-uncased'\n",
    "#tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "x_train = X_train_processed.to_list()\n",
    "x_valid = X_valid_processed.to_list()\n",
    "\n",
    "\n",
    "train_encodings = tokenizer(x_train, max_length=MAX_LEN, truncation=True, padding='max_length',return_tensors='tf')\n",
    "valid_encodings = tokenizer(x_valid, max_length=MAX_LEN, truncation=True, padding='max_length',return_tensors='tf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[  101  2147 27883  3384  6536  2718  4344  1040  2595  1054  5950  4344\n",
      "  3255  4012 23041 17572 26721  8445 21412 23292 28315   102     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0], shape=(45,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0], shape=(45,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(train_encodings['input_ids'][0])\n",
    "print(train_encodings['attention_mask'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Labels .....\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"Encoding Labels .....\")\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "y_train_encode = np.asarray(encoder.transform(y_train))\n",
    "y_valid_encode = np.asarray(encoder.transform(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = { id:str(label) for id, label in enumerate(encoder.classes_)}\n",
    "label2id = { str(label):id for id, label in enumerate(encoder.classes_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes in train : 43\n",
      "classes in valid : 43\n"
     ]
    }
   ],
   "source": [
    "print('classes in train :',len(np.unique(y_train_encode)))\n",
    "print('classes in valid :',len(np.unique(y_valid_encode)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_path='./data'\n",
    "os.makedirs(data_path,exist_ok=True)\n",
    "\n",
    "from pickle import dump\n",
    "dump(encoder,open(os.path.join(data_path,'encode.pkl'),'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TF Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.81 ms, sys: 0 ns, total: 3.81 ms\n",
      "Wall time: 3.67 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def construct_tfdataset(encodings, y=None):\n",
    "    if y is not None:\n",
    "        return tf.data.Dataset.from_tensor_slices((dict(encodings),y))\n",
    "    else:\n",
    "        # this case is used when making predictions on unseen samples after training\n",
    "        return tf.data.Dataset.from_tensor_slices(dict(encodings))\n",
    "    \n",
    "train_tfdataset = construct_tfdataset(train_encodings, y_train_encode)\n",
    "valid_tfdataset = construct_tfdataset(valid_encodings, y_valid_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TF Records File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _save_feature_as_tfrecord(tfdataset,file_path):\n",
    "    \"\"\" Helper function to save the tf dataset as tfrecords\"\"\"\n",
    "    \n",
    "\n",
    "    def single_example_data(encoding,label_id):\n",
    "\n",
    "        input_ids =encoding['input_ids']\n",
    "        attention_mask = encoding['attention_mask']\n",
    "\n",
    "        tfrecord_features = collections.OrderedDict()\n",
    "\n",
    "        def _int64_feature(value):\n",
    "            \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "            return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "        tfrecord_features['input_ids'] = _int64_feature(input_ids)\n",
    "        tfrecord_features['attention_mask'] = _int64_feature(attention_mask)\n",
    "        tfrecord_features['label_ids'] =  _int64_feature([label_id])\n",
    "\n",
    "        _example = tf.train.Example(features=tf.train.Features(feature=tfrecord_features))\n",
    "\n",
    "        return _example.SerializeToString()\n",
    "\n",
    "    def data_generator():\n",
    "        for features in tfdataset:\n",
    "            yield single_example_data(*features)\n",
    "\n",
    "    serialized_tfdataset = tf.data.Dataset.from_generator(\n",
    "        data_generator, output_types=tf.string, output_shapes=())        \n",
    "\n",
    "    writer = tf.data.experimental.TFRecordWriter(file_path)\n",
    "    writer.write(serialized_tfdataset)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "# training data\n",
    "_save_feature_as_tfrecord(train_tfdataset,'./data/train.tfrecord')\n",
    "\n",
    "#validation data\n",
    "_save_feature_as_tfrecord(valid_tfdataset,'./data/valid.tfrecord')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://cdc-cdh-sagemaker-s3fs-dev/projects/project006/injury-data'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = 'projects/project006/injury-data'\n",
    "tfrecord_data_location = sagemaker_session.upload_data(path = './data',\n",
    "                                                      bucket = bucket,\n",
    "                                                      key_prefix = prefix)\n",
    "tfrecord_data_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Run DistilBERT Training Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training output path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://cdc-cdh-sagemaker-s3fs-dev/projects/project006/output/'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = 'projects/project006/output'\n",
    "output_path = 's3://{}/{}/'.format(bucket, prefix )\n",
    "output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m  TFDistilBertForSequenceClassification,DistilBertConfig\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TFBertForSequenceClassification, BertConfig\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TFRobertaForSequenceClassification, RobertaConfig\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptimizers\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mschedules\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m PolynomialDecay\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptimization_tf\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m WarmUp, AdamWeightDecay\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m load\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_parse_args\u001b[39;49;00m():\n",
      "    parser = argparse.ArgumentParser()\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output-data-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--sm-model-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,default=os.environ.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,default=os.environ.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    \n",
      "    \u001b[37m## Hyperparams\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,default=\u001b[33m'\u001b[39;49;00m\u001b[33mDistilBERT\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--num_records\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--optimizer\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,default=\u001b[33m'\u001b[39;49;00m\u001b[33madam\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--max_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,default=\u001b[34m45\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,default=\u001b[34m5e-5\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,default=\u001b[34m1\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,default=\u001b[34m32\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--valid_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,default=\u001b[34m8\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--steps_per_epoch\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--validation_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m  parser.parse_known_args()\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mDistilBERT\u001b[39;49;00m:\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, params):\n",
      "        \u001b[36mself\u001b[39;49;00m.num_labels = params[\u001b[33m'\u001b[39;49;00m\u001b[33mnum_labels\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.max_len = params[\u001b[33m'\u001b[39;49;00m\u001b[33mmax_len\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.learning_rate = params[\u001b[33m'\u001b[39;49;00m\u001b[33mlearning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.id2label = params[\u001b[33m'\u001b[39;49;00m\u001b[33mid2label\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.label2id = params[\u001b[33m'\u001b[39;49;00m\u001b[33mlabel2id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.batch_size = params[\u001b[33m'\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.epochs = params[\u001b[33m'\u001b[39;49;00m\u001b[33mepochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.num_records = params[\u001b[33m'\u001b[39;49;00m\u001b[33mnum_records\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mbuild\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\n",
      "        \n",
      "        freeze_bert_layer = \u001b[34mFalse\u001b[39;49;00m\n",
      "        \n",
      "        \n",
      "        transformer_config = DistilBertConfig(num_labels=\u001b[36mself\u001b[39;49;00m.num_labels,\n",
      "                                   id2label = \u001b[36mself\u001b[39;49;00m.id2label,\n",
      "                                   label2id  = \u001b[36mself\u001b[39;49;00m.label2id,\n",
      "                                   output_attentions = \u001b[34mFalse\u001b[39;49;00m,\n",
      "                                   output_hidden_states= \u001b[34mFalse\u001b[39;49;00m)\n",
      "                                   \n",
      "        transformer_model = TFDistilBertForSequenceClassification.from_pretrained(\u001b[33m\"\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,config=transformer_config)\n",
      "        input_ids = tf.keras.Input(shape = (\u001b[36mself\u001b[39;49;00m.max_len,),name=\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,dtype=\u001b[33m'\u001b[39;49;00m\u001b[33mint32\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        attention_mask = tf.keras.Input(shape = (\u001b[36mself\u001b[39;49;00m.max_len,),name=\u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,dtype=\u001b[33m'\u001b[39;49;00m\u001b[33mint32\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \n",
      "        embbeding_layer = transformer_model.distilbert(input_ids,attention_mask=attention_mask)[\u001b[34m0\u001b[39;49;00m]\n",
      "        X = tf.keras.layers.Conv1D(\u001b[34m512\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(embbeding_layer)\n",
      "        X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
      "        X = tf.keras.layers.Dense(\u001b[34m256\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(X)\n",
      "        X = tf.keras.layers.Dropout(\u001b[34m0.2\u001b[39;49;00m)(X)\n",
      "        X = tf.keras.layers.Dense(\u001b[36mself\u001b[39;49;00m.num_labels, activation=\u001b[33m'\u001b[39;49;00m\u001b[33msoftmax\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(X)\n",
      "        \n",
      "        model = tf.keras.Model(inputs=[input_ids,attention_mask], outputs = X)\n",
      "        \n",
      "        \u001b[34mfor\u001b[39;49;00m layer \u001b[35min\u001b[39;49;00m model.layers[:\u001b[34m3\u001b[39;49;00m]:\n",
      "            layer.trainable = \u001b[35mnot\u001b[39;49;00m freeze_bert_layer\n",
      "            \n",
      "        num_train_steps = (\u001b[36mself\u001b[39;49;00m.num_records // \u001b[36mself\u001b[39;49;00m.batch_size) * \u001b[36mself\u001b[39;49;00m.epochs\n",
      "        decay_schedule_fn = PolynomialDecay(\n",
      "            initial_learning_rate=\u001b[36mself\u001b[39;49;00m.learning_rate,\n",
      "            end_learning_rate=\u001b[34m0.\u001b[39;49;00m,\n",
      "            decay_steps=num_train_steps\n",
      "            )\n",
      "        \n",
      "        warmup_steps = num_train_steps * \u001b[34m0.1\u001b[39;49;00m\n",
      "        warmup_schedule  = WarmUp(\u001b[36mself\u001b[39;49;00m.learning_rate,decay_schedule_fn,warmup_steps)\n",
      "        \n",
      "        \u001b[37m# fine optimizer and loss\u001b[39;49;00m\n",
      "        \u001b[37m#optimizer = tf.keras.optimizers.Adam(learning_rate=lr_scheduler)\u001b[39;49;00m\n",
      "        optimizer = AdamWeightDecay (learning_rate=warmup_schedule, weight_decay_rate=\u001b[34m0.01\u001b[39;49;00m, epsilon=\u001b[34m1e-6\u001b[39;49;00m)\n",
      "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits= \u001b[34mFalse\u001b[39;49;00m)\n",
      "        metrics = [\u001b[33m'\u001b[39;49;00m\u001b[33macc\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
      "\n",
      "        \u001b[34mreturn\u001b[39;49;00m model\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mBaseBERT\u001b[39;49;00m:\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, params):\n",
      "        \u001b[36mself\u001b[39;49;00m.num_labels = params[\u001b[33m'\u001b[39;49;00m\u001b[33mnum_labels\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.max_len = params[\u001b[33m'\u001b[39;49;00m\u001b[33mmax_len\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.learning_rate = params[\u001b[33m'\u001b[39;49;00m\u001b[33mlearning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.id2label = params[\u001b[33m'\u001b[39;49;00m\u001b[33mid2label\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.label2id = params[\u001b[33m'\u001b[39;49;00m\u001b[33mlabel2id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.batch_size = params[\u001b[33m'\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.epochs = params[\u001b[33m'\u001b[39;49;00m\u001b[33mepochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.num_records = params[\u001b[33m'\u001b[39;49;00m\u001b[33mnum_records\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mbuild\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):      \n",
      "        \n",
      "        freeze_bert_layer = \u001b[34mFalse\u001b[39;49;00m\n",
      "        \n",
      "        transformer_model = TFBertForSequenceClassification.from_pretrained(\u001b[33m\"\u001b[39;49;00m\u001b[33mbert-base-uncased\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,num_labels=\u001b[36mself\u001b[39;49;00m.num_labels, \n",
      "                 output_attentions = \u001b[34mFalse\u001b[39;49;00m,\n",
      "                output_hidden_states= \u001b[34mFalse\u001b[39;49;00m )\n",
      "        input_ids = tf.keras.Input(shape = (\u001b[36mself\u001b[39;49;00m.max_len,),name=\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,dtype=\u001b[33m'\u001b[39;49;00m\u001b[33mint32\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        attention_mask = tf.keras.Input(shape = (\u001b[36mself\u001b[39;49;00m.max_len,),name=\u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,dtype=\u001b[33m'\u001b[39;49;00m\u001b[33mint32\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \n",
      "        embbeding_layer = transformer_model.bert(input_ids,attention_mask=attention_mask)[\u001b[34m0\u001b[39;49;00m]\n",
      "        X = tf.keras.layers.Conv1D(\u001b[34m512\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(embbeding_layer)\n",
      "        X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
      "        X = tf.keras.layers.Dense(\u001b[34m256\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(X)\n",
      "        X = tf.keras.layers.Dropout(\u001b[34m0.2\u001b[39;49;00m)(X)\n",
      "        X = tf.keras.layers.Dense(\u001b[36mself\u001b[39;49;00m.num_labels, activation=\u001b[33m'\u001b[39;49;00m\u001b[33msoftmax\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(X)\n",
      "\n",
      "        model = tf.keras.Model(inputs=[input_ids,attention_mask], outputs = X)\n",
      "        \n",
      "        \u001b[34mfor\u001b[39;49;00m layer \u001b[35min\u001b[39;49;00m model.layers[:\u001b[34m3\u001b[39;49;00m]:\n",
      "            layer.trainable = \u001b[35mnot\u001b[39;49;00m freeze_bert_layer\n",
      "        \n",
      "        num_train_steps = (\u001b[36mself\u001b[39;49;00m.num_records // \u001b[36mself\u001b[39;49;00m.batch_size) * \u001b[36mself\u001b[39;49;00m.epochs\n",
      "        decay_schedule_fn = PolynomialDecay(\n",
      "            initial_learning_rate=\u001b[36mself\u001b[39;49;00m.learning_rate,\n",
      "            end_learning_rate=\u001b[34m0.\u001b[39;49;00m,\n",
      "            decay_steps=num_train_steps\n",
      "            )\n",
      "        \n",
      "        warmup_steps = num_train_steps * \u001b[34m0.1\u001b[39;49;00m\n",
      "        warmup_schedule  = WarmUp(\u001b[36mself\u001b[39;49;00m.learning_rate,decay_schedule_fn,warmup_steps)\n",
      "        \n",
      "        \u001b[37m#optimizer = tf.keras.optimizers.Adam(learning_rate=lr_scheduler)\u001b[39;49;00m\n",
      "        optimizer = AdamWeightDecay (learning_rate=warmup_schedule, weight_decay_rate=\u001b[34m0.01\u001b[39;49;00m, epsilon=\u001b[34m1e-6\u001b[39;49;00m)\n",
      "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    \n",
      "        model.compile(optimizer=optimizer, loss=loss, metrics=[\u001b[33m'\u001b[39;49;00m\u001b[33macc\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "        \n",
      "        \u001b[34mreturn\u001b[39;49;00m model\n",
      "    \n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mBaseRoberta\u001b[39;49;00m:\n",
      "    \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, params):\n",
      "        \u001b[36mself\u001b[39;49;00m.num_labels = params[\u001b[33m'\u001b[39;49;00m\u001b[33mnum_labels\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.max_len = params[\u001b[33m'\u001b[39;49;00m\u001b[33mmax_len\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.learning_rate = params[\u001b[33m'\u001b[39;49;00m\u001b[33mlearning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.id2label = params[\u001b[33m'\u001b[39;49;00m\u001b[33mid2label\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.label2id = params[\u001b[33m'\u001b[39;49;00m\u001b[33mlabel2id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.batch_size = params[\u001b[33m'\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.epochs = params[\u001b[33m'\u001b[39;49;00m\u001b[33mepochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[36mself\u001b[39;49;00m.num_records = params[\u001b[33m'\u001b[39;49;00m\u001b[33mnum_records\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mbuild\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):      \n",
      "        \n",
      "        freeze_bert_layer = \u001b[34mFalse\u001b[39;49;00m\n",
      "        \n",
      "        transformer_model = TFRobertaForSequenceClassification.from_pretrained(\u001b[33m\"\u001b[39;49;00m\u001b[33mroberta-base\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, num_labels=\u001b[36mself\u001b[39;49;00m.num_labels, \n",
      "                 output_attentions = \u001b[34mFalse\u001b[39;49;00m,\n",
      "                output_hidden_states= \u001b[34mFalse\u001b[39;49;00m )\n",
      "        input_ids = tf.keras.Input(shape = (\u001b[36mself\u001b[39;49;00m.max_len,),name=\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,dtype=\u001b[33m'\u001b[39;49;00m\u001b[33mint32\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        attention_mask = tf.keras.Input(shape = (\u001b[36mself\u001b[39;49;00m.max_len,),name=\u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,dtype=\u001b[33m'\u001b[39;49;00m\u001b[33mint32\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \n",
      "        embbeding_layer = transformer_model.roberta(input_ids,attention_mask=attention_mask)[\u001b[34m0\u001b[39;49;00m]\n",
      "        X = tf.keras.layers.GlobalMaxPool1D()(embbeding_layer)\n",
      "        X = tf.keras.layers.Dense(\u001b[36mself\u001b[39;49;00m.num_labels, activation=\u001b[33m'\u001b[39;49;00m\u001b[33msoftmax\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(X)\n",
      "        \n",
      "        model = tf.keras.Model(inputs=[input_ids,attention_mask], outputs = X)\n",
      "        \n",
      "        \u001b[34mfor\u001b[39;49;00m layer \u001b[35min\u001b[39;49;00m model.layers[:\u001b[34m3\u001b[39;49;00m]:\n",
      "            layer.trainable = \u001b[35mnot\u001b[39;49;00m freeze_bert_layer\n",
      "        \n",
      "        num_train_steps = (\u001b[36mself\u001b[39;49;00m.num_records // \u001b[36mself\u001b[39;49;00m.batch_size) * \u001b[36mself\u001b[39;49;00m.epochs\n",
      "        decay_schedule_fn = PolynomialDecay(\n",
      "            initial_learning_rate=\u001b[36mself\u001b[39;49;00m.learning_rate,\n",
      "            end_learning_rate=\u001b[34m0.\u001b[39;49;00m,\n",
      "            decay_steps=num_train_steps\n",
      "            )\n",
      "        \n",
      "        warmup_steps = num_train_steps * \u001b[34m0.1\u001b[39;49;00m\n",
      "        warmup_schedule  = WarmUp(\u001b[36mself\u001b[39;49;00m.learning_rate,decay_schedule_fn,warmup_steps)\n",
      "        \n",
      "        \u001b[37m#optimizer = tf.keras.optimizers.Adam(learning_rate=lr_scheduler)\u001b[39;49;00m\n",
      "        optimizer = AdamWeightDecay (learning_rate=warmup_schedule, weight_decay_rate=\u001b[34m0.01\u001b[39;49;00m, epsilon=\u001b[34m1e-6\u001b[39;49;00m)\n",
      "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=\u001b[34mFalse\u001b[39;49;00m)\n",
      "        \n",
      "        model.compile(optimizer=optimizer, loss=loss, metrics=[\u001b[33m'\u001b[39;49;00m\u001b[33macc\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "        \n",
      "        \u001b[34mreturn\u001b[39;49;00m model\n",
      "        \n",
      "        \n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_load_data\u001b[39;49;00m(train_dir,MAX_LEN,epochs,batch_size,valid_batch_size,steps_per_epoch,validation_steps):\n",
      "          \n",
      "    train_file = os.path.join(train_dir,\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain.tfrecord\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \n",
      "    valid_file = os.path.join(train_dir,\u001b[33m\"\u001b[39;49;00m\u001b[33mvalid.tfrecord\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# Create a description of the features.\u001b[39;49;00m\n",
      "    feature_description = {\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: tf.io.FixedLenFeature([MAX_LEN], tf.int64),\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: tf.io.FixedLenFeature([MAX_LEN], tf.int64),\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: tf.io.FixedLenFeature([], tf.int64),\n",
      "    }\n",
      "        \n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m_parse_function\u001b[39;49;00m(example_proto):\n",
      "\n",
      "        \u001b[37m# Parse the input `tf.train.Example` proto using the dictionary above.\u001b[39;49;00m\n",
      "        parsed  = tf.io.parse_single_example(example_proto, feature_description)\n",
      "\n",
      "        \u001b[34mreturn\u001b[39;49;00m {\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:parsed[\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:parsed[\u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]},parsed[\u001b[33m'\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \n",
      "    \n",
      "    train_dataset = tf.data.TFRecordDataset(train_file)\n",
      "    train_dataset = train_dataset.repeat(epochs * steps_per_epoch)\n",
      "    train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
      "    train_dataset = train_dataset.map(_parse_function,num_parallel_calls=tf.data.AUTOTUNE)\n",
      "    train_dataset = train_dataset.batch(batch_size)\n",
      "    \n",
      "    train_dataset.cache()\n",
      "    \n",
      "    valid_dataset = tf.data.TFRecordDataset(valid_file)\n",
      "    valid_dataset = valid_dataset.repeat(epochs * validation_steps)\n",
      "    valid_dataset = valid_dataset.prefetch(tf.data.AUTOTUNE)\n",
      "    valid_dataset = valid_dataset.map(_parse_function,num_parallel_calls=tf.data.AUTOTUNE)\n",
      "    valid_dataset = valid_dataset.batch(valid_batch_size)\n",
      "    \n",
      "    valid_dataset.cache()\n",
      "   \n",
      "    \u001b[34mreturn\u001b[39;49;00m train_dataset, valid_dataset\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_load_encoder\u001b[39;49;00m(train_dir):\n",
      "    \u001b[37m# load the encoder\u001b[39;49;00m\n",
      "    encoder_file = os.path.join(train_dir,\u001b[33m'\u001b[39;49;00m\u001b[33mencode.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \n",
      "    encoder = load(\u001b[36mopen\u001b[39;49;00m(encoder_file, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m encoder\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m():\n",
      "    args, unknown = _parse_args()\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33minput train: \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,args.train)\n",
      "\n",
      "    train_dataset, valid_dataset = _load_data(args.train,\n",
      "                                              args.max_len,\n",
      "                                              args.epochs,\n",
      "                                              args.batch_size,\n",
      "                                              args.valid_batch_size,\n",
      "                                              args.steps_per_epoch,args.validation_steps)    \n",
      "    encoder = _load_encoder(args.train)\n",
      "    \n",
      "    CLASSES = encoder.classes_\n",
      "    id2label = { \u001b[36mid\u001b[39;49;00m:\u001b[36mstr\u001b[39;49;00m(label) \u001b[34mfor\u001b[39;49;00m \u001b[36mid\u001b[39;49;00m, label \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(encoder.classes_)}\n",
      "    label2id = { \u001b[36mstr\u001b[39;49;00m(label):\u001b[36mid\u001b[39;49;00m \u001b[34mfor\u001b[39;49;00m \u001b[36mid\u001b[39;49;00m, label \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(encoder.classes_)}\n",
      "    \n",
      "    params = {\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mnum_labels\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[36mlen\u001b[39;49;00m(CLASSES),\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mmax_len\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.max_len,\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mlearning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.learning_rate,\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mid2label\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: id2label,\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mlabel2id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: label2id,\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.batch_size,\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mepochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.epochs,\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mnum_records\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:args.num_records\n",
      "        }\n",
      "    \n",
      "    \u001b[34mif\u001b[39;49;00m args.model_name == \u001b[33m'\u001b[39;49;00m\u001b[33mDistilBERT\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m :\n",
      "        model = DistilBERT(params).build()   \n",
      "    \u001b[34melif\u001b[39;49;00m args.model_name == \u001b[33m'\u001b[39;49;00m\u001b[33mBaseBERT\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "        model = BaseBERT(params).build()\n",
      "    \u001b[34melif\u001b[39;49;00m args.model_name == \u001b[33m'\u001b[39;49;00m\u001b[33mBaseRoberta\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "        model = BaseRoberta(params).build()\n",
      "        \n",
      "    \u001b[36mprint\u001b[39;49;00m(model.summary())\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation steps:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,args.validation_steps)\n",
      "        \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mTraining....\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    history = model.fit(x=train_dataset.shuffle(args.num_records),\n",
      "                        steps_per_epoch = args.steps_per_epoch,\n",
      "                        batch_size=args.batch_size,\n",
      "                        epochs=args.epochs,\n",
      "                        validation_data=valid_dataset,\n",
      "                        validation_steps=args.validation_steps)\n",
      "        \n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33msaving model...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[37m#model.save(os.path.join(args.sm_model_dir,f\"{args.model_name}.h5\"))\u001b[39;49;00m\n",
      "    model.save(os.path.join(args.sm_model_dir,\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{args.model_name}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    \u001b[37m#model.save(f\"./output/model/{args.model_name}/{args.model_name}_test.h5\")\u001b[39;49;00m\n",
      "\n",
      "    \n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    main()\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow 2.3 script\n",
    "!pygmentize './src/train.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records = len(X_train)\n",
    "num_valid_records = len(X_valid)\n",
    "max_len = MAX_LEN\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "valid_batch_size = 16\n",
    "steps_per_epoch = num_records // batch_size\n",
    "validation_steps = num_valid_records // valid_batch_size\n",
    "learning_rate = 5e-5\n",
    "optimizer = 'adam'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Hugging Face Estimator to start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from time import gmtime, strftime\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "model_name = 'DistilBERT'\n",
    "job_name_prefix = f\"training-{model_name}\"\n",
    "timestamp = strftime(\"-%m-%d-%M-%S\", gmtime())\n",
    "\n",
    "job_name = job_name_prefix + timestamp\n",
    "\n",
    "_distillbert_estimator = HuggingFace(\n",
    "        base_job_name  = job_name,\n",
    "        entry_point=\"train.py\",\n",
    "        source_dir = \"./src/\",\n",
    "        role=role,\n",
    "        instance_count=1,\n",
    "        volume_size = 5,\n",
    "        max_run = 18000,\n",
    "        instance_type='ml.p3.2xlarge',\n",
    "        transformers_version = \"4.4\",\n",
    "        tensorflow_version  = \"2.4\",\n",
    "        py_version=\"py37\",\n",
    "        output_path = output_path,\n",
    "        hyperparameters = {\n",
    "                \"model_name\": model_name,\n",
    "                \"num_records\":  num_records,\n",
    "                \"max_len\":max_len,\n",
    "                \"epochs\":int(epochs),\n",
    "                \"learning_rate\":float(learning_rate),\n",
    "                \"batch_size\":int(batch_size),\n",
    "                \"valid_batch_size\":valid_batch_size,\n",
    "                \"steps_per_epoch\": steps_per_epoch,\n",
    "                \"validation_steps\": validation_steps,\n",
    "                \"optimizer\":optimizer\n",
    "                },\n",
    "        metric_definitions = [{'Name':'train:loss','Regex':'loss: ([0-9\\\\.]+)'},\n",
    "                                    {'Name':'train:accuracy','Regex':'acc: ([0-9\\\\.]+)'},\n",
    "                                    {'Name':'validation:loss','Regex':'val_loss: ([0-9\\\\.]+)'},\n",
    "                                    {'Name':'validation:accuracy','Regex':'val_acc: ([0-9\\\\.]+)'}],\n",
    "        enable_sagemaker_metrics = True\n",
    "    )\n",
    "\n",
    "_distillbert_estimator.fit({'train':tfrecord_data_location}, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the trained distilbert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = ''\n",
    "\n",
    "def download_model(sagemaker_session,job_name,estimator = None):\n",
    "    \n",
    "    if job_name is not None :\n",
    "        modeldesc = sagemaker_session.describe_training_job(job_name)\n",
    "        s3_model_path = modeldesc['ModelArtifacts']['S3ModelArtifacts']\n",
    "    elif estimator is not None:\n",
    "        s3_model_path =  _distillbert_estimator.model_data\n",
    "\n",
    "    \n",
    "    os.makedirs(f\"./output/model/{job_name}/\",exist_ok=True)\n",
    "\n",
    "    S3Downloader.download(\n",
    "        s3_uri=s3_model_path, # s3 uri where the trained model is located\n",
    "        local_path=f\"./output/model/{job_name}/\", # local path where *.targ.gz is saved\n",
    "        sagemaker_session=sagemaker_session # sagemaker session used for training the model\n",
    "    )\n",
    "    \n",
    "    return s3_model_path, modeldesc['HyperParameters']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading model\n",
    "s3_model_path, hp = download_model(sagemaker_session,job_name)\n",
    "\n",
    "print(s3_model_path)\n",
    "print(hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract and load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "from transformers.optimization_tf import AdamWeightDecay\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "\n",
    "def extract_data_load_model(job_name,model_name):\n",
    "\n",
    "    t = tarfile.open(f'./output/model/{job_name}/model.tar.gz', 'r:gz')\n",
    "    t.extractall(path=f'./output/model/{job_name}')\n",
    "    _model = tf.keras.models.load_model(f\"./output/model/{job_name}/{model_name}\",custom_objects={'AdamWeightDecay':AdamWeightDecay})\n",
    "    \n",
    "    return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'DistilBERT'\n",
    "loaded_model = extract_data_load_model(job_name,model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_data(train_dir,MAX_LEN,epochs,batch_size,valid_batch_size,steps_per_epoch,validation_steps):\n",
    "    \"\"\" Helper function to load,parse and create input data pipeline from TFRecords\"\"\"\n",
    "          \n",
    "    train_file = os.path.join(train_dir,\"train.tfrecord\") \n",
    "    valid_file = os.path.join(train_dir,\"valid.tfrecord\")\n",
    "    \n",
    "    # Create a description of the features.\n",
    "    feature_description = {\n",
    "        'input_ids': tf.io.FixedLenFeature([MAX_LEN], tf.int64),\n",
    "        'attention_mask': tf.io.FixedLenFeature([MAX_LEN], tf.int64),\n",
    "        'label_ids': tf.io.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "        \n",
    "    def _parse_function(example_proto):\n",
    "\n",
    "        # Parse the input `tf.train.Example` proto using the dictionary above.\n",
    "        parsed  = tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "        return {'input_ids':parsed['input_ids'],'attention_mask':parsed['attention_mask']},parsed['label_ids']\n",
    "        \n",
    "    \n",
    "    train_dataset = tf.data.TFRecordDataset(train_file)\n",
    "    train_dataset = train_dataset.repeat(epochs * steps_per_epoch)\n",
    "    train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    train_dataset = train_dataset.map(_parse_function,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    train_dataset = train_dataset.batch(batch_size)\n",
    "    \n",
    "    \n",
    "    \n",
    "    valid_dataset = tf.data.TFRecordDataset(valid_file)\n",
    "    valid_dataset = valid_dataset.repeat(epochs * validation_steps)\n",
    "    valid_dataset = valid_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    valid_dataset = valid_dataset.map(_parse_function,num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    valid_dataset = valid_dataset.batch(valid_batch_size)\n",
    "    \n",
    "   \n",
    "    return train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset,valid_dataset = _load_data('./data',MAX_LEN,epochs,batch_size,valid_batch_size,steps_per_epoch,validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Training data...\n",
      "8659/8659 [==============================] - 6041s 697ms/step - loss: 0.1104 - acc: 0.9659\n",
      "Training loss:  0.11043110489845276\n",
      "Training accuracy:  0.9659242033958435\n",
      "Evaluating Validation data...\n",
      "962/962 [==============================] - 673s 699ms/step - loss: 0.5459 - acc: 0.8493\n",
      "Validation loss:  0.5459089875221252\n",
      "Validation accuracy:  0.8492723703384399\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def _evaluate_model(loaded_model):\n",
    "    print(\"Evaluating Training data...\")\n",
    "    train_score = loaded_model.evaluate(train_dataset,\n",
    "                                     steps = steps_per_epoch,\n",
    "                                     batch_size=batch_size)\n",
    "\n",
    "    print(\"Training loss: \", train_score[0])\n",
    "    print(\"Training accuracy: \", train_score[1])\n",
    "\n",
    "    print(\"Evaluating Validation data...\")\n",
    "    valid_score = loaded_model.evaluate(valid_dataset, steps = validation_steps,batch_size=valid_batch_size)\n",
    "    print(\"Validation loss: \", valid_score[0])\n",
    "    print(\"Validation accuracy: \", valid_score[1])\n",
    "    \n",
    "_evaluate_model(loaded_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the fine-tuned model to predict new samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "\n",
    "def distilBert_create_predictor(model, encoder,model_name, max_len,text):\n",
    "    tkzr = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "    x = [text]\n",
    "    encodings =  tkzr(x, max_length=max_len, truncation=True, padding='max_length',return_tensors='tf')\n",
    "    tfdataset = construct_tfdataset(encodings)\n",
    "    tfdataset = tfdataset.batch(1)\n",
    "    preds = model.predict(tfdataset)\n",
    "    categories = encoder.classes_.tolist()\n",
    "    enc = np.argmax(preds[0])\n",
    "    \n",
    "    return {     'text' : x,\n",
    "                 'predict_proba' : preds[0][np.argmax(preds[0])],\n",
    "                 'predicted_class' : categories[np.argmax(preds)]                             \n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['sp lt ankle going steps twisted ankle work'],\n",
       " 'predict_proba': 0.9913174,\n",
       " 'predicted_class': 73}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 'sp lt ankle going steps twisted ankle work'\n",
    "distilBert_create_predictor(loaded_model, encoder,MODEL_NAME, MAX_LEN,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Save Predictions and calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1534\n",
      "CPU times: user 1min 39s, sys: 4.6 s, total: 1min 44s\n",
      "Wall time: 16.9 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>true_event</th>\n",
       "      <th>preds_encode</th>\n",
       "      <th>preds_event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r hand pain c r hand pain rest denies injury overuse pushing carts local grocery store work</td>\n",
       "      <td>71</td>\n",
       "      <td>23</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>transferring patient fell landing floor back sprain</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>head cont work slipped threshold door falling backward striking back head denies loc</td>\n",
       "      <td>42</td>\n",
       "      <td>10</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>contusion hand fell work</td>\n",
       "      <td>42</td>\n",
       "      <td>10</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trying flush iv work became clogged sprayed pt face dx exposure bodily fluids</td>\n",
       "      <td>55</td>\n",
       "      <td>16</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dx forearm elbow laceration p sustained laceration working</td>\n",
       "      <td>60</td>\n",
       "      <td>17</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>subject officer c rt nd finger pain altercation w persondx finger strain</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>burn r index finger w hot oil work x nd degree burn</td>\n",
       "      <td>53</td>\n",
       "      <td>15</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>l forearm lac walking w sharp metal place arm sharp edge several minutes moved arm noted gaping laceration</td>\n",
       "      <td>60</td>\n",
       "      <td>19</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>strained lower back work</td>\n",
       "      <td>70</td>\n",
       "      <td>22</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                         text  \\\n",
       "0                 r hand pain c r hand pain rest denies injury overuse pushing carts local grocery store work   \n",
       "1                                                         transferring patient fell landing floor back sprain   \n",
       "2                        head cont work slipped threshold door falling backward striking back head denies loc   \n",
       "3                                                                                    contusion hand fell work   \n",
       "4                               trying flush iv work became clogged sprayed pt face dx exposure bodily fluids   \n",
       "5                                                  dx forearm elbow laceration p sustained laceration working   \n",
       "6                                    subject officer c rt nd finger pain altercation w persondx finger strain   \n",
       "7                                                         burn r index finger w hot oil work x nd degree burn   \n",
       "8  l forearm lac walking w sharp metal place arm sharp edge several minutes moved arm noted gaping laceration   \n",
       "9                                                                                    strained lower back work   \n",
       "\n",
       "   true_event  preds_encode  preds_event  \n",
       "0          71            23           71  \n",
       "1          12            10           42  \n",
       "2          42            10           42  \n",
       "3          42            10           42  \n",
       "4          55            16           55  \n",
       "5          60            17           60  \n",
       "6          11             0           11  \n",
       "7          53            15           53  \n",
       "8          60            19           63  \n",
       "9          70            22           70  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "\n",
    "model_name = 'distilbert-base-uncased'\n",
    "x = X_valid_processed.tolist()\n",
    "y = y_valid.tolist()\n",
    "\n",
    "def distilbert_batch_predict(model, encoder,model_name, max_len,x,y) :\n",
    "    tkzr = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "    encodings_x =  tkzr(x, max_length=MAX_LEN, truncation=True, padding='max_length',return_tensors='tf')\n",
    "    tfdataset = construct_tfdataset(encodings_x).batch(32)\n",
    "    preds = loaded_model.predict(tfdataset)\n",
    "    predictions_encode = pd.DataFrame(data=preds).apply(lambda x: np.argmax(x),axis=1)\n",
    "    categories = encoder.classes_.tolist()\n",
    "    predictions_event= predictions_encode.apply(lambda x:categories[x])\n",
    "\n",
    "\n",
    "    print(len(predictions_event))\n",
    "    results = pd.DataFrame({'text': x,\n",
    "                      'true_event':y,\n",
    "                      'preds_encode': predictions_encode,\n",
    "                      'preds_event':predictions_event\n",
    "                            })\n",
    "\n",
    "\n",
    "    return results\n",
    "\n",
    "results = distilbert_batch_predict(loaded_model,encoder,model_name,MAX_LEN,x,y)\n",
    "results.to_csv('solution.csv')\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7946544980443285,\n",
       " 'balanced_accuracy': 0.697549854765685,\n",
       " 'f1': 0.7125622769786281,\n",
       " 'precision': 0.7423009184922906,\n",
       " 'recall': 0.697549854765685}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score,balanced_accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.true_event\n",
    "    preds = pred.preds_event\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    bal_acc = balanced_accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels,preds,average='macro')\n",
    "    recall = recall_score(labels,preds,average='macro')\n",
    "    f1 = f1_score(labels,preds,average='macro')\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'balanced_accuracy':bal_acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "compute_metrics(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning (HPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tuning-DistilBERT-07-28-27-28'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuning_job_name_prefix = f\"tuning-{model_name}\"\n",
    "timestamp = strftime(\"-%m-%d-%M-%S\", gmtime())\n",
    "base_tuning_job_name = tuning_job_name_prefix + timestamp\n",
    "base_tuning_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training-DistilBERT-07-28-27-19 tuning-DistilBERT-07-28-27-28\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tuner import HyperparameterTuner, ContinuousParameter,CategoricalParameter\n",
    "\n",
    "# Configure HyperparameterTuner\n",
    "_tuner = HyperparameterTuner(\n",
    "                             base_tuning_job_name = base_tuning_job_name,\n",
    "                             estimator=_distillbert_estimator,  # previously-configured Estimator object\n",
    "                             objective_metric_name='validation:accuracy',\n",
    "                             hyperparameter_ranges={'learning_rate': CategoricalParameter(['2e-5','3e-5', '5e-5']),\n",
    "                                                      'epochs':CategoricalParameter(['4','5']),\n",
    "                                                      'batch_size':CategoricalParameter(['16','32']) \n",
    "                                                   },\n",
    "                             metric_definitions = [{'Name':'validation:accuracy','Regex':'val_acc: ([0-9\\\\.]+)'}],\n",
    "                             max_jobs=12,\n",
    "                             max_parallel_jobs=2)\n",
    "    \n",
    "   \n",
    "    \n",
    "print(job_name,base_tuning_job_name)\n",
    "_tuner.fit({'train':tfrecord_data_location}, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from time import gmtime, strftime\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "model_name = 'BaseBERT'\n",
    "job_name_prefix = f\"training-{model_name}\"\n",
    "timestamp = strftime(\"-%m-%d-%M-%S\", gmtime())\n",
    "\n",
    "job_name = job_name_prefix + timestamp\n",
    "\n",
    "_bert_estimator = HuggingFace(\n",
    "        base_job_name  = job_name,\n",
    "        entry_point=\"train.py\",\n",
    "        source_dir = \"./src/\",\n",
    "        role=role,\n",
    "        instance_count=1,\n",
    "        volume_size = 5,\n",
    "        max_run = 18000,\n",
    "        instance_type='ml.p3.2xlarge',\n",
    "        transformers_version = \"4.4\",\n",
    "        tensorflow_version  = \"2.4\",\n",
    "        py_version=\"py37\",\n",
    "        output_path = output_path,\n",
    "        hyperparameters = {\n",
    "                \"model_name\": model_name,\n",
    "                \"num_records\":  num_records,\n",
    "                \"max_len\":max_len,\n",
    "                \"epochs\":int(epochs),\n",
    "                \"learning_rate\":float(learning_rate),\n",
    "                \"batch_size\":int(batch_size),\n",
    "                \"valid_batch_size\":valid_batch_size,\n",
    "                \"steps_per_epoch\": steps_per_epoch,\n",
    "                \"validation_steps\": validation_steps,\n",
    "                \"optimizer\":optimizer\n",
    "                },\n",
    "        metric_definitions = [{'Name':'train:loss','Regex':'loss: ([0-9\\\\.]+)'},\n",
    "                                    {'Name':'train:accuracy','Regex':'acc: ([0-9\\\\.]+)'},\n",
    "                                    {'Name':'validation:loss','Regex':'val_loss: ([0-9\\\\.]+)'},\n",
    "                                    {'Name':'validation:accuracy','Regex':'val_acc: ([0-9\\\\.]+)'}],\n",
    "        enable_sagemaker_metrics = True\n",
    "    )\n",
    "\n",
    "_bert_estimator.fit({'train':tfrecord_data_location}, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "S3Downloader.download(\n",
    "    s3_uri=_bert_estimator.model_data, # s3 uri where the trained model is located\n",
    "    local_path='./output/model/basebert/', # local path where *.targ.gz is saved\n",
    "    sagemaker_session=sagemaker_session # sagemaker session used for training the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "from transformers.optimization_tf import AdamWeightDecay\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "\n",
    "\n",
    "t = tarfile.open('./output/model/basebert/model.tar.gz', 'r:gz')\n",
    "t.extractall(path='./output/model/basebert')\n",
    "bert_model = tf.keras.models.load_model('./output/model/basebert/BaseBERT',custom_objects={'AdamWeightDecay':AdamWeightDecay})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Training data...\n",
      "8659/8659 [==============================] - 3195s 369ms/step - loss: 0.1164 - acc: 0.9641\n",
      "Training loss:  0.11638274043798447\n",
      "Training accuracy:  0.9641124606132507\n",
      "Evaluating Validation data...\n",
      "962/962 [==============================] - 355s 368ms/step - loss: 0.5353 - acc: 0.8526\n",
      "Validation loss:  0.535329282283783\n",
      "Validation accuracy:  0.8525857329368591\n"
     ]
    }
   ],
   "source": [
    "def _evaluate_model(loaded_model):\n",
    "    print(\"Evaluating Training data...\")\n",
    "    train_score = loaded_model.evaluate(train_dataset,\n",
    "                                     steps = steps_per_epoch,\n",
    "                                     batch_size=batch_size)\n",
    "\n",
    "    print(\"Training loss: \", train_score[0])\n",
    "    print(\"Training accuracy: \", train_score[1])\n",
    "\n",
    "    print(\"Evaluating Validation data...\")\n",
    "    valid_score = loaded_model.evaluate(valid_dataset, steps = validation_steps,batch_size=valid_batch_size)\n",
    "    print(\"Validation loss: \", valid_score[0])\n",
    "    print(\"Validation accuracy: \", valid_score[1])\n",
    "    \n",
    "_evaluate_model(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
